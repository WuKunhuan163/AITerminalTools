#!/usr/bin/env python3
"""
LEARN.py - æ™ºèƒ½å­¦ä¹ ç³»ç»Ÿ
ç‹¬ç«‹çš„å­¦ä¹ ææ–™ç”Ÿæˆå·¥å…·ï¼Œæ”¯æŒäº¤äº’æ¨¡å¼å’Œç›´æ¥è°ƒç”¨
"""

import os
import sys
import argparse
import subprocess
import json
import time
import datetime
from pathlib import Path
from typing import Dict, Any, Optional

# åŠ è½½ç¯å¢ƒå˜é‡
from dotenv import load_dotenv
load_dotenv()

# Parameter mappings for bilingual support
MODE_MAPPING = {
    # Chinese
    'åˆå­¦è€…': 'beginner',
    'ä¸­çº§': 'intermediate', 
    'é«˜çº§': 'advanced',
    'ä¸“å®¶': 'expert',
    # English
    'beginner': 'beginner',
    'intermediate': 'intermediate',
    'advanced': 'advanced', 
    'expert': 'expert',
    # Capitalized English
    'Beginner': 'beginner',
    'Intermediate': 'intermediate',
    'Advanced': 'advanced',
    'Expert': 'expert'
}

STYLE_MAPPING = {
    # Chinese
    'ç®€æ´æ˜äº†': 'concise',
    'è¯¦ç»†æ·±å…¥': 'detailed',
    'å®ä¾‹ä¸°å¯Œ': 'practical',
    'ç†è®ºå¯¼å‘': 'theoretical',
    # English
    'concise': 'concise',
    'detailed': 'detailed',
    'example-rich': 'practical',
    'theory-oriented': 'theoretical',
    # Alternative English
    'brief': 'concise',
    'comprehensive': 'detailed',
    'practical': 'practical',
    'theoretical': 'theoretical',
    # Capitalized/other variants
    'Concise': 'concise',
    'Detailed': 'detailed',
    'Practical': 'practical',
    'Theoretical': 'theoretical',
    'Witty': 'practical',  # Legacy support
    # Additional test mappings
    'RichExamples': 'practical',
    'TheoryOriented': 'theoretical'
}

# å…¨å±€æ—¶é—´è·Ÿè¸ªå™¨
LEARN_START_TIME = None

def get_elapsed_time():
    """è·å–ä»LEARNå¼€å§‹è¿è¡Œä»¥æ¥çš„ç»è¿‡æ—¶é—´"""
    if LEARN_START_TIME is None:
        return "00:00"
    elapsed = time.time() - LEARN_START_TIME
    minutes = int(elapsed // 60)
    seconds = int(elapsed % 60)
    return f"{minutes:02d}:{seconds:02d}"

def log_progress(message, level="INFO"):
    """è¾“å‡ºå¸¦æ—¶é—´æˆ³çš„è¿›åº¦ä¿¡æ¯"""
    elapsed = get_elapsed_time()
    timestamp = datetime.datetime.now().strftime("%H:%M:%S")
    print(f"[{timestamp}|{elapsed}] {level}: {message}")

def start_timer():
    """å¯åŠ¨å…¨å±€è®¡æ—¶å™¨"""
    global LEARN_START_TIME
    LEARN_START_TIME = time.time()
    log_progress("LEARNç³»ç»Ÿå¯åŠ¨", "START")

def normalize_parameters(args):
    """Normalize bilingual parameters to Chinese equivalents"""
    if args.mode and args.mode in MODE_MAPPING:
        args.mode = MODE_MAPPING[args.mode]
    elif args.mode and args.mode not in MODE_MAPPING:
        # If not found, suggest available options
        available_modes = list(set(MODE_MAPPING.keys()))
        print(f"Error: Invalid mode '{args.mode}'. Available options: {', '.join(available_modes)}")
        return None
        
    if args.style and args.style in STYLE_MAPPING:
        args.style = STYLE_MAPPING[args.style]
    elif args.style and args.style not in STYLE_MAPPING:
        # If not found, suggest available options
        available_styles = list(set(STYLE_MAPPING.keys()))
        print(f"Error: Invalid style '{args.style}'. Available options: {', '.join(available_styles)}")
        return None
        
    return args

def is_run_environment(command_identifier=None):
    """Check if running in RUN environment by checking environment variables"""
    if command_identifier:
        return os.environ.get(f'RUN_IDENTIFIER_{command_identifier}') == 'True'
    return False

def write_to_json_output(data, command_identifier=None):
    """å°†ç»“æœå†™å…¥åˆ°æŒ‡å®šçš„ JSON è¾“å‡ºæ–‡ä»¶ä¸­"""
    if not is_run_environment(command_identifier):
        return False
    
    # Get the specific output file for this command identifier
    if command_identifier:
        output_file = os.environ.get(f'RUN_DATA_FILE_{command_identifier}')
    else:
        output_file = os.environ.get('RUN_DATA_FILE')
    
    if not output_file:
        return False
    
    try:
        output_path = Path(output_file)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
        return True
    except Exception as e:
        print(f"Error writing to JSON output file: {e}")
        return False

def clear_terminal():
    """Clear the terminal screen."""
    os.system('clear' if os.name == 'posix' else 'cls')


def interactive_select(prompt, options, default_index=0):
    """Interactive selection with numbered options."""
    print(f"{prompt}")
    for i, option in enumerate(options):
        print(f"  {i+1}. {option}")
    
    while True:
        try:
            choice = input(f"Choose (1-{len(options)}, default: {default_index+1}): ").strip()
            if not choice:
                print(f"Selected: {options[default_index]}")
                return default_index
            
            choice_num = int(choice) - 1
            if 0 <= choice_num < len(options):
                print(f"Selected: {options[choice_num]}")
                return choice_num
            else:
                print(f"Please enter a number between 1 and {len(options)}")
        except ValueError:
            print(f"Please enter a valid number between 1 and {len(options)}")
        except KeyboardInterrupt:
            print("\nCancelled.")
            return None


def check_and_confirm_overwrite(output_dir, not_default=False, no_override_material=False):
    """Check if tutorial.md or question.md exists and handle overwrite based on options."""
    tutorial_path = Path(output_dir) / "tutorial.md"
    question_path = Path(output_dir) / "question.md"
    
    existing_files = []
    if tutorial_path.exists():
        existing_files.append("tutorial.md")
    if question_path.exists():
        existing_files.append("question.md")
    
    if not existing_files:
        return True, output_dir  # No files to overwrite, use original directory
    
    # å¦‚æœæŒ‡å®šäº†--no-override-materialï¼Œè‡ªåŠ¨é‡å‘½å
    if no_override_material:
        return handle_auto_rename(output_dir)
    
    # é»˜è®¤æ¨¡å¼ï¼ˆ--not-defaultæœªæŒ‡æ˜ï¼‰ï¼šç›´æ¥è¦†ç›–
    if not not_default:
        print(f"Overwriting existing files in {output_dir}: {', '.join(existing_files)}")
        return True, output_dir
    
    # äº¤äº’æ¨¡å¼ï¼šè¯¢é—®ç”¨æˆ·
    print(f"\nWarning: The following files already exist in {output_dir}:")
    for file in existing_files:
        print(f"  - {file}")
    
    while True:
        try:
            choice = input("\nChoose action: (o) overwrite / (r) rename / (c) cancel [o/r/c]: ").strip().lower()
            if choice in ['o', 'overwrite', 'è¦†ç›–']:
                return True, output_dir
            elif choice in ['r', 'rename', 'é‡å‘½å', 'rename']:
                return handle_auto_rename(output_dir)
            elif choice in ['c', 'cancel', 'å–æ¶ˆ', '']:
                return False, None
            else:
                print("Please enter o (overwrite) / r (rename) / c (cancel)")
        except KeyboardInterrupt:
            print("\nOperation cancelled")
            return False, None


def handle_auto_rename(output_dir):
    """Handle automatic renaming of output directory to avoid overwriting files."""
    output_path = Path(output_dir)
    base_name = output_path.name
    parent_dir = output_path.parent
    
    counter = 1
    while True:
        new_name = f"{base_name}_{counter}"
        new_path = parent_dir / new_name
        
        # æ£€æŸ¥æ–°ç›®å½•ä¸­æ˜¯å¦ä¹Ÿæœ‰å†²çªæ–‡ä»¶
        tutorial_path = new_path / "tutorial.md"
        question_path = new_path / "question.md"
        
        if not new_path.exists() or (not tutorial_path.exists() and not question_path.exists()):
            # åˆ›å»ºæ–°ç›®å½•
            new_path.mkdir(parents=True, exist_ok=True)
            print(f"Auto-renamed output directory: {new_path}")
            return True, str(new_path)
        
        counter += 1
        if counter > 100:  # é˜²æ­¢æ— é™å¾ªç¯
            print("Error: Unable to find a suitable directory name, please manually clean up the output directory")
            return False, None


def get_output_directory():
    """Get output directory using tkinter directory selection."""
    print("Select project directory...")
    return get_output_directory_tkinter()


def get_output_directory_tkinter():
    """Get output directory using tkinter as fallback."""
    try:
        import tkinter as tk
        from tkinter import filedialog
        
        print("ğŸ“ Please select the output folder in the pop-up window...")
        
        # åˆ›å»ºtkinteræ ¹çª—å£å¹¶éšè—
        root = tk.Tk()
        root.withdraw()
        root.attributes('-topmost', True)
        
        # æ‰“å¼€ç›®å½•é€‰æ‹©å¯¹è¯æ¡†
        selected_dir = filedialog.askdirectory(
            title="Select project directory"
        )
        
        # é”€æ¯tkinterçª—å£
        root.destroy()
        
        if selected_dir:
            print(f"Selected directory: {selected_dir}")
            return selected_dir
        else:
            print("Error: No directory selected")
            return None
            
    except ImportError:
        print("Error: tkinter is not available, please manually enter the directory path")
        return None
    except Exception as e:
        print(f"Error: Directory selection failed: {e}")
        return None


def get_paper_file():
    """Get paper file using tkinter file selection."""
    try:
        import tkinter as tk
        from tkinter import filedialog
        
        print("Please select the paper file in the pop-up window...")
        
        # åˆ›å»ºtkinteræ ¹çª—å£å¹¶éšè—
        root = tk.Tk()
        root.withdraw()
        root.attributes('-topmost', True)
        
        # æ‰“å¼€æ–‡ä»¶é€‰æ‹©å¯¹è¯æ¡†
        selected_file = filedialog.askopenfilename(
            title="Select paper file",
            filetypes=[
                ("PDFæ–‡ä»¶", "*.pdf"),
                ("Markdownæ–‡ä»¶", "*.md"),
                ("æ–‡æœ¬æ–‡ä»¶", "*.txt"),
                ("æ‰€æœ‰æ–‡ä»¶", "*.*")
            ]
        )
        
        # é”€æ¯tkinterçª—å£
        root.destroy()
        
        if selected_file:
            print(f"Selected file: {selected_file}")
            return selected_file
        else:
            print("Error: No file selected")
            return None
            
    except ImportError:
        print("Error: tkinter is not available, please manually enter the file path")
        return None
    except Exception as e:
        print(f"Error: File selection failed: {e}")
        return None


def run_interactive_mode():
    """Run in interactive mode to collect parameters."""
    clear_terminal()
    print("=== LEARN Intelligent Learning System ===")
    print("Welcome to the intelligent learning content generation tool!")
    print()
    
    # Step 1: Select learning type
    print("ğŸ“š Step 1: Select learning type")
    type_choice = interactive_select(
        "Learning type:",
        ["General topic learning", "Academic paper learning"]
    )
    if type_choice is None:
        return None
    
    params = {}
    
    if type_choice == 0:  # General topic
        params["type"] = "general"
        
        # Get topic
        print("\nStep 2: Input learning topic")
        while True:
            topic = input("Please enter the learning topic (e.g., Python basics, machine learning, data structure): ").strip()
            if topic:
                try:
                    # è§£ææ–‡ä»¶å¼•ç”¨
                    expanded_topic, has_file_ref = parse_file_references(topic)
                    params["topic"] = expanded_topic
                    params["has_file_reference"] = has_file_ref
                    # å¦‚æœæ£€æµ‹åˆ°æ–‡ä»¶å¼•ç”¨ï¼Œè‡ªåŠ¨å¯ç”¨contextæ¨¡å¼
                    if has_file_ref:
                        params['context_mode'] = True
                        print("ğŸ“„ Detected @file reference, automatically enable --context mode")
                    break
                except (FileNotFoundError, ValueError) as e:
                    print(f"Error: {e}")
                    print("Please enter a valid topic or file path")
                    continue
            print("Please enter a valid topic")
        
    else:  # Paper-based
        params["type"] = "paper"
        
        print("\nğŸ“„ Step 2: Select paper input method")
        input_choice = interactive_select(
            "Paper input method:",
            ["Local Markdown file", "Local PDF file", "Paper URL", "Paper description/search"]
        )
        if input_choice is None:
            return None
            
        params["input_type"] = input_choice
        
        if input_choice == 0:  # Markdown file
            paper_file = get_paper_file()
            if not paper_file:
                return None
            params["paper_path"] = paper_file
            
            # Read markdown content
            try:
                with open(paper_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                params["paper_content"] = content
                print(f"Read Markdown file: {len(content)} characters")
            except Exception as e:
                print(f"Error: Read file failed: {e}")
                return None
                
        elif input_choice == 1:  # PDF file
            paper_file = get_paper_file()
            if not paper_file:
                return None
            params["paper_path"] = paper_file
            
            # Ask about image processing
            print("\nğŸ–¼ï¸  Image processing options")
            image_choice = interactive_select(
                "Process images, formulas, and tables in PDF?",
                ["No (only extract text, faster)", "Yes (full processing, requires API call)"]
            )
            params["read_images"] = image_choice == 1
            
        elif input_choice == 2:  # URL
            while True:
                url = input("Please enter the paper URL: ").strip()
                if url:
                    params["paper_url"] = url
                    break
                print("Please enter a valid URL")
                
            # Ask about image processing
            print("\nğŸ–¼ï¸  Image processing options")
            image_choice = interactive_select(
                "Process images, formulas, and tables in PDF?",
                ["No (only extract text, faster)", "Yes (full processing, requires API call)"]
            )
            params["read_images"] = image_choice == 1
            
        elif input_choice == 3:  # Description/Search
            while True:
                description = input("Please enter the paper description or keywords: ").strip()
                if description:
                    params["paper_description"] = description
                    break
                print("Please enter a valid description")
                
            # Ask about image processing
            print("\nğŸ–¼ï¸  Image processing options")
            image_choice = interactive_select(
                "Process images, formulas, and tables in PDF?",
                ["No (only extract text, faster)", "Yes (full processing, requires API call)"]
            )
            params["read_images"] = image_choice == 1
    
    # Step 3: Select learning level
    print("\nğŸ¯ Step 3: Select learning level")
    mode_choice = interactive_select(
        "Learning level:",
        ["Beginner", "Intermediate", "Advanced", "Expert"]
    )
    if mode_choice is None:
        return None
    
    modes = ["åˆå­¦è€…", "ä¸­çº§", "é«˜çº§", "ä¸“å®¶"]
    params["mode"] = modes[mode_choice]
    
    # Step 4: Select explanation style
    print("\nğŸ“– Step 4: Select explanation style")
    style_choice = interactive_select(
        "Explanation style:",
        ["Concise and clear", "Detailed and in-depth", "Rich examples", "Theoreticalå¯¼å‘"]
    )
    if style_choice is None:
        return None
    
    styles = ["Concise and clear", "Detailed and in-depth", "Rich examples", "Theoreticalå¯¼å‘"]
    params["style"] = styles[style_choice]
    
    # Step 5: Get output directory
    print("\nğŸ“ Step 5: Select output directory")
    output_dir = get_output_directory()
    if not output_dir:
        return None
    
    params["output_dir"] = output_dir
    
    # Check for existing files and handle overwrite
    can_continue, final_output_dir = check_and_confirm_overwrite(
        output_dir, 
        params.get('not_default', False),
        params.get('no_override_material', False)
    )
    
    if not can_continue:
        print("Operation cancelled")
        return None
    
    # Update output directory if it was renamed
    if final_output_dir != output_dir:
        params["output_dir"] = final_output_dir
    
    return params


def parse_direct_command(args):
    """Parse direct command line arguments."""
    parser = argparse.ArgumentParser(description='LEARN - Intelligent Learning System')
    
    # Basic options
    parser.add_argument('topic', nargs='?', help='Learning topic')
    parser.add_argument('-o', '--output-dir', help='Output directory')
    parser.add_argument('-m', '--mode', choices=list(MODE_MAPPING.keys()), 
                       default='Intermediate', help='Learning level (Intermediate)')
    parser.add_argument('-s', '--style', choices=list(STYLE_MAPPING.keys()),
                       default='Detailed and in-depth', help='Explanation style (Detailed and in-depth)')
    
    # File options
    parser.add_argument('--file', help='Directly process file path (supports PDF, MD, TXT)')
    parser.add_argument('-u', '--url', help='Paper URL')
    parser.add_argument('-d', '--description', help='Paper description/search keywords')
    parser.add_argument('--negative', help='Negative prompt: specify content or paper type you don\'t want')
    parser.add_argument('--read-images', action='store_true', help='Process images, formulas, and tables in PDF')
    parser.add_argument('--gen-command', help='Generate LEARN command based on description')
    parser.add_argument('--paper-based', action='store_true', help='Force use of paper-based learning mode, even if only a description is provided, it will search and download papers')
    parser.add_argument('--sources', help='Specify paper search engines, separated by commas (arxiv,google_scholar), default is automatic recommendation')
    
    # Model options
    parser.add_argument('--model', help='Specify OpenRouter model')
    parser.add_argument('--max-tokens', type=int, help='Maximum token number')
    parser.add_argument('--temperature', type=float, help='Temperature parameter (0.0-2.0, control creativity of response)')
    parser.add_argument('--key', help='Specify OpenRouter API key')
    parser.add_argument('--not-default', action='store_true', help='Non-default mode, requires user confirmation')
    parser.add_argument('--no-override-material', action='store_true', help='Do not overwrite existing files, automatically rename')
    parser.add_argument('--brainstorm-only', action='store_true', help='Do not automatically create files, only generate content')
    parser.add_argument('--context', action='store_true', help='Treat description as direct context for brainstorming, skip paper search')
    
    try:
        parsed_args = parser.parse_args(args)
    except SystemExit:
        return None
    
    # Normalize bilingual parameters
    parsed_args = normalize_parameters(parsed_args)
    if parsed_args is None:
        return None
    
    # æ£€æŸ¥äº’æ–¥å‚æ•°
    if parsed_args.context and parsed_args.brainstorm_only:
        print("Error: --context and --brainstorm-only options are mutually exclusive, cannot be used together")
        print("   --context: Skip brainstorming, directly generate tutorial")
        print("   --brainstorm-only: Only perform brainstorming, do not generate tutorial")
        return None
    
    # Check if output is required for actual operation (not for --help)
    if not parsed_args.output_dir and not any(arg in ['-h', '--help'] for arg in args):
        # Default to current directory if not specified
        parsed_args.output_dir = str(Path.cwd())
        print(f"â„¹ï¸  No output directory specified, using current directory: {parsed_args.output_dir}")
    
    # Build parameters
    params = {
        'mode': parsed_args.mode,
        'style': parsed_args.style,
        'output_dir': parsed_args.output_dir,
        'not_default': parsed_args.not_default,
        'no_override_material': parsed_args.no_override_material,
        'brainstorm_only': parsed_args.brainstorm_only,
        'context_mode': parsed_args.context
    }
    
    if parsed_args.model:
        params['selected_model'] = parsed_args.model
    if parsed_args.max_tokens:
        params['max_tokens'] = parsed_args.max_tokens
    if parsed_args.temperature is not None:
        params['temperature'] = parsed_args.temperature
    if parsed_args.key:
        params['api_key'] = parsed_args.key
    
    # Determine type based on arguments
    if parsed_args.file:
        # --fileé€‰é¡¹å¤„ç†
        file_path = parsed_args.file
        params['type'] = 'paper'
        
        # æ ¹æ®æ–‡ä»¶æ‰©å±•ååˆ¤æ–­ç±»å‹
        if file_path.endswith('.md'):
            params['input_type'] = 0  # Markdown file
            # è¯»å–markdownæ–‡ä»¶å†…å®¹
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    params['paper_content'] = f.read()
                params['paper_path'] = file_path
            except Exception as e:
                print(f"Error: Read markdown file failed: {e}")
                return 1
        elif file_path.endswith('.txt'):
            params['input_type'] = 0  # Text file (treated as markdown)
            # è¯»å–æ–‡æœ¬æ–‡ä»¶å†…å®¹
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    params['paper_content'] = f.read()
                params['paper_path'] = file_path
            except Exception as e:
                print(f"Error: Read text file failed: {e}")
                return 1
        elif file_path.endswith('.pdf'):
            params['input_type'] = 1  # PDF file
            params['paper_path'] = file_path
        else:
            # é»˜è®¤æŒ‰PDFå¤„ç†
            params['input_type'] = 1  # PDF file
            params['paper_path'] = file_path
        params['read_images'] = parsed_args.read_images
    elif parsed_args.url:
        params['type'] = 'paper'
        params['input_type'] = 2  # URL
        params['paper_url'] = parsed_args.url
        params['read_images'] = parsed_args.read_images
    elif parsed_args.description:
        params['type'] = 'paper'
        params['input_type'] = 3  # Description/Search
        try:
            expanded_description, has_file_ref = parse_file_references(parsed_args.description)
            params['paper_description'] = expanded_description
            params['has_file_reference'] = has_file_ref
            # å¦‚æœæ£€æµ‹åˆ°æ–‡ä»¶å¼•ç”¨ï¼Œè‡ªåŠ¨å¯ç”¨contextæ¨¡å¼
            if has_file_ref:
                params['context_mode'] = True
                print("ğŸ“„ Detected @file reference, automatically enable --context mode")
        except (FileNotFoundError, ValueError) as e:
            print(f"Error: {e}")
            return None
        params['negative_prompt'] = parsed_args.negative
        params['read_images'] = parsed_args.read_images
        params['sources'] = parsed_args.sources
    elif parsed_args.topic:
        try:
            expanded_topic, has_file_ref = parse_file_references(parsed_args.topic)
            
            # æ£€æŸ¥topicæ˜¯å¦æ˜¯ä¸€ä¸ªæ–‡ä»¶è·¯å¾„
            topic_path = Path(parsed_args.topic)
            if topic_path.exists() and topic_path.is_file():
                # å¦‚æœæ˜¯æ–‡ä»¶è·¯å¾„ï¼ŒæŒ‰æ–‡ä»¶ç±»å‹å¤„ç†
                file_ext = topic_path.suffix.lower()
                if file_ext == '.pdf':
                    params['type'] = 'paper'
                    params['input_type'] = 1  # PDF file
                    params['paper_path'] = str(topic_path)
                    params['read_images'] = parsed_args.read_images
                    print(f"Detected PDF file path, switch to paper learning mode: {topic_path}")
                elif file_ext in ['.md', '.txt']:
                    params['type'] = 'paper'
                    params['input_type'] = 0 if file_ext == '.md' else 4  # Markdown or direct file
                    if file_ext == '.md':
                        # è¯»å–markdownæ–‡ä»¶å†…å®¹
                        with open(topic_path, 'r', encoding='utf-8') as f:
                            params['paper_content'] = f.read()
                        params['paper_path'] = str(topic_path)
                    else:
                        params['file_path'] = str(topic_path)
                    params['read_images'] = parsed_args.read_images
                else:
                    # å…¶ä»–æ–‡ä»¶ç±»å‹ä»ç„¶æŒ‰ä¸€èˆ¬ä¸»é¢˜å¤„ç†
                    params['type'] = 'general'
                    params['topic'] = expanded_topic
                    params['has_file_reference'] = has_file_ref
            else:
                # æ£€æŸ¥æ˜¯å¦æŒ‡å®šäº†--paper-basedæ ‡å¿—
                if parsed_args.paper_based:
                    # å¼ºåˆ¶ä½¿ç”¨è®ºæ–‡æ¨¡å¼ï¼Œå°†topicä½œä¸ºè®ºæ–‡æè¿°æœç´¢
                    params['type'] = 'paper'
                    params['input_type'] = 3  # Description/Search
                    params['paper_description'] = expanded_topic
                    params['has_file_reference'] = has_file_ref
                    params['negative_prompt'] = parsed_args.negative
                    params['read_images'] = parsed_args.read_images
                    params['sources'] = parsed_args.sources
                    print(f"ğŸ” --paper-based mode: use topic as paper search keywords: {expanded_topic}")
                else:
                    # ä¸æ˜¯æ–‡ä»¶è·¯å¾„ï¼ŒæŒ‰ä¸€èˆ¬ä¸»é¢˜å¤„ç†
                    params['type'] = 'general'
                    params['topic'] = expanded_topic
                    params['has_file_reference'] = has_file_ref
            
            # å¦‚æœæ£€æµ‹åˆ°@æ–‡ä»¶å¼•ç”¨ï¼Œè‡ªåŠ¨å¯ç”¨contextæ¨¡å¼
            if has_file_ref:
                params['context_mode'] = True
                print("ğŸ“„ Detected @file reference, automatically enable --context mode")
        except (FileNotFoundError, ValueError) as e:
            print(f"Error: {e}")
            return None
    else:
        print("Error: must specify learning topic or paper information")
        return None
    
    # Check for existing files and handle overwrite in direct mode
    if params['output_dir']:
        can_continue, final_output_dir = check_and_confirm_overwrite(
            params['output_dir'], 
            params.get('not_default', False),
            params.get('no_override_material', False)
        )
        
        if not can_continue:
            print("Operation cancelled")
            return None
        
        # Update output directory if it was renamed
        if final_output_dir != params['output_dir']:
            params['output_dir'] = final_output_dir
    
    return params


def get_openrouter_models():
    """Get available OpenRouter models."""
    try:
        script_dir = Path(__file__).parent
        openrouter_data_file = script_dir / "OPENROUTER_PROJ" / "openrouter_models.json"
        
        if openrouter_data_file.exists():
            import json
            with open(openrouter_data_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                models_dict = data.get('models', {})
                if isinstance(models_dict, dict):
                    # Extract model names from dictionary format
                    models = list(models_dict.keys())
                    model_details = models_dict
                else:
                    # Handle list format (legacy)
                    models = models_dict
                    model_details = data.get('model_details', {})
                return models, model_details
        else:
            # Fallback to default models
            default_models = [
                "deepseek/deepseek-r1:free",
                "deepseek/deepseek-chat",
                "openai/gpt-4o-mini",
                "anthropic/claude-3-haiku"
            ]
            return default_models, {}
    except Exception as e:
        print(f"Warning:  Get model list failed: {e}")
        # Return minimal fallback
        return ["deepseek/deepseek-r1:free"], {}


def select_openrouter_model(params):
    """Select OpenRouter model with token limits."""
    models, model_details = get_openrouter_models()
    
    if not models:
        print("Error:  No available models")
        return None, None
    
    # Check if model is already specified
    if params.get("selected_model"):
        selected_model = params["selected_model"]
        max_tokens = params.get("max_tokens", 4000)
        print(f"Using specified model: {selected_model}")
        return selected_model, max_tokens
    
    # Auto-select for default mode (use "auto" for automatic model selection)
    if not params.get('not_default', False):
        selected_model = "auto"  # ä½¿ç”¨autoæ¨¡å¼è‡ªåŠ¨é€‰æ‹©
        max_tokens = 4000
        print(f"Default mode: automatic model selection")
        return selected_model, max_tokens
    
    # Interactive mode - let user choose
    print(f"\nğŸ“‹ Available model list:")
    print("=" * 80)
    for i, model in enumerate(models):
        model_info = model_details.get(model, {})
        input_cost = model_info.get('input_cost_per_1m', 0)
        output_cost = model_info.get('output_cost_per_1m', 0)
        context_length = model_info.get('context_length', 0)
        
        print(f" {i+1}. {model}")
        print(f"    ğŸ“Š Rate: input ${input_cost:.2f}/1M, output ${output_cost:.2f}/1M")
        print(f"    ğŸ“ Context length: {context_length:,} tokens")
        print()
    
    print(f" {len(models)+1}. auto (auto select best model)")
    print("    ğŸ¤– The system will automatically select available models based on priority")
    print()
    
    while True:
        try:
            choice = input(f"Select model (1-{len(models)+1}, default: auto): ").strip()
            
            if not choice or choice.lower() == 'auto':
                selected_model = "auto"
                break
            
            choice_num = int(choice)
            if choice_num == len(models) + 1:  # autoé€‰é¡¹
                selected_model = "auto"
                break
            elif 1 <= choice_num <= len(models):
                selected_model = models[choice_num - 1]
                break
            else:
                print(f"Error: Please enter a number between 1 and {len(models)+1}")
                
        except ValueError:
            print("Error:  Please enter a valid number")
        except KeyboardInterrupt:
            print("\nâŒ User cancelled")
            return None, None
    
    # Set max tokens based on model
    if selected_model == "auto":
        max_tokens = 40960  # Higher default value, will be dynamically adjusted when called
        print(f"ğŸ¤– Select auto mode")
    else:
        model_info = model_details.get(selected_model, {})
        context_length = model_info.get('context_length', 4000)
        max_tokens = context_length // 4  # Use 1/4 of context length
        print(f"Select model: {selected_model} (max_tokens: {max_tokens})")
    
    return selected_model, max_tokens


def generate_content_structure_prompt(params):
    """Generate prompt for content structure brainstorming."""
    if params["type"] == "general":
        topic = params['topic']
        mode = params['mode']
        style = params['style']
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«æ–‡ä»¶å¼•ç”¨
        if params.get("has_file_reference", False):
            print("ğŸ“„ Detected file reference, will create tutorial based on file content")
            return f'Create detailed learning tutorial structure based on the following content, suitable for {mode} level learners, using {style} explanation style:\n\n{topic}'
        else:
            return f'Create detailed learning tutorial structure for "{topic}", suitable for {mode} level learners, using {style} explanation style.'
        
    elif params["type"] == "paper":
        mode = params['mode']
        style = params['style']
        
        # é¦–å…ˆè¿›è¡Œæ¨¡å‹é€‰æ‹©ï¼ˆå¦‚æœè¿˜æ²¡æœ‰é€‰æ‹©çš„è¯ï¼‰
        if not params.get("selected_model"):
            selected_model, max_tokens = select_openrouter_model(params)
            if not selected_model:
                print("Error:  No model selected")
                return None
            
            # Store selected model info in params
            params["selected_model"] = selected_model
            params["max_tokens"] = max_tokens
        
        # For paper type, prepare content first
        paper_content, paper_path, token_count = prepare_paper_content(params)
        if not paper_content:
            return None
            
        # Store prepared content in params
        params['paper_content'] = paper_content
        params['paper_path'] = paper_path
        params['token_count'] = token_count
        
        # Check if content is too long and needs summarization
        # è·å–åŠ¨æ€max_tokensè®¾ç½®
        dynamic_max_tokens = params.get("max_tokens", 40960)  # é»˜è®¤å€¼
        
        # å¦‚æœæ˜¯è‡ªåŠ¨æ¨¡å¼ï¼Œä½¿ç”¨æ›´åˆç†çš„é˜ˆå€¼ï¼ˆåŸºäºdeepseekæ¨¡å‹çš„context lengthï¼‰
        if params.get("selected_model") == "auto" or not params.get("selected_model"):
            # è‡ªåŠ¨æ¨¡å¼ï¼šä½¿ç”¨deepseekæ¨¡å‹çš„å®é™…context lengthè®¡ç®—é˜ˆå€¼
            deepseek_context_length = 163840
            dynamic_max_tokens = deepseek_context_length // 4  # 40960
            content_threshold = dynamic_max_tokens  # ç›´æ¥ä½¿ç”¨max_tokensä½œä¸ºé˜ˆå€¼
        else:
            # ç›´æ¥ä½¿ç”¨max_tokensä½œä¸ºé˜ˆå€¼
            content_threshold = dynamic_max_tokens
        
        if token_count > content_threshold:
            print(f"Warning:  Paper content is too long ({token_count:,} tokens), exceeds recommended processing length ({content_threshold:,} tokens)")
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºé»˜è®¤æ¨¡å¼
            if params.get("not_default", False):
                # éé»˜è®¤æ¨¡å¼ï¼šè¯¢é—®ç”¨æˆ·é€‰æ‹©
                approach_choice = interactive_select(
                    "Content processing method:",
                    ["Direct use (may exceed model limit)", "Smart summary (recommended)", "Manual truncate"]
                )
            else:
                # é»˜è®¤æ¨¡å¼ï¼šè‡ªåŠ¨é€‰æ‹©ç¬¬ä¸€ä¸ªé€‰é¡¹
                print("Content processing method:")
                print("  1. Direct use (may exceed model limit)")
                print("  2. Smart summary (recommended)")
                print("  3. Manual truncate")
                print("Choose (1-3, default: 1): 1")
                print("Selected: Direct use (may exceed model limit)")
                approach_choice = 0  # å¯¹åº”ç¬¬ä¸€ä¸ªé€‰é¡¹
            
            if approach_choice == 1:  # Smart summary
                print("ğŸ“ Generating paper summary...")
                # Generate summary prompt
                summary_prompt = f"""Please generate a detailed summary of the following academic paper, preserving key technical details:

{paper_content[:20000]}

Please include:
1. Research background and problem
2. Main methods and techniques
3. Key innovations
4. Experimental results
5. Conclusion and significance

The summary should be detailed but concise, suitable for subsequent tutorial creation."""
                
                # Call API for summary
                summary_response, summary_token_info, _ = call_openrouter_with_retry(
                    summary_prompt, 
                    params.get("selected_model", "deepseek/deepseek-r1:free"), 
                    params.get("max_tokens", 4000), 
                    "è®ºæ–‡æ‘˜è¦ç”Ÿæˆ",
                    params=params
                )
                
                if summary_response:
                    paper_content = summary_response
                    print(f"Summary generated ({count_tokens(paper_content)} tokens)")
                else:
                    print("Error:  Summary generation failed, using original content")
                    
            elif approach_choice == 2:  # Manual truncate
                paper_content = paper_content[:60000]  # Keep first 60k characters
                print(f"Truncated first part of content ({count_tokens(paper_content)} tokens)")
        
        # Update params with processed content
        params['paper_content'] = paper_content
        
        # Generate brainstorming prompt for paper
        return f"""Please analyze the content of this academic paper, prepare for tutorial creation:

Paper path: {paper_path}
Learning level: {mode}
Explanation style: {style}

Paper content:
{paper_content}

Please analyze:
1. The core contributions and innovations of the paper
2. Key concepts and technical methods
3. Focused content suitable for {mode} level learners
4. Possible difficulties and misconceptions
5. Practical applications and extended thinking

Please provide structured analysis, prepare for subsequent detailed tutorial creation."""
    
    return None


def call_openrouter_for_structure(prompt, model=None, max_tokens=None, retry_count=0, temperature=None, api_key=None):
    """Call OpenRouter API for structure generation with improved error handling."""
    import time
    import json
    import re
    
    try:
        script_dir = Path(__file__).parent
        run_path = script_dir / "RUN.py"
        
        if retry_count == 0:
            print("ğŸ”„ Connecting to OpenRouter API...", file=sys.stderr)
        else:
            print(f"ğŸ”„ Retrying API call (attempt {retry_count})...", file=sys.stderr)
            
        # å¤„ç†æ¨¡å‹é€‰æ‹©
        if not model or model == "auto":
            print("ğŸ¤– Using auto model selection", file=sys.stderr)
            # ä½¿ç”¨call_openrouter_with_auto_modelè¿›è¡Œè‡ªåŠ¨é€‰æ‹©
            result = call_openrouter_with_auto_model(prompt, model="auto")
            
            if result['success']:
                content = result['content']
                usage_info = {
                    'input_tokens': result['usage']['input_tokens'],
                    'output_tokens': result['usage']['output_tokens'],
                    'total_tokens': result['usage']['total_tokens'],
                    'cost': result['cost'],
                    'model': result['model'],
                    'api_duration': 0  # call_openrouter_with_auto_model doesn't return duration
                }
                return content, usage_info
            else:
                return f"ERROR: {result['error']}", {"error": result['error']}
        
        else:
            # ä½¿ç”¨æŒ‡å®šæ¨¡å‹
            print(f"ğŸ¤– Using model: {model}", file=sys.stderr)
            if max_tokens:
                print(f"ğŸ”¢ Maximum tokens: {max_tokens}", file=sys.stderr)
            print("â³ Please wait ...", file=sys.stderr)
            
            # è®°å½•å¼€å§‹æ—¶é—´
            start_time = time.time()
            
            # æ„å»ºå‘½ä»¤ - ä½¿ç”¨RUN --showè°ƒç”¨OPENROUTERå·¥å…·ï¼Œé€šè¿‡stdinä¼ é€’prompt
            cmd = [sys.executable, str(run_path), "--show", "OPENROUTER"]
            
            if model:
                cmd.extend(["--model", model])
            
            # ä¼ å…¥max-tokenså‚æ•°ï¼ˆOPENROUTERå·¥å…·ä¼šè‡ªåŠ¨å¤„ç†åŠ¨æ€è°ƒæ•´ï¼‰
            if max_tokens:
                cmd.extend(["--max-tokens", str(max_tokens)])
            
            # ä¼ å…¥temperatureå‚æ•°
            if temperature is not None:
                cmd.extend(["--temperature", str(temperature)])
            
            # ä¼ å…¥APIå¯†é’¥å‚æ•°
            if api_key:
                cmd.extend(["--key", api_key])
            
            # ä½¿ç”¨RUN --showæ¨¡å¼è°ƒç”¨OPENROUTERå·¥å…·ï¼Œé¿å…å“åº”è¢«æˆªæ–­
            try:
                result = subprocess.run(
                    cmd,
                    input=prompt,
                    text=True,
                    capture_output=True,
                    timeout=120,  # 2åˆ†é’Ÿè¶…æ—¶
                    encoding='utf-8'
                )
                
                # è®°å½•ç»“æŸæ—¶é—´
                end_time = time.time()
                api_duration = end_time - start_time
                
                # è§£æJSONå“åº”
                if result.returncode == 0:
                    try:
                        response_data = json.loads(result.stdout)
                        
                        if response_data.get('success'):
                            content = response_data.get('content', '')
                            
                            # æå–tokenä½¿ç”¨ä¿¡æ¯
                            usage = response_data.get('usage', {})
                            cost = response_data.get('cost', 0)
                            model_used = response_data.get('model', model)
                            
                            usage_info = {
                                'input_tokens': usage.get('input_tokens', 0),
                                'output_tokens': usage.get('output_tokens', 0),
                                'total_tokens': usage.get('total_tokens', 0),
                                'cost': cost,
                                'model': model_used,
                                'api_duration': api_duration
                            }
                            
                            print(f"OpenRouter API call successful (duration: {api_duration:.2f} seconds)", file=sys.stderr)
                            return content, usage_info
                        else:
                            error_msg = response_data.get('error', 'Unknown error')
                            print(f"Error: OpenRouter API returned error: {error_msg}", file=sys.stderr)
                            return f"ERROR: {error_msg}", {"error": error_msg}
                            
                    except json.JSONDecodeError as e:
                        print(f"Error: Parsing OpenRouter response failed: {e}", file=sys.stderr)
                        print(f"Original response: {result.stdout[:500]}...", file=sys.stderr)
                        return f"ERROR: JSON parsing failed: {e}", {"error": f"JSON parsing failed: {e}"}
                else:
                    error_msg = result.stderr or "Command execution failed"
                    print(f"Error: OpenRouter command execution failed: {error_msg}", file=sys.stderr)
                    return f"ERROR: {error_msg}", {"error": error_msg}
                    
            except subprocess.TimeoutExpired:
                print("Error:  OpenRouter API call timed out", file=sys.stderr)
                return "ERROR: API call timed out", {"error": "API call timed out"}
            except Exception as e:
                print(f"Error: OpenRouter API call exception: {e}", file=sys.stderr)
                return f"ERROR: {e}", {"error": str(e)}
        
    except Exception as e:
        print(f"Error: call_openrouter_for_structure exception: {e}", file=sys.stderr)
        return f"ERROR: {e}", {"error": str(e)}


def extract_response_data(response_data):
    """Extract response content and usage info from API response."""
    response_content = ""
    usage_info = {}
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯RUN --showçš„åŒ…è£…æ ¼å¼
    if 'output' in response_data:
        try:
            output_content = response_data['output']
            if output_content.strip().startswith('{'):
                # outputæ˜¯JSONæ ¼å¼
                import json
                inner_data = json.loads(output_content)
                if inner_data.get('success'):
                    response_content = inner_data.get('content', '')
                    usage = inner_data.get('usage', {})
                    usage_info = {
                        'input_tokens': usage.get('input_tokens', 0),
                        'output_tokens': usage.get('output_tokens', 0),
                        'total_tokens': usage.get('total_tokens', 0),
                        'cost': inner_data.get('cost', 0)
                    }
                else:
                    response_content = output_content
            else:
                # outputæ˜¯çº¯æ–‡æœ¬ï¼Œä½†æ£€æŸ¥æ˜¯å¦æœ‰RUN_DATA_FILE
                response_content = output_content
                # å°è¯•ä»RUN_DATA_FILEä¸­è¯»å–tokenä¿¡æ¯
                if '_RUN_DATA_file' in response_data:
                    try:
                        import json
                        with open(response_data['_RUN_DATA_file'], 'r', encoding='utf-8') as f:
                            run_data = json.load(f)
                            if 'usage' in run_data:
                                usage = run_data['usage']
                                usage_info = {
                                    'input_tokens': usage.get('input_tokens', 0),
                                    'output_tokens': usage.get('output_tokens', 0),
                                    'total_tokens': usage.get('total_tokens', 0),
                                    'cost': run_data.get('cost', 0)
                                }
                    except (FileNotFoundError, json.JSONDecodeError, KeyError):
                        pass
        except json.JSONDecodeError:
            # å¦‚æœè§£æå¤±è´¥ï¼Œç›´æ¥ä½¿ç”¨outputå†…å®¹
            response_content = response_data['output']
    else:
        # ç›´æ¥ä»response_dataä¸­æå–
        response_content = response_data.get('content', 
                          response_data.get('response', 
                          response_data.get('message', '')))
        usage = response_data.get('usage', {})
        usage_info = {
            'input_tokens': usage.get('input_tokens', 0),
            'output_tokens': usage.get('output_tokens', 0),
            'total_tokens': usage.get('total_tokens', 0),
            'cost': response_data.get('cost', 0)
        }
    
    return response_content, usage_info


def clean_markdown_wrapper(content):
    """Clean markdown code block wrapper if present."""
    if '```markdown' in content:
        # ä½¿ç”¨```markdownåˆ†å‰²å†…å®¹
        parts = content.split('```markdown')
        if len(parts) >= 2:
            # å–ç¬¬äºŒéƒ¨åˆ†ï¼ˆ```markdownä¹‹åçš„å†…å®¹ï¼‰
            markdown_content = parts[1]
            # ç§»é™¤æœ€åçš„```ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if '```' in markdown_content:
                markdown_content = markdown_content.split('```')[0]
            return markdown_content.strip()
    return content


def generate_tutorial_prompt(params, brainstorming_response):
    """Generate prompt for tutorial.md creation."""
    if params["type"] == "general":
        # General topic
        topic = params['topic']
        mode = params['mode']
        style = params['style']
        
        brainstorming_summary = brainstorming_response
        
        prompt = f"""Please create a concise tutorial.md file for "{topic}".

Learning mode: {mode}
Explanation style: {style}

Based on the following points, create a tutorial.md file:
{brainstorming_summary}

Please create a concise tutorial.md file, including:
1. Title and table of contents
2. Detailed explanation of 3-4 core concepts (including code examples, practice questions, etc., if applicable)
3. Simple learning path guidance
4. 2-3 practice exercise suggestions
5. Selected resource recommendations

Please ensure the content is suitable for {mode} level learners, and use {style} explanation style.
Please provide markdown format content, do not use any separators."""
        
    else:
        # Paper-based
        paper_path = params.get('paper_path', 'Paper')
        paper_content = params.get('paper_content', '')
        mode = params['mode']
        style = params['style']
        
        # Use brainstorming response if available, otherwise use paper content directly
        if brainstorming_response:
            content_base = f"""Brainstorming analysis result:
{brainstorming_response}

Original paper content (reference):
{paper_content[:5000]}{'...' if len(paper_content) > 5000 else ''}"""
        else:
            content_base = f"""Paper content:
{paper_content}"""
        
        prompt = f"""Please create a detailed tutorial.md file based on the academic paper content.

Paper: {paper_path}
Learning mode: {mode}
Explanation style: {style}

Based on the following content, create a tutorial.md file:
{content_base}

Please create a complete tutorial.md file, including:

1. **Paper overview**
   - Paper title, author, publication information
   - Research background and motivation
   - Main contributions and innovations

2. **Core concept explanation**
   - Definition and explanation of key concepts
   - Detailed explanation of technical methods
   - How the algorithm or model works

3. **Technical details**
   - Implementation methods and technicalè·¯çº¿
   - Experimental design and evaluation methods
   - Result analysis and discussion

4. **Learning points**
   - Summary of key points
   - Comparison with existing methods
   - Analysis of advantages and limitations

5. **Extended reading**
   - Recommended related papers
   - Further learning resources

Please ensure the content is suitable for {mode} level learners, and use {style} explanation style.
Please provide markdown format content, do not use any separators."""
    
    return prompt


def generate_question_prompt(params, tutorial_content):
    """Generate prompt for question.md creation."""
    if params["type"] == "general":
        # General topic
        topic = params['topic']
        mode = params['mode']
        style = params['style']
        
        tutorial_summary = tutorial_content
        
        prompt = f"""Please create a concise question.md file for "{topic}".

Learning mode: {mode}
Explanation style: {style}

Based on the following tutorial content, create a question.md file:
{tutorial_summary}

Please create a concise question.md file, including:
1. Basic knowledge test questions (3 questions)
2. Understanding application questions (3 questions)
3. Practice exercise questions (2 questions)
4. Thinking questions (2 questions)
5. Each question must have a clear and concise answer

Please use HTML's details/summary tags to create expandable answer regions.
Please ensure the questions are suitable for {mode} level learners.
Please control the content length, keep it concise."""
        
    else:
        # Paper-based
        paper_path = params.get('paper_path', 'Paper')
        mode = params['mode']
        style = params['style']
        
        tutorial_summary = tutorial_content
        
        prompt = f"""Please create a comprehensive question.md file based on the academic paper tutorial.

Paper: {paper_path}
Learning mode: {mode}
Explanation style: {style}

Based on the following tutorial content, create a question.md file:
{tutorial_summary}

Please create a comprehensive question.md file, including:

1. **Paper understanding questions (4 questions)**
   - Understanding the research background and motivation
   - Main contributions and innovations
   - Technical methods and principles
   - Experimental results and conclusions

2. **Concept explanation questions (3 questions)**
   - Key concept definition
   - Technical terminology explanation
   - Method comparison analysis

3. **Critical thinking questions (3 questions)**
   - Analysis of method advantages and disadvantages
   - Improvement suggestions and extensions
   - Application scenario discussion

4. **Practice application questions (2 questions)**
   - Practical application design
   - Implementation analysis

Each question must:
- Based on the specific content in the tutorial
- Have a detailed and accurate answer
- Use HTML's <details> and <summary> tags to implement answer folding

Format example:
### Question 1: What are the main contributions of this paper?
<details>
<summary>Click to view answer</summary>

[Detailed answer content...]

</details>

Please ensure the questions are suitable for {mode} level learners, and use {style} explanation style."""
    
    return prompt


def create_learning_files_from_responses(params, tutorial_response, question_response, prompts_and_responses=None):
    """Create learning files from separate tutorial and question responses."""
    log_progress("Start creating files", "FILE")
    output_dir = params['output_dir']
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    Path(output_dir).mkdir(parents=True, exist_ok=True)
    log_progress(f"Output directory prepared: {output_dir}", "FILE")
    
    try:
        # åˆ›å»ºtutorial.md
        log_progress("Create tutorial.md file", "FILE")
        tutorial_path = Path(output_dir) / "tutorial.md"
        with open(tutorial_path, 'w', encoding='utf-8') as f:
            f.write(tutorial_response)
        log_progress(f"tutorial.md created successfully: {tutorial_path}", "FILE")
        print(f"Create file: {tutorial_path}")
        
        # åˆ›å»ºquestion.md
        log_progress("Create question.md file", "FILE")
        question_path = Path(output_dir) / "question.md"
        with open(question_path, 'w', encoding='utf-8') as f:
            f.write(question_response)
        log_progress(f"question.md created successfully: {question_path}", "FILE")
        print(f"Create file: {question_path}")
        
        # åˆ›å»ºOPENROUTER_promptsæ–‡ä»¶å¤¹å¹¶ä¿å­˜promptså’Œresponses
        if prompts_and_responses:
            prompts_dir = Path(output_dir) / "OPENROUTER_prompts"
            prompts_dir.mkdir(exist_ok=True)
            
            for i, (prompt, response, token_info) in enumerate(prompts_and_responses, 1):
                # ä¿å­˜prompt
                prompt_path = prompts_dir / f"prompt_{i}.txt"
                with open(prompt_path, 'w', encoding='utf-8') as f:
                    f.write(prompt)
                    f.write(f"\n\n--- Openrouter API Call Info ---\n")
                    f.write(f"model: {token_info.get('model', 'unknown')}\n")
                    f.write(f"prompt_tokens: {token_info.get('prompt_tokens', 0)}\n")
                    f.write(f"completion_tokens: {token_info.get('completion_tokens', 0)}\n")
                    f.write(f"total_tokens: {token_info.get('total_tokens', 0)}\n")
                    f.write(f"cost: ${token_info.get('cost', 0):.6f}\n")
                    f.write(f"api_duration: {token_info.get('api_duration', 0):.2f} seconds\n")
                
                # ä¿å­˜response
                response_path = prompts_dir / f"response_{i}.txt"
                with open(response_path, 'w', encoding='utf-8') as f:
                    f.write(response)
                    f.write(f"\n\n--- Openrouter API Call Info ---\n")
                    f.write(f"model: {token_info.get('model', 'unknown')}\n")
                    f.write(f"prompt_tokens: {token_info.get('prompt_tokens', 0)}\n")
                    f.write(f"completion_tokens: {token_info.get('completion_tokens', 0)}\n")
                    f.write(f"total_tokens: {token_info.get('total_tokens', 0)}\n")
                    f.write(f"cost: ${token_info.get('cost', 0):.6f}\n")
                    f.write(f"api_duration: {token_info.get('api_duration', 0):.2f} seconds\n")
                
                print(f"Save prompt and response: {prompt_path.name}, {response_path.name}")
                model_used = token_info.get('model', 'unknown')
                cost = token_info.get('cost', 0)
                print(f"ğŸ“Š Token usage: {token_info.get('total_tokens', 0)} tokens - Model: {model_used} - Cost: ${cost:.6f} - Duration: {token_info.get('api_duration', 0):.2f} seconds")
        
        file_count = 2 + (len(prompts_and_responses) * 2 if prompts_and_responses else 0)
        print(f"\nğŸ“ Created {file_count} files:")
        print(f"  - {tutorial_path}")
        print(f"  - {question_path}")
        if prompts_and_responses:
            print(f"  - OPENROUTER_prompts/ folder contains {len(prompts_and_responses)} groups of prompts and responses")
        
        return True
        
    except Exception as e:
        print(f"Error: Error creating files: {e}")
        return False


def call_openrouter_with_retry(prompt, model, max_tokens, step_name, max_retries=3, params=None):
    """Call OpenRouter API with retry mechanism and model switching."""
    log_progress(f"Start {step_name}", "API")
    current_model = model
    
    # æå–é¢å¤–å‚æ•°
    temperature = params.get('temperature') if params else None
    api_key = params.get('api_key') if params else None
    
    for attempt in range(max_retries):
        log_progress(f"{step_name} - Attempt {attempt + 1}", "API")
        response, token_info = call_openrouter_for_structure(prompt, current_model, max_tokens, attempt, temperature, api_key)
        
        # æ£€æŸ¥æ˜¯å¦æˆåŠŸï¼ˆä¸æ˜¯Noneä¸”ä¸æ˜¯é”™è¯¯ï¼‰
        if response is not None and not (isinstance(response, str) and response.startswith("ERROR:")):
            log_progress(f"{step_name} completed successfully - Using model: {current_model}", "API")
            return response, token_info, current_model
        
        log_progress(f"{step_name} failed (Attempt {attempt + 1}) - Error: {str(response)[:100]}...", "ERROR")
        print(f"Error: {step_name} failed (Attempt {attempt + 1})", file=sys.stderr)
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯429é”™è¯¯ï¼ˆé€Ÿç‡é™åˆ¶ï¼‰æˆ–å…¶ä»–é”™è¯¯éœ€è¦åˆ‡æ¢æ¨¡å‹
        if should_switch_model(response, attempt, max_retries):
            current_model = handle_model_switching(current_model, params, step_name)
            if not current_model:
                break
            
            # ç”¨æ–°æ¨¡å‹é‡è¯•
            response, token_info = call_openrouter_for_structure(prompt, current_model, max_tokens, 0, temperature, api_key)
            if response is not None and not (isinstance(response, str) and response.startswith("ERROR:")):
                return response, token_info, current_model
    
    return None, None, current_model


def should_switch_model(response, attempt, max_retries):
    """Determine if we should switch models based on error type and attempt."""
    if response and isinstance(response, str):
        # ç«‹å³åˆ‡æ¢çš„æƒ…å†µï¼š429é”™è¯¯
        if "429" in response or "rate-limited" in response:
            return True
        # æœ€åä¸€æ¬¡é‡è¯•æ—¶åˆ‡æ¢çš„æƒ…å†µï¼šå…¶ä»–é”™è¯¯
        if attempt == max_retries - 1:
            return True
    return False


def handle_model_switching(current_model, params, step_name):
    """Handle model switching logic."""
    # è·å–æ‰€æœ‰å¯ç”¨æ¨¡å‹
    all_models, model_details = get_openrouter_models()
    if not all_models:
        print("Error:  Unable to get model list", file=sys.stderr)
        return None
    
    # ç§»é™¤å½“å‰å¤±è´¥çš„æ¨¡å‹
    available_models = [m for m in all_models if m != current_model]
    if not available_models:
        print("Error:  No other available models", file=sys.stderr)
        return None
    
    # åˆ†ç±»æ¨¡å‹
    free_models = [m for m in available_models if ":free" in m]
    paid_models = [m for m in available_models if ":free" not in m]
    
    # é»˜è®¤æ¨¡å¼ï¼šè‡ªåŠ¨åˆ‡æ¢
    if params and not params.get('not_default', False):
        if current_model and ":free" in current_model and free_models:
            new_model = free_models[0]
            print(f"ğŸ”„ Automatically switch to next free model: {new_model}", file=sys.stderr)
            return new_model
        elif paid_models:
            new_model = paid_models[0]
            print(f"ğŸ”„ Automatically switch to paid model: {new_model}", file=sys.stderr)
            return new_model
    else:
        # äº¤äº’æ¨¡å¼ï¼šè®©ç”¨æˆ·é€‰æ‹©
        return interactive_model_selection(current_model, free_models, paid_models, step_name)
    
    return None


def interactive_model_selection(failed_model, free_models, paid_models, step_name):
    """Interactive model selection when switching models."""
    print(f"\nâš ï¸ Model '{failed_model}' call failed", file=sys.stderr)
    print("Available alternative models:", file=sys.stderr)
    
    all_available = []
    if free_models:
        print("Free models:", file=sys.stderr)
        for i, model_name in enumerate(free_models):
            print(f"  {len(all_available) + 1}. {model_name}", file=sys.stderr)
            all_available.append(model_name)
    
    if paid_models:
        print("Paid models:", file=sys.stderr)
        for i, model_name in enumerate(paid_models):
            print(f"  {len(all_available) + 1}. {model_name}", file=sys.stderr)
            all_available.append(model_name)
    
    try:
        choice = input(f"\nSelect model (1-{len(all_available)}) or press Enter to skip: ").strip()
        if choice and choice.isdigit():
            choice_idx = int(choice) - 1
            if 0 <= choice_idx < len(all_available):
                new_model = all_available[choice_idx]
                print(f"Switch to model: {new_model}", file=sys.stderr)
                return new_model
    except (KeyboardInterrupt, EOFError):
        print("\nUser cancelled operation", file=sys.stderr)
    
    return None


def generate_learning_content(params):
    """Generate learning content based on collected parameters."""
    log_progress("Start generating learning content", "MAIN")
    print("\nğŸ¤– Generating learning content structure...")
    
    # ç”¨äºä¿å­˜æ‰€æœ‰çš„promptså’Œresponsesï¼Œç°åœ¨åŒ…å«tokenä¿¡æ¯
    prompts_and_responses = []
    
    # For paper type, model selection might already be done in generate_content_structure_prompt
    if params["type"] == "paper" and params.get("selected_model"):
        selected_model = params["selected_model"]
        max_tokens = params["max_tokens"]
        log_progress(f"Using pre-selected model: {selected_model}", "MODEL")
        print(f"Using pre-selected model: {selected_model}")
    else:
        # è®©ç”¨æˆ·é€‰æ‹©æ¨¡å‹
        log_progress("Start model selection process", "MODEL")
        selected_model, max_tokens = select_openrouter_model(params)
        if not selected_model:
            log_progress("Model selection failed", "ERROR")
            print("Error:  No model selected")
            return None
        
        log_progress(f"Model selection completed: {selected_model}", "MODEL")
        # Store selected model info in params for later use
        params["selected_model"] = selected_model
        params["max_tokens"] = max_tokens
    
    # Step 1: Brainstorming (optional for papers)
    brainstorming_response = None
    brainstorming_token_info = None
    
    # æ£€æŸ¥æ˜¯å¦è·³è¿‡brainstormingï¼ˆåªæœ‰contextæ¨¡å¼æ‰è·³è¿‡ï¼‰
    if params.get("context_mode", False):
        log_progress("Skip brainstorming step (context mode)", "SKIP")
        print("\nâ­ï¸ Skip brainstorming step (--context mode)")
        # ç›´æ¥å‡†å¤‡è®ºæ–‡å†…å®¹ç”¨äºåç»­æ­¥éª¤
        if params["type"] == "paper":
            structure_prompt = generate_content_structure_prompt(params)
            if structure_prompt is None:
                print("Error:  Content preparation failed, cannot continue generating learning materials")
                return None
    else:
        print("\nğŸ“ Step 1: Ask AI for brainstorming...")
        structure_prompt = generate_content_structure_prompt(params)
        
        # Check if content preparation failed (e.g., PDF extraction failed)
        if structure_prompt is None and params["type"] == "paper":
            print("Error:  Content preparation failed, cannot continue generating learning materials")
            return None
    
    if structure_prompt and not params.get("context_mode", False):  # Brainstorming was requested
        log_progress("Start brainstorming step", "STEP")
        print("Query content:")
        print("-" * 40)
        print(structure_prompt[:500] + "..." if len(structure_prompt) > 500 else structure_prompt)
        print("-" * 40)
        
        # Call OpenRouter API for brainstorming with retry
        brainstorming_response, brainstorming_token_info, current_model = call_openrouter_with_retry(
            structure_prompt, selected_model, max_tokens, "å¤´è„‘é£æš´", params=params
        )
        
        if brainstorming_response is None:
            log_progress("Brainstorming step failed", "ERROR")
            print("Error:  Brainstorming failed")
            return None
        
        log_progress("Brainstorming step completed", "STEP")
        # ä¿å­˜ç¬¬ä¸€ç»„promptå’Œresponse
        prompts_and_responses.append((structure_prompt, brainstorming_response, brainstorming_token_info))
        
        # å¦‚æœæ˜¯brainstorm_onlyæ¨¡å¼ï¼Œåªè¿”å›brainstormingç»“æœ
        if params.get("brainstorm_only", False):
            log_progress("Only brainstorming mode, return results", "COMPLETE")
            print("\nğŸ“‹ Brainstorming completed! Here are the generated structure suggestions:")
            print("=" * 60)
            print(brainstorming_response)
            print("=" * 60)
            print("\nğŸ’¡ You can manually create tutorial.md and question.md files based on the above suggestions")
            return {
                'brainstorming_response': brainstorming_response,
                'prompts_and_responses': prompts_and_responses
            }
    else:
        print("â­ï¸ Skip brainstorming, directly generate tutorial")
        
        # For paper type without brainstorming, check if we should continue
        if params["type"] == "paper":
            creation_mode = determine_creation_mode(params, selected_model)
            if creation_mode == "manual":
                params["brainstorm_only"] = True
    
    # Step 2: Generate tutorial.md
    log_progress("Start generating tutorial.md", "STEP")
    print("\nğŸ“ Step 2: Generate tutorial.md based on content...")
    tutorial_prompt = generate_tutorial_prompt(params, brainstorming_response)
    
    print("Query content:")
    print("-" * 40)
    print(tutorial_prompt[:500] + "..." if len(tutorial_prompt) > 500 else tutorial_prompt)
    print("-" * 40)
    
    tutorial_response, tutorial_token_info, current_model = call_openrouter_with_retry(
        tutorial_prompt, selected_model, max_tokens, "tutorial.mdç”Ÿæˆ", params=params
    )
    
    if tutorial_response is None:
        log_progress("tutorial.md generation failed", "ERROR")
        print("Error:  tutorial.md generation failed")
        return None
    
    log_progress("tutorial.md generation completed", "STEP")
    # ä¿å­˜ç¬¬äºŒç»„promptå’Œresponse
    prompts_and_responses.append((tutorial_prompt, tutorial_response, tutorial_token_info))
    
    # Step 3: Generate question.md
    log_progress("Start generating question.md", "STEP")
    print("\nğŸ“ Step 3: Generate question.md based on tutorial.md...")
    question_prompt = generate_question_prompt(params, tutorial_response)
    
    print("Query content:")
    print("-" * 40)
    print(question_prompt[:500] + "..." if len(question_prompt) > 500 else question_prompt)
    print("-" * 40)
    
    question_response, question_token_info, current_model = call_openrouter_with_retry(
        question_prompt, selected_model, max_tokens, "question.mdç”Ÿæˆ", params=params
    )
    
    if question_response is None:
        log_progress("question.md generation failed", "ERROR")
        print("Error:  question.md generation failed")
        return None
    
    log_progress("question.md generation completed", "STEP")
    # ä¿å­˜ç¬¬ä¸‰ç»„promptå’Œresponse
    prompts_and_responses.append((question_prompt, question_response, question_token_info))
    
    log_progress("All content generation completed", "COMPLETE")
    # è¿”å›æ‰€æœ‰ç”Ÿæˆçš„å†…å®¹
    return {
        'tutorial_response': tutorial_response,
        'question_response': question_response,
        'brainstorming_response': brainstorming_response,
        'prompts_and_responses': prompts_and_responses
    }


def determine_creation_mode(params, selected_model):
    """Determine creation mode for paper type without brainstorming."""
    # Auto-proceed in default mode or with free models
    if not params.get('not_default', False):
        print("ğŸš€ Default mode: automatically select creation mode...")
        return "auto"
    
    # Check if using free model
    if selected_model:
        models, model_details = get_openrouter_models()
        if models:
            details = model_details.get(selected_model, {})
            is_free_model = details.get('input_cost_per_1m', 0) == 0
            if is_free_model:
                print("ğŸš€ Free model: automatically select creation mode...")
                return "auto"
    
    # Ask user about creation mode
    print("\nğŸ¯ Select creation mode:")
    creation_choice = interactive_select(
        "Creation mode:", 
        ["Auto create (AI generates 3 times)", "Manual create (AI generates 1 time, you create the file)"]
    )
    
    return "manual" if creation_choice == 1 else "auto"


def count_tokens(text):
    """Simple token counting approximation."""
    # Rough approximation: 1 token â‰ˆ 4 characters for Chinese/English mixed text
    return len(text) // 4


def prepare_paper_content(params):
    """Prepare paper content based on input type."""
    input_type = params.get("input_type", 1)
    paper_content = None
    paper_path = None
    
    if input_type == 0:  # Markdown file
        paper_content = params.get("paper_content")
        paper_path = params.get("paper_path")
        print("Using provided Markdown content")
        
    elif input_type == 1:  # PDF file
        paper_path = params.get("paper_path")
        read_images = params.get("read_images", False)
        paper_content, processed_path = process_paper_with_extract_pdf(paper_path, read_images)
        if processed_path:
            paper_path = processed_path
            
    elif input_type == 2:  # URL
        paper_url = params.get("paper_url")
        print(f"ğŸ“¥ Download paper: {paper_url}")
        
        # Extract filename from URL or use generic name
        import urllib.parse
        parsed_url = urllib.parse.urlparse(paper_url)
        filename = Path(parsed_url.path).name or "downloaded_paper.pdf"
        
        downloaded_path, title = download_paper(paper_url, filename.replace('.pdf', ''), 
                                               output_dir=params.get('output_dir'))
        if downloaded_path:
            read_images = params.get("read_images", False)
            paper_content, processed_path = process_paper_with_extract_pdf(downloaded_path, read_images)
            if processed_path:
                paper_path = processed_path
        else:
            print("Error:  Unable to download paper")
            return None, None, 0
            
    elif input_type == 3:  # Description/Search
        paper_description = params.get("paper_description")
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºcontextæ¨¡å¼ï¼ˆåŒ…æ‹¬æ–‡ä»¶å¼•ç”¨æˆ–æ‰‹åŠ¨å¯ç”¨ï¼‰
        if params.get("context_mode", False):
            print("ğŸ“„ Context mode: directly use description content instead of searching for papers")
            # ç›´æ¥ä½¿ç”¨descriptionä¸­çš„å†…å®¹
            paper_content = paper_description
            paper_path = "context_content"
            # ä¼°ç®—tokenæ•°é‡
            token_count = len(paper_content) // 4  # ç²—ç•¥ä¼°ç®—
            print(f"Context content processed, content length: {token_count} tokens")
        else:
            paper_content, downloaded_path, token_count = search_and_download_paper(paper_description, params)
            if paper_content:
                print(f"Paper processed, content length: {token_count} tokens")
                paper_path = downloaded_path  # PDFè·¯å¾„
            else:
                print("Error:  Unable to find or download paper")
                return None, None, 0
    

    
    if not paper_content:
        print("Error:  Unable to get paper content")
        return None, None, 0
    
    # Count tokens
    token_count = count_tokens(paper_content)
    print(f"\nğŸ“Š Paper content statistics:")
    print(f"   Character count: {len(paper_content):,}")
    print(f"   Estimated token count: {token_count:,}")
    
    return paper_content, paper_path, token_count


def call_openrouter_with_auto_model(prompt, model="auto", max_retries=3):
    """
    è°ƒç”¨OPENROUTER APIï¼Œæ”¯æŒè‡ªåŠ¨æ¨¡å‹é€‰æ‹©
    
    Args:
        prompt: æç¤ºè¯
        model: æ¨¡å‹IDï¼Œ"auto"è¡¨ç¤ºè‡ªåŠ¨é€‰æ‹©
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        
    Returns:
        APIè°ƒç”¨ç»“æœ
    """
    try:
        from OPENROUTER import call_openrouter_api, get_useable_models
        
        if model == "auto":
            # è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨ï¼ŒæŒ‰ä¼˜å…ˆçº§æ’åº
            useable_models = get_useable_models()
            if not useable_models:
                print("Error:  No useable models")
                return {"success": False, "error": "No useable models available"}
            
            # å°è¯•æŒ‰é¡ºåºè°ƒç”¨æ¨¡å‹
            for i, model_id in enumerate(useable_models):
                print(f"ğŸ¤– Try model {i+1}/{len(useable_models)}: {model_id}")
                
                try:
                    result = call_openrouter_api(prompt, model=model_id)
                    if result['success']:
                        print(f"Model {model_id} call successful")
                        return result
                    else:
                        print(f"Warning: Model {model_id} call failed: {result.get('error', 'Unknown error')}")
                        if i < len(useable_models) - 1:  # ä¸æ˜¯æœ€åä¸€ä¸ªæ¨¡å‹
                            print(f"ğŸ”„ Try next model...")
                            continue
                        
                except Exception as e:
                    print(f"Warning: Model {model_id} call exception: {e}")
                    if i < len(useable_models) - 1:
                        print(f"ğŸ”„ Try next model...")
                        continue
            
            # æ‰€æœ‰æ¨¡å‹éƒ½å¤±è´¥äº†
            return {"success": False, "error": "All models failed"}
        
        else:
            # ä½¿ç”¨æŒ‡å®šæ¨¡å‹
            print(f"ğŸ¯ Use specified model: {model}")
            return call_openrouter_api(prompt, model=model)
            
    except Exception as e:
        return {"success": False, "error": f"APIè°ƒç”¨å¼‚å¸¸: {e}"}


def optimize_search_query_with_ai(user_description):
    """ä½¿ç”¨AIä¼˜åŒ–æœç´¢æŸ¥è¯¢ï¼Œå°†ç”¨æˆ·æè¿°è½¬æ¢ä¸ºæ›´å¥½çš„è‹±æ–‡æœç´¢è¯"""
    try:
        prompt = f"""You are an academic search expert. The user wants to search for papers on the following topic:

User description: {user_description}

Please help optimize this search query and generate 2-4 best English search keywords or phrases for searching related papers in academic databases.

Requirements:
1. Use English keywords or phrases
2. Each keyword/phrase is no more than 3 words
3. Total vocabulary is no more than 10 words
4. Include core technical terms to ensure precise matching
5. Avoid too broad vocabulary (e.g. "machine learning" alone)
6. Suitable for searching in arXiv, Google Scholar, etc.
7. Prioritize specific algorithm names or technical terms

Please only return search keywords, separated by commas, no other explanation.

For example:
- If the user says "3DGS mesh reconstruction", return: "3D Gaussian Splatting, mesh reconstruction, neural rendering"
- If the user says "machine learning optimization algorithm", return: "gradient descent optimization, SGD algorithms, optimization methods"
- If the user says "Hong Kong environmental movement", return: "Hong Kong environmental policy, sustainability initiatives, urban environmental management"

Search keywords: """

        print("ğŸ¤– Calling OpenRouter to optimize search query...")
        result = call_openrouter_with_auto_model(prompt, model="auto")
        
        if result['success']:
            optimized_query = result['content'].strip()
            print(f"Optimized search keywords: {optimized_query}")
            return optimized_query
        else:
            print(f"Warning: AI optimization failed, using original description: {result['error']}")
            return user_description
            
    except Exception as e:
        print(f"Warning: AI optimization failed, using original description: {e}")
        return user_description


def recommend_search_engines_with_ai(user_description, optimized_query):
    """ä½¿ç”¨AIæ¨èæœ€é€‚åˆçš„è®ºæ–‡æœç´¢å¼•æ“"""
    try:
        prompt = f"""You are an academic search expert. Please recommend the most suitable paper search platform based on the user's search requirements.

User description: {user_description}
Optimized search keywords: {optimized_query}

Available search platforms:
1. arxiv - Mainly includes preprint papers in computer science, physics, mathematics, statistics, etc.
2. google_scholar - Covers all academic fields, including published journal papers, conference papers, and thesis papers.

Please recommend the most suitable search platform based on the user's search requirements:

- If the user is searching for computer science, AI, machine learning, deep learning, physics, mathematics, etc., recommend: arxiv,google_scholar
- If the user is searching for optimization algorithms, machine learning algorithms, specific algorithm research, etc., recommend: google_scholar (Google Scholar has more algorithm papers)
- If the user is searching for social science, environmental policy, economics, management, medicine, etc., recommend: google_scholar
- If the user is searching for cross-disciplinary or uncertain fields, recommend: arxiv,google_scholar

Please only return the recommended search engines, separated by commas, no other explanation.
For example: arxiv,google_scholar or google_scholar

Recommended search engines: """

        print("ğŸ¤– Calling OpenRouter to recommend the most suitable search engines...")
        result = call_openrouter_with_auto_model(prompt, model="auto")
        
        if result['success']:
            recommended_sources = result['content'].strip()
            print(f"Recommended search engines: {recommended_sources}")
            return recommended_sources
        else:
            print(f"Warning: Search engine recommendation failed, using default: arxiv,google_scholar")
            return "arxiv,google_scholar"
            
    except Exception as e:
        print(f"Warning: Search engine recommendation failed, using default: {e}")
        return "arxiv,google_scholar"


def select_best_papers_with_ai(search_results, user_description, max_papers=3, negative_prompt=None):
    """ä½¿ç”¨AIä»æœç´¢ç»“æœä¸­é€‰æ‹©æœ€ç›¸å…³çš„è®ºæ–‡"""
    try:
        # å‡†å¤‡è®ºæ–‡ä¿¡æ¯
        papers_info = []
        for i, paper in enumerate(search_results[:10]):  # æœ€å¤šåˆ†æå‰10ç¯‡
            info = f"""Paper {i+1}:
Title: {paper.get('title', 'Unknown')}
Authors: {', '.join(paper.get('authors', [])[:3])}
Abstract: {paper.get('abstract', 'No abstract')[:300]}...
Published time: {paper.get('published', 'Unknown')}
Citation count: {paper.get('citation_count', 'Unknown')}
Source: {paper.get('source', 'Unknown')}
"""
            papers_info.append(info)
        
        papers_text = '\n\n'.join(papers_info)
        
        # æ„å»ºåŸºç¡€prompt
        prompt = f"""You are an academic research expert. The user is looking for papers on the following topic:

User requirements: {user_description}

Here is the list of papers found:

{papers_text}

Please select the most relevant and valuable {max_papers} papers from these papers, considering the following factors:
1. Direct relevance to user requirements (must have clear topic association)
2. Quality and impact of the paper (citation count, publication time, etc.)
3. Novelty and importance of the research

**Screening criteria**ï¼š
- The title and abstract of the paper must contain core concepts or technical terms related to the user's requirements
- For technical topics, the paper should involve the same or related algorithms, methods, or technical fields
- Completely irrelevant papers (e.g. movie generation vs optimization algorithm, antenna design vs machine learning) should be excluded
- If no truly relevant papers are found, please be honest and say "No relevant papers"

**Example**ï¼š
- User requirements "machine learning optimization algorithm" â†’ Accept: gradient descent, SGD, Adam optimizer related papers
- User requirements "machine learning optimization algorithm" â†’ Reject: movie generation, antenna design, medical imaging, etc. irrelevant papers"""

        # å¦‚æœæœ‰negative promptï¼Œæ·»åŠ åˆ°æŒ‡ä»¤ä¸­
        if negative_prompt:
            prompt += f"""

**Special attention**: Please avoid selecting papers related to the following descriptions: {negative_prompt}
Prioritize papers that are directly relevant to the user's requirements and do not contain the above unwanted content."""

        prompt += f"""

Please return the selected paper numbers (1-{len(papers_info)}), separated by commas.
For example: if you select the 1st, 3rd, and 5th papers, return: 1,3,5

Only return the numbers, no other explanation: """

        print("ğŸ¤– Calling OpenRouter to smartly select the best papers...")
        result = call_openrouter_with_auto_model(prompt, model="auto")
        
        if result['success']:
            selected_indices = result['content'].strip()
            print(f"AI recommended papers: {selected_indices}")
            
            # æ£€æŸ¥æ˜¯å¦AIè®¤ä¸ºæ²¡æœ‰ç›¸å…³è®ºæ–‡ - ä½†å¦‚æœç”¨æˆ·æ˜ç¡®è¦æ±‚åŸºäºè®ºæ–‡å­¦ä¹ ï¼Œåˆ™æ”¾å®½æ ‡å‡†
            strict_no_relevant_keywords = ['no relevant paper', 'none of the provided']
            if any(keyword in selected_indices.lower() for keyword in strict_no_relevant_keywords):
                # æ£€æŸ¥æ˜¯å¦æœ‰å¤‡é€‰æ¨èï¼ˆå³ä½¿AIè®¤ä¸ºä¸æ˜¯å¾ˆç›¸å…³ï¼‰
                if any(char.isdigit() for char in selected_indices):
                    print("âš ï¸ AI thinks the papers are not highly relevant, but still provides alternative recommendations, continue processing...")
                else:
                    print("Error:  AI judgment: no relevant papers found")
                    return []  # è¿”å›ç©ºåˆ—è¡¨è¡¨ç¤ºæ²¡æœ‰ç›¸å…³è®ºæ–‡
            
            # è§£æé€‰æ‹©çš„è®ºæ–‡ç¼–å· - æ”¹è¿›çš„è§£æé€»è¾‘
            try:
                # æ–¹æ³•1: å°è¯•ç›´æ¥è§£æï¼ˆé€‚ç”¨äºç®€æ´å›ç­”å¦‚ "1,3,5"ï¼‰
                if ',' in selected_indices and len(selected_indices.strip()) < 20:
                    indices = [int(x.strip()) - 1 for x in selected_indices.split(',')]
                    selected_papers = [search_results[i] for i in indices if 0 <= i < len(search_results)]
                    return selected_papers[:max_papers]
                
                # æ–¹æ³•2: ä»é•¿æ–‡æœ¬ä¸­æå–æ•°å­—ï¼ˆé€‚ç”¨äºè¯¦ç»†è§£é‡Šï¼‰
                import re
                
                # æŸ¥æ‰¾ "Final selection:" æˆ–ç±»ä¼¼æ¨¡å¼åçš„æ•°å­—
                final_selection_patterns = [
                    r'Final selection:\s*([0-9,\s]+)',
                    r'final selection:\s*([0-9,\s]+)', 
                    r'é€‰æ‹©:\s*([0-9,\s]+)',
                    r'æ¨è:\s*([0-9,\s]+)'
                ]
                
                for pattern in final_selection_patterns:
                    match = re.search(pattern, selected_indices, re.IGNORECASE)
                    if match:
                        numbers_str = match.group(1).strip()
                        indices = [int(x.strip()) - 1 for x in numbers_str.split(',') if x.strip().isdigit()]
                        if indices:
                            selected_papers = [search_results[i] for i in indices if 0 <= i < len(search_results)]
                            print(f"Extracted from detailed reply: {[i+1 for i in indices]}")
                            return selected_papers[:max_papers]
                
                # æ–¹æ³•3: æŸ¥æ‰¾æ‰€æœ‰æ•°å­—æ¨¡å¼ï¼ˆå¦‚ "8,6,9" æˆ– "8, 6, 9"ï¼‰
                number_patterns = re.findall(r'\b\d+(?:\s*,\s*\d+)+\b', selected_indices)
                if number_patterns:
                    # ä½¿ç”¨æœ€åä¸€ä¸ªæ‰¾åˆ°çš„æ•°å­—åºåˆ—ï¼ˆé€šå¸¸æ˜¯æœ€ç»ˆé€‰æ‹©ï¼‰
                    numbers_str = number_patterns[-1]
                    indices = [int(x.strip()) - 1 for x in numbers_str.split(',') if x.strip().isdigit()]
                    if indices:
                        selected_papers = [search_results[i] for i in indices if 0 <= i < len(search_results)]
                        print(f"Extracted from text: {[i+1 for i in indices]}")
                        return selected_papers[:max_papers]
                
                # å¦‚æœæ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥ï¼ŒæŠ›å‡ºå¼‚å¸¸è¿›å…¥fallbacké€»è¾‘
                raise ValueError("Failed to extract valid paper numbers from AI response")
                
            except (ValueError, IndexError) as e:
                print(f"Warning:  Failed to parse AI selection: {e}")
                # å¦‚æœè§£æå¤±è´¥ä¸”åŒ…å«"æ— ç›¸å…³"ç­‰å…³é”®è¯ï¼Œè¿”å›ç©ºåˆ—è¡¨
                if any(keyword in selected_indices for keyword in ['no relevant paper', 'no relevant', 'no relevant']):
                    print("Error:  No relevant papers found")
                    return []
                print(f"Return the first {max_papers} papers as backup")
                return search_results[:max_papers]
        else:
            print(f"Warning:  AI selection failed, return the first {max_papers} papers: {result['error']}")
            return search_results[:max_papers]
            
    except Exception as e:
        print(f"Warning:  AI selection failed, return the first {max_papers} papers: {e}")
        return search_results[:max_papers]


def process_paper_with_extract_pdf(paper_path, read_images=False):
    """ä½¿ç”¨EXTRACT_PDFå¤„ç†PDFæ–‡ä»¶ï¼Œè¿”å›å†…å®¹å’Œå¤„ç†åçš„è·¯å¾„"""
    try:
        import subprocess
        from pathlib import Path
        
        paper_path = Path(paper_path)
        if not paper_path.exists():
            print(f"Error: PDF file does not exist: {paper_path}")
            return None, None
        
        # ä½¿ç”¨EXTRACT_PDFå¤„ç†PDF
        extract_pdf_path = Path(__file__).parent / "EXTRACT_PDF.py"
        if not extract_pdf_path.exists():
            print("Error:  EXTRACT_PDF.py does not exist")
            return None, None
        
        print(f"ğŸ”„ Using EXTRACT_PDF to process: {paper_path.name}")
        
        # æ„å»ºå‘½ä»¤
        cmd = ["/usr/bin/python3", str(extract_pdf_path)]
        cmd.append(str(paper_path))
        
        if not read_images:
            cmd.extend(["--engine", "basic-asyn"])
        
        # æ‰§è¡Œå‘½ä»¤
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)
        
        if result.returncode != 0:
            print(f"Error: EXTRACT_PDF processing failed: {result.stderr}")
            return None, None
        
        # æŸ¥æ‰¾ç”Ÿæˆçš„markdownæ–‡ä»¶
        md_path = paper_path.with_suffix('.md')
        if md_path.exists():
            with open(md_path, 'r', encoding='utf-8') as f:
                content = f.read()
            print(f"PDF processing completed: {md_path.name}")
            return content, str(md_path)
        else:
            print("Error:  No generated markdown file found")
            return None, None
            
    except Exception as e:
        print(f"Error: Error processing PDF: {e}")
        return None, None


def search_and_download_paper(paper_description, params=None):
    """Search for paper and download if found."""
    print(f"\nğŸ” Searching for papers: {paper_description}")
    
    try:
        # æ­¥éª¤1: ä½¿ç”¨AIä¼˜åŒ–æœç´¢æŸ¥è¯¢
        print("ğŸ“ Step 1/10: Using AI to optimize search query...")
        optimized_query = optimize_search_query_with_ai(paper_description)
        
        # æ­¥éª¤2: æ¨èæœç´¢å¼•æ“ï¼ˆå¦‚æœç”¨æˆ·æ²¡æœ‰æŒ‡å®šï¼‰
        print("ğŸ” Step 2/10: Recommend search engines...")
        sources = params.get('sources') if params else None
        if not sources:
            sources = recommend_search_engines_with_ai(paper_description, optimized_query)
        else:
            print(f"Using user-specified search engines: {sources}")
        
        script_dir = Path(__file__).parent
        search_paper_path = script_dir / "SEARCH_PAPER"
        
        # æ­¥éª¤3: ä½¿ç”¨ä¼˜åŒ–åçš„æŸ¥è¯¢å’Œæ¨èçš„æœç´¢å¼•æ“æœç´¢è®ºæ–‡
        print("ğŸ” Step 3/10: Execute SEARCH_PAPER search...")
        
        # å¯¹äºæŠ€æœ¯ä¸»é¢˜ï¼Œå¢åŠ æœç´¢ç»“æœæ•°é‡ä»¥è·å¾—æ›´å¥½çš„åŒ¹é…
        max_results = 15 if any(keyword in paper_description.lower() 
                               for keyword in ['algorithm', 'optimization', 'machine learning', 'deep learning', 'neural']) else 10
        
        cmd = [str(search_paper_path), optimized_query, "--max-results", str(max_results)]
        if sources:
            cmd.extend(["--sources", sources])
        
        result = subprocess.run(cmd, capture_output=True, text=True)
        
        if result.returncode != 0:
            print(f"Error: Search failed: {result.stderr}")
            return None, None, 0
            
        print("SEARCH_PAPER search completed")
        
        # æ­¥éª¤4: è§£ææœç´¢ç»“æœ
        print("ğŸ“Š Step 4/10: Parse search results...")
        search_results = parse_search_results()
        if not search_results:
            print("Error:  No relevant papers found")
            return None, None, 0

        print(f"Found {len(search_results)} relevant papers")
        
        # æ­¥éª¤5: ä½¿ç”¨AIç­›é€‰æœ€ä½³è®ºæ–‡
        print("ğŸ¤– Step 5/10: Using AI to select the best papers...")
        selected_papers = select_best_papers_with_ai(
            search_results, 
            paper_description, 
            max_papers=3, 
            negative_prompt=params.get('negative_prompt') if params else None
        )
        
        if not selected_papers:
            print("Error:  No relevant papers found, cannot continue")
            return None, None, 0
        
        # æ­¥éª¤6: æ˜¾ç¤ºAIæ¨èçš„è®ºæ–‡ä¾›ç”¨æˆ·é€‰æ‹©
        print("AI selection completed")
        print(f"ğŸ“‹ Step 6/10: Display {len(selected_papers)} best papers recommended by AI:")
        for i, paper in enumerate(selected_papers):
            title = paper.get('title', 'Unknown')
            authors = paper.get('authors', [])
            author_str = ', '.join(authors[:3]) + ('...' if len(authors) > 3 else '')
            citation_count = paper.get('citation_count', 'Unknown')
            print(f"  {i+1}. {title}")
            print(f"     Authors: {author_str}")
            print(f"     Citation count: {citation_count}")
            print()
        
        # æ­¥éª¤7: è®©ç”¨æˆ·é€‰æ‹©æˆ–è‡ªåŠ¨é€‰æ‹©ç¬¬ä¸€ç¯‡
        print("ğŸ¯ Step 7/10: Select papers...")
        if len(selected_papers) == 1:
            selected_paper = selected_papers[0]
            print(f"Automatically select the only recommended paper")
        else:
            # ç®€åŒ–é€‰æ‹©ï¼šè‡ªåŠ¨é€‰æ‹©ç¬¬ä¸€ç¯‡ï¼ˆAIæ¨èçš„æœ€ä½³è®ºæ–‡ï¼‰
            selected_paper = selected_papers[0]
            print(f"Automatically select the best paper recommended by AI: {selected_paper.get('title', 'Unknown')}")

        # æ­¥éª¤8: å°è¯•ä¸‹è½½è®ºæ–‡
        print("ğŸ“¥ Step 8/10: Download papers...")
        pdf_url = selected_paper.get('pdf_url')
        if not pdf_url:
            print("Error:  No PDF download link found")
            return None, None, 0
        
        print(f"ğŸ“¥ Downloading paper: {selected_paper.get('title', 'Unknown')}")
        # ç¡®å®šä¸‹è½½ç›®å½•ï¼šæµ‹è¯•æ—¶ä½¿ç”¨/tmpï¼Œæ­£å¸¸ä½¿ç”¨æ—¶ä½¿ç”¨paramsä¸­çš„output_dir
        download_dir = None
        if params and params.get('output_dir'):
            # å¦‚æœoutput_diræ˜¯æµ‹è¯•ç›®å½•ï¼ˆåŒ…å«testã€tmpç­‰ï¼‰ï¼Œä½¿ç”¨/tmp
            output_dir_str = str(params.get('output_dir')).lower()
            if any(keyword in output_dir_str for keyword in ['test', 'tmp', '/tmp']):
                download_dir = '/tmp'
            else:
                download_dir = params.get('output_dir')
        
        downloaded_path, original_title = download_paper(
            pdf_url, 
            selected_paper.get('title', 'paper'),
            output_dir=download_dir
        )
        
        if not downloaded_path:
            print("Error:  Paper download failed")
            return None, None, 0
        
        # æ­¥éª¤9: ä½¿ç”¨AIç»™PDFé‡å‘½åä¸ºç®€æ´æ˜äº†çš„åå­—
        print("ğŸ¤– Step 9/10: Generate a simple and clear filename for the PDF...")
        new_filename = generate_simple_filename_with_ai(selected_paper, paper_description)
        
        # é‡å‘½åPDFæ–‡ä»¶
        downloaded_pdf_path = Path(downloaded_path)
        new_pdf_path = downloaded_pdf_path.parent / f"{new_filename}.pdf"
        
        try:
            downloaded_pdf_path.rename(new_pdf_path)
            print(f"PDF has been renamed: {new_pdf_path.name}")
            downloaded_path = str(new_pdf_path)
        except Exception as e:
            print(f"Warning:  Renaming failed, using original filename: {e}")
        
        # æ­¥éª¤10: ä½¿ç”¨EXTRACT_PDFæå–è®ºæ–‡å†…å®¹
        print("ğŸ“„ Step 10/10: Extract PDF content...")
        markdown_path = extract_pdf_content(downloaded_path, params)
        
        if not markdown_path:
            print("Error:  PDF content extraction failed")
            return None, None, 0
        
        # æ­¥éª¤11: è¯»å–æå–çš„markdownå†…å®¹
        print("ğŸ“– Step 11/11: Read extracted markdown content...")
        try:
            with open(markdown_path, 'r', encoding='utf-8') as f:
                paper_content = f.read()
            
            print(f"Paper content extraction completed: {markdown_path}")
            token_count = len(paper_content.split())  # ç®€å•çš„tokenä¼°ç®—
            print(f"ğŸ“Š Extracted content length: {token_count} tokens")
            
            # æ£€æŸ¥å†…å®¹é•¿åº¦ï¼Œå¦‚æœå¤ªå°‘å°±ä¸­æ–­
            min_content_length = 1000  # æœ€å°‘1000ä¸ªå­—ç¬¦
            if len(paper_content.strip()) < min_content_length:
                print(f"Error: Paper content is too short ({len(paper_content)} characters < {min_content_length}), possibly extraction failed")
                raise Exception(f"Paper content extraction incomplete: only {len(paper_content)} characters, less than the minimum requirement of {min_content_length} characters")
            
            return paper_content, downloaded_path, token_count
            
        except Exception as e:
            print(f"Error: Failed to read markdown file: {e}")
            return None, None, 0
            
    except Exception as e:
        print(f"Error: Search process error: {e}")
        return None, None, 0


def generate_simple_filename_with_ai(paper_info, user_description):
    """ä½¿ç”¨AIä¸ºè®ºæ–‡ç”Ÿæˆç®€æ´æ˜äº†çš„æ–‡ä»¶å"""
    try:
        title = paper_info.get('title', 'Unknown')
        authors = paper_info.get('authors', [])
        
        prompt = f"""Please generate a simple and clear English filename for the PDF file.

Paper information:
Title: {title}
Authors: {', '.join(authors[:3])}
User search description: {user_description}

Requirements:
1. The filename should be simple and clear, no more than 50 characters
2. Only use English letters, numbers, underscores, and hyphens
3. Avoid special symbols and spaces
4. Reflect the core theme of the paper
5. Easy to understand and identify

Examples:
- "3D Gaussian Splatting for Real-Time Radiance Field Rendering" -> "3DGS_Real_Time_Rendering"
- "Neural Radiance Fields" -> "NeRF"
- "Instant Neural Graphics Primitives" -> "InstantNGP"

Please only return the filename (without the .pdf extension), no other explanation: """

        result = call_openrouter_with_auto_model(prompt, model="auto")
        
        if result['success']:
            filename = result['content'].strip()
            # æ¸…ç†æ–‡ä»¶åï¼Œç¡®ä¿ç¬¦åˆæ–‡ä»¶ç³»ç»Ÿè¦æ±‚
            import re
            filename = re.sub(r'[^\w\-_]', '', filename)
            filename = re.sub(r'[-_]+', '_', filename)
            
            if len(filename) > 50:
                filename = filename[:50]
            
            print(f"AI generated filename: {filename}")
            return filename
        else:
            print(f"Warning:  AI generated filename failed: {result['error']}")
            # ä½¿ç”¨ç®€åŒ–çš„æ ‡é¢˜ä½œä¸ºå¤‡é€‰
            import re
            safe_title = re.sub(r'[^\w\s-]', '', title)
            safe_title = re.sub(r'[-\s]+', '_', safe_title)
            return safe_title[:30]
            
    except Exception as e:
        print(f"Warning:  Error generating filename: {e}")
        return "paper"


def extract_pdf_content(pdf_path, params=None):
    """ä½¿ç”¨EXTRACT_PDFæå–PDFå†…å®¹"""
    try:
        script_dir = Path(__file__).parent
        extract_pdf_path = script_dir / "EXTRACT_PDF"
        
        # æ„å»ºEXTRACT_PDFå‘½ä»¤
        cmd = [str(extract_pdf_path), str(pdf_path)]
        
        # æ ¹æ®LEARNå‚æ•°å†³å®šæ˜¯å¦å¤„ç†å›¾åƒ
        if params and params.get('read_images', False):
            print("ğŸ–¼ï¸  Enable image, formula, and table processing")
            cmd.extend(["--engine", "full"])  # ä½¿ç”¨fullæ¨¡å¼
        else:
            print("ğŸ“  Only extract text content (skip image processing)")
            cmd.extend(["--engine", "basic-asyn"])  # ä½¿ç”¨basic-asynæ¨¡å¼ï¼Œæ›´å¿«çš„å¼‚æ­¥å¤„ç†
        
        print(f"ğŸ”„  Executing command: {' '.join(cmd)}")
        
        # æ‰§è¡ŒEXTRACT_PDFå‘½ä»¤
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=86400)  # 1 day timeout (dummy)
        
        if result.returncode == 0:
            # æŸ¥æ‰¾ç”Ÿæˆçš„markdownæ–‡ä»¶
            pdf_path_obj = Path(pdf_path)
            expected_md_path = pdf_path_obj.with_suffix('.md')
            
            if expected_md_path.exists():
                print(f"PDF content extraction successful: {expected_md_path}")
                return str(expected_md_path)
            else:
                print(f"Error: No expected markdown file found: {expected_md_path}")
                # å°è¯•æŸ¥æ‰¾å…¶ä»–å¯èƒ½çš„markdownæ–‡ä»¶
                possible_paths = [
                    pdf_path_obj.parent / f"{pdf_path_obj.stem}.md",
                    Path.cwd() / f"{pdf_path_obj.stem}.md"
                ]
                for path in possible_paths:
                    if path.exists():
                        print(f"Found markdown file: {path}")
                        return str(path)
                return None
        else:
            print(f"Error: EXTRACT_PDF execution failed:")
            print(f"    Return code: {result.returncode}")
            print(f"    Standard output: {result.stdout}")
            print(f"    Error output: {result.stderr}")
            return None
            
    except subprocess.TimeoutExpired:
        print("Error:  PDF extraction timeout")
        return None
    except Exception as e:
        print(f"Error: PDF extraction error: {e}")
        return None


def parse_search_results():
    """Parse search results from SEARCH_PAPER_DATA."""
    try:
        script_dir = Path(__file__).parent
        search_data_dir = script_dir / "SEARCH_PAPER_DATA" / "results"
        
        if not search_data_dir.exists():
            return None
        
        # Get the most recent search results file
        result_files = list(search_data_dir.glob("search_results_*.json"))
        if not result_files:
            return None
        
        latest_file = max(result_files, key=lambda x: x.stat().st_mtime)
        
        import json
        with open(latest_file, 'r', encoding='utf-8') as f:
            search_results = json.load(f)
        
        # ç¡®ä¿è¿”å›çš„æ˜¯åˆ—è¡¨æ ¼å¼
        if isinstance(search_results, dict):
            # å¦‚æœæ˜¯å­—å…¸æ ¼å¼ï¼Œå°è¯•æå–è®ºæ–‡åˆ—è¡¨
            if 'papers' in search_results:
                search_results = search_results['papers']
            elif 'results' in search_results:
                search_results = search_results['results']
            else:
                # å¦‚æœå­—å…¸ä¸­æ²¡æœ‰æ˜ç¡®çš„è®ºæ–‡åˆ—è¡¨ï¼Œå°†æ•´ä¸ªå­—å…¸ä½œä¸ºå•ä¸ªç»“æœ
                search_results = [search_results]
        
        # ç¡®ä¿æ˜¯åˆ—è¡¨ä¸”ä¸ä¸ºç©º
        if isinstance(search_results, list) and search_results:
            return search_results
        else:
            return None
            
    except Exception as e:
        print(f"Error: Failed to parse search results: {e}")
        return None


def download_paper(pdf_url, paper_title, output_dir=None):
    """Download paper from URL."""
    try:
        script_dir = Path(__file__).parent
        download_path = script_dir / "DOWNLOAD"
        
        # Create a safe filename
        import re
        safe_title = re.sub(r'[^\w\s-]', '', paper_title)
        safe_title = re.sub(r'[-\s]+', '-', safe_title)
        filename = f"{safe_title}.pdf"
        
        # Determine download directory
        if output_dir:
            download_dir = Path(output_dir)
            download_dir.mkdir(parents=True, exist_ok=True)
        else:
            download_dir = Path.cwd()
        
        target_path = download_dir / filename
        
        # Try to download
        print(f"ğŸ“¥ Downloading: {pdf_url}")
        print(f"Target directory: {download_dir}")
        
        result = subprocess.run([
            str(download_path), pdf_url, str(target_path)
        ], capture_output=True, text=True)
        
        if result.returncode == 0:
            if target_path.exists():
                print(f"Download successful: {target_path}")
                return str(target_path), paper_title
            else:
                print("Error:  Downloaded file does not exist")
                return None, None
        else:
            print(f"Error: Download failed: {result.stderr}")
            print("ğŸ”„ Trying other download links...")
            return None, None
            
    except Exception as e:
        print(f"Error: Download process error: {e}")
        return None, None


def parse_file_references(text):
    """è§£ææ–‡æœ¬ä¸­çš„@"æ–‡ä»¶è·¯å¾„"å¼•ç”¨ï¼Œå±•å¼€ä¸ºæ–‡ä»¶å†…å®¹
    
    Returns:
        tuple: (expanded_text, has_file_reference)
    """
    import re
    from pathlib import Path
    
    # åŒ¹é… @"æ–‡ä»¶è·¯å¾„" æ¨¡å¼
    pattern = r'@"([^"]+)"'
    
    def clean_markdown_content(content, file_path):
        """æ¸…ç†markdownå†…å®¹ä¸­çš„placeholderå’Œæœ¬åœ°å›¾ç‰‡é“¾æ¥"""
        # ç§»é™¤å„ç§ç±»å‹çš„placeholder
        # [placeholder: xxx], [image: xxx], [formula: xxx], [table: xxx]
        content = re.sub(r'\[(?:placeholder|image|formula|table):\s*[^\]]*\]\s*\n?', '', content, flags=re.IGNORECASE)
        
        # ç§»é™¤åŒ…å«"placeholder"çš„æ•´è¡Œ
        lines = content.split('\n')
        cleaned_lines = []
        for line in lines:
            if '[placeholder:' not in line.lower() and '[image:' not in line.lower() and '[formula:' not in line.lower() and '[table:' not in line.lower() and '[formula:' not in line.lower():
                cleaned_lines.append(line)
        content = '\n'.join(cleaned_lines)
        
        # ç§»é™¤å›¾ç‰‡hash IDï¼ˆé€šå¸¸æ˜¯32-64ä½åå…­è¿›åˆ¶å­—ç¬¦ä¸²ï¼‰
        content = re.sub(r'\b[a-f0-9]{32,64}\b\s*\n?', '', content)
        
        # ç§»é™¤å›¾ç‰‡å¼•ç”¨ï¼ˆåŒ…å«hashçš„ï¼‰
        content = re.sub(r'!\[[^\]]*\]\([^)]*[a-f0-9]{32,64}[^)]*\)\s*\n?', '', content)
        
        # ç§»é™¤æœ¬åœ°å›¾ç‰‡å¼•ç”¨ ![...](images/xxx) æˆ– ![...](./images/xxx) ç­‰
        # ä¿ç•™ç½‘ç»œå›¾ç‰‡é“¾æ¥ (http/https)
        content = re.sub(r'!\[[^\]]*\]\((?!https?://)[^)]*\)\s*\n?', '', content)
        
        # ç§»é™¤é”™è¯¯ä¿¡æ¯å ä½ç¬¦
        content = re.sub(r'\[message:\s*[^\]]*\]\s*\n?', '', content, flags=re.IGNORECASE)
        
        # ç§»é™¤åŒ…å«ç‰¹å®šå…³é”®è¯çš„è¡Œï¼ˆæ›´å…¨é¢çš„æ¸…ç†ï¼‰
        forbidden_keywords = ['image_', 'formula_', 'table_', 'å›¾ç‰‡å¤„ç†å¤±è´¥', 'images/']
        lines = content.split('\n')
        cleaned_lines = []
        for line in lines:
            line_lower = line.lower()
            if not any(keyword.lower() in line_lower for keyword in forbidden_keywords):
                cleaned_lines.append(line)
        content = '\n'.join(cleaned_lines)
        
        # æ¸…ç†å¤šä½™çš„ç©ºè¡Œï¼ˆ3ä¸ªæˆ–æ›´å¤šè¿ç»­ç©ºè¡Œå‹ç¼©ä¸º2ä¸ªï¼‰
        content = re.sub(r'\n{3,}', '\n\n', content)
        
        # ç§»é™¤è¡Œé¦–å°¾ç©ºç™½ä½†ä¿ç•™æ®µè½ç»“æ„
        lines = content.split('\n')
        cleaned_lines = [line.rstrip() for line in lines]
        content = '\n'.join(cleaned_lines)
        
        return content.strip()
    
    def replace_reference(match):
        file_path = match.group(1)
        try:
            path_obj = Path(file_path).expanduser().resolve()
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not path_obj.exists():
                raise FileNotFoundError(f"File referenced by @ symbol does not exist: {file_path}")
                
            # æ£€æŸ¥æ˜¯å¦æ˜¯ç¬¦å·é“¾æ¥æˆ–å…¶ä»–ç‰¹æ®Šæƒ…å†µ
            if not path_obj.is_file():
                raise ValueError(f"Path referenced by @ symbol is not a valid file: {file_path}")
            
            # æ£€æŸ¥æ–‡ä»¶ç±»å‹
            allowed_extensions = {'.txt', '.md', '.pdf'}
            if path_obj.suffix.lower() not in allowed_extensions:
                return f"[Unsupported file type: {file_path}, only .txt, .md, and .pdf files are supported]"
            
            # è¯»å–æ–‡ä»¶å†…å®¹
            try:
                if path_obj.suffix.lower() == '.pdf':
                    # å¤„ç†PDFæ–‡ä»¶ - ä½¿ç”¨basicå¼•æ“è¿›è¡Œè§£æ
                    import tempfile
                    import subprocess
                    
                    print(f"ğŸ“ Parsing PDF file: {file_path} (using basic engine)")
                    
                    # åœ¨/tmpä¸­åˆ›å»ºä¸´æ—¶ç›®å½•è¿›è¡ŒPDFè§£æ
                    with tempfile.TemporaryDirectory(prefix='learn_pdf_', dir='/tmp') as temp_dir:
                        temp_dir_path = Path(temp_dir)
                        
                        # è°ƒç”¨EXTRACT_PDFè¿›è¡Œè§£æ
                        extract_cmd = [
                            'python3', str(Path(__file__).parent / 'EXTRACT_PDF.py'),
                            str(path_obj),
                            '--engine', 'basic-asyn',  # ä½¿ç”¨basicå¼•æ“ï¼Œä¸è¿›è¡Œå›¾åƒå¤„ç†
                            '--output', str(temp_dir_path)
                        ]
                        
                        try:
                            result = subprocess.run(extract_cmd, capture_output=True, text=True, timeout=60)
                            if result.returncode == 0:
                                # æŸ¥æ‰¾ç”Ÿæˆçš„markdownæ–‡ä»¶
                                md_files = list(temp_dir_path.glob('*.md'))
                                if md_files:
                                    md_file = md_files[0]
                                    with open(md_file, 'r', encoding='utf-8') as f:
                                        content = f.read()
                                    
                                    # æ¸…ç†PDFè§£æç”Ÿæˆçš„markdownå†…å®¹
                                    original_length = len(content)
                                    content = clean_markdown_content(content, file_path)
                                    cleaned_length = len(content)
                                    
                                    tokens_saved = (original_length - cleaned_length) // 4
                                    print(f"ğŸ“ PDF parsing completed: {file_path} ({cleaned_length} characters, cleaned and saved approximately {tokens_saved} tokens)")
                                    
                                    return f"\n\n--- Referenced PDF file: {file_path} ---\n{content}\n--- File reference end ---\n"
                                else:
                                    return f"[PDF parsing failed: {file_path} - No markdown file generated]"
                            else:
                                return f"[PDF parsing failed: {file_path} - {result.stderr}]"
                        except subprocess.TimeoutExpired:
                            return f"[PDF parsing timeout: {file_path}]"
                        except Exception as e:
                            return f"[PDF parsing error: {file_path} - {str(e)}]"
                
                else:
                    # å¤„ç†æ–‡æœ¬æ–‡ä»¶
                    with open(path_obj, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    # å¦‚æœæ˜¯markdownæ–‡ä»¶ï¼Œè¿›è¡Œæ™ºèƒ½æ¸…ç†
                    if path_obj.suffix.lower() == '.md':
                        original_length = len(content)
                        content = clean_markdown_content(content, file_path)
                        cleaned_length = len(content)
                        
                        if original_length > cleaned_length:
                            tokens_saved = (original_length - cleaned_length) // 4  # ç²—ç•¥ä¼°ç®—èŠ‚çœçš„tokens
                            print(f"ğŸ“ Expanding file reference: {file_path} ({cleaned_length} characters, cleaned and saved approximately {tokens_saved} tokens)")
                        else:
                            print(f"ğŸ“ Expanding file reference: {file_path} ({cleaned_length} characters)")
                    else:
                        print(f"ğŸ“ Expanding file reference: {file_path} ({len(content)} characters)")
                    
                    return f"\n\n--- Referenced file: {file_path} ---\n{content}\n--- File reference end ---\n"
                
            except (FileNotFoundError, ValueError):
                # é‡æ–°æŠ›å‡ºæ–‡ä»¶ä¸å­˜åœ¨æˆ–è·¯å¾„æ— æ•ˆçš„å¼‚å¸¸
                raise
            except Exception as e:
                return f"[Failed to read file: {file_path} - {str(e)}]"
                
        except (FileNotFoundError, ValueError):
            # é‡æ–°æŠ›å‡ºæ–‡ä»¶ä¸å­˜åœ¨æˆ–è·¯å¾„æ— æ•ˆçš„å¼‚å¸¸
            raise
        except Exception as e:
            return f"[File path parsing failed: {file_path} - {str(e)}]"
    
    # æ›¿æ¢æ‰€æœ‰æ–‡ä»¶å¼•ç”¨
    expanded_text = re.sub(pattern, replace_reference, text)
    
    # æ£€æŸ¥æ˜¯å¦æœ‰å¼•ç”¨è¢«å±•å¼€
    has_file_reference = expanded_text != text
    if has_file_reference:
        print("ğŸ”— Detected file reference, automatically expanded and cleaned useless content")
    
    return expanded_text, has_file_reference


def generate_learn_command(description):
    """æ ¹æ®ç”¨æˆ·æè¿°ç”ŸæˆLEARNå‘½ä»¤"""
    try:
        # è¯»å–LEARN.mdæ–‡æ¡£ä½œä¸ºå‚è€ƒ
        script_dir = Path(__file__).parent
        learn_md_path = script_dir / "LEARN.md"
        
        learn_doc = ""
        if learn_md_path.exists():
            with open(learn_md_path, 'r', encoding='utf-8') as f:
                learn_doc = f.read()
        
        # æ„å»ºprompt
        prompt = f"""You are an expert assistant for the LEARN tool. Please generate the corresponding LEARN command based on the user's description.

LEARNå·¥å…·æ–‡æ¡£ï¼š
{learn_doc}

ç”¨æˆ·æè¿°ï¼š{description}

Please analyze the user's needs and generate the most appropriate LEARN command. Consider the following factors:
1. Whether the user needs to learn specific papers, topics, or general knowledge
2. Learning level (beginner, intermediate, advanced, expert)
3. Explanation style (simple and clear, detailed and deep, rich examples, theoretical oriented)
4. Whether special options are needed (e.g., --file, --description, --negative, --read-images, etc.)
5. Output directory suggestion
6. OpenRouter model options:
   - --model: Specify a specific AI model
   - --max-tokens: Control output length
   - --temperature: Control creativity (0.0-2.0, the higher the value, the more creative)
   - --key: Use a specific API key

Please return the complete LEARN command starting with "LEARN", without any other explanation.
If a file path is needed, use placeholders like "/path/to/file".
If the user needs a creative response, add the --temperature parameter.
If the user needs to specify a model, add the --model parameter.

Example format:
LEARN -o ~/tutorials -m beginner -s simple and clear "Python basic programming"
LEARN -o ~/tutorials -m intermediate --file "/path/to/paper.pdf"
LEARN -o ~/tutorials -m advanced -d "Deep learning" --negative "GAN"
LEARN -o ~/tutorials -m expert -s rich examples --temperature 0.8 "Creative writing skills"
LEARN -o ~/tutorials -m intermediate --model "deepseek/deepseek-r1" --max-tokens 8000 "Machine learning algorithms"

Generated command: """

        print("ğŸ¤– Calling OpenRouter to analyze user needs and generate LEARN command...")
        result = call_openrouter_with_auto_model(prompt, model="auto")
        
        if result['success']:
            command = result['content'].strip()
            print(f"\nGenerated LEARN command:")
            print(f"```bash")
            print(f"{command}")
            print(f"```")
            return True
        else:
            print(f"Error: Command generation failed: {result['error']}")
            return False
            
    except Exception as e:
        print(f"Error: Error generating command: {e}")
        return False


def main():
    """Main function."""
    # å¯åŠ¨å…¨å±€è®¡æ—¶å™¨ - æ”¾åœ¨æœ€å¼€å§‹ï¼Œç¡®ä¿åœ¨ä»»ä½•OpenRouterè°ƒç”¨ä¹‹å‰
    start_timer()
    
    # è·å–command_identifier
    args = sys.argv[1:]
    command_identifier = None
    
    # æ£€æŸ¥æ˜¯å¦è¢«RUNè°ƒç”¨ï¼ˆç¬¬ä¸€ä¸ªå‚æ•°æ˜¯command_identifierï¼‰
    if args and is_run_environment(args[0]):
        command_identifier = args[0]
        args = args[1:]  # ç§»é™¤command_identifierï¼Œä¿ç•™å®é™…å‚æ•°
        # é‡æ–°æ„å»ºsys.argvä»¥ä¾›argparseä½¿ç”¨
        sys.argv = [sys.argv[0]] + args
    
    # Check if running in interactive mode (no arguments)
    if len(args) == 0:
        print("LEARN - Intelligent learning system")
        print("Starting interactive mode...")
        print()
        
        params = run_interactive_mode()
        if params is None:
            return 1
        
        # Generate learning content
        result = generate_learning_content(params)
        if result is None:
            return 1
        
        # å¦‚æœæ˜¯brainstorm_onlyæ¨¡å¼ï¼Œä¸åˆ›å»ºæ–‡ä»¶
        if params.get("brainstorm_only", False):
            print("Brainstorming completed!")
            return 0
        
        # åˆ›å»ºæ–‡ä»¶
        print("\nğŸ“ Creating tutorial files...")
        success = create_learning_files_from_responses(
            params, 
            result['tutorial_response'], 
            result['question_response'], 
            result['prompts_and_responses']
        )
        
        if success:
            print("File creation completed!")
            return 0
        else:
            print("Error:  File creation failed")
            return 1
    
    # Parse direct command
    try:
        # å…ˆæ£€æŸ¥æ˜¯å¦æ˜¯--helpæ¨¡å¼
        if '--help' in sys.argv or '-h' in sys.argv:
            parser = argparse.ArgumentParser(description='LEARN - Intelligent learning system')
            parser.add_argument('topic', nargs='?', help='Learning topic')
            parser.add_argument('-o', '--output-dir', help='Output directory')
            parser.add_argument('-m', '--mode', choices=list(MODE_MAPPING.keys()),
                               default='intermediate', help='Learning level (beginner, intermediate, advanced, expert)')
            parser.add_argument('-s', '--style', choices=list(STYLE_MAPPING.keys()),
                               default='detailed and deep', help='Explanation style (simple and clear, detailed and deep, rich examples, theoretical oriented)')
            parser.add_argument('--file', help='Directly process file path (supports PDF, MD, TXT)')
            parser.add_argument('-u', '--url', help='Paper URL')
            parser.add_argument('-d', '--description', help='Paper description/search keywords')
            parser.add_argument('--negative', help='Negative prompt: specify content or paper type you do not want')
            parser.add_argument('--read-images', action='store_true', help='Process images, formulas, and tables in PDF')
            parser.add_argument('--gen-command', help='Generate LEARN command based on description')
            parser.add_argument('--paper-based', action='store_true', help='Force use of paper-based learning mode, even if only a description is provided')
            parser.add_argument('--sources', help='Specify paper search engines, separated by commas (arxiv,google_scholar), default is automatic recommendation')
            parser.add_argument('--model', help='æŒ‡å®šOpenRouteræ¨¡å‹')
            parser.add_argument('--max-tokens', type=int, help='æœ€å¤§tokenæ•°')
            parser.add_argument('--temperature', type=float, help='æ¸©åº¦å‚æ•° (0.0-2.0ï¼Œæ§åˆ¶å›å¤çš„åˆ›é€ æ€§)')
            parser.add_argument('--key', help='æŒ‡å®šOpenRouter APIå¯†é’¥')
            parser.add_argument('--not-default', action='store_true', help='éé»˜è®¤æ¨¡å¼ï¼Œéœ€è¦ç”¨æˆ·ç¡®è®¤')
            parser.add_argument('--no-override-material', action='store_true', help='ä¸è¦†ç›–å·²å­˜åœ¨çš„æ–‡ä»¶ï¼Œè‡ªåŠ¨é‡å‘½å')
            parser.add_argument('--brainstorm-only', action='store_true', help='ä¸è‡ªåŠ¨åˆ›å»ºæ–‡ä»¶ï¼Œä»…ç”Ÿæˆå†…å®¹')
            parser.add_argument('--context', action='store_true', help='å°†descriptionè§†ä½œç›´æ¥contextè¿›å…¥brainstormingï¼Œè·³è¿‡è®ºæ–‡æœç´¢')
            
            # æ•è·helpè¾“å‡ºè€Œä¸æ˜¯è®©å®ƒexit
            import io
            from contextlib import redirect_stdout
            
            help_output = io.StringIO()
            try:
                with redirect_stdout(help_output):
                    parser.print_help()
                print(help_output.getvalue())
                return 0
            except:
                parser.print_help()
                return 0
        
        # å…ˆæ£€æŸ¥æ˜¯å¦æ˜¯gen-commandæ¨¡å¼
        elif '--gen-command' in sys.argv:
            parser = argparse.ArgumentParser(description='LEARN - æ™ºèƒ½å­¦ä¹ ç³»ç»Ÿ')
            parser.add_argument('--gen-command', help='æ ¹æ®æè¿°ç”ŸæˆLEARNå‘½ä»¤')
            
            # åªè§£ægen-commandå‚æ•°ï¼Œå¿½ç•¥å…¶ä»–å‚æ•°
            args, _ = parser.parse_known_args()
            
            if args.gen_command:
                success = generate_learn_command(args.gen_command)
                return 0 if success else 1
        
        params = parse_direct_command(sys.argv[1:])
        
        # æ£€æŸ¥å‚æ•°æ”¶é›†æ˜¯å¦æˆåŠŸ
        if not params:
            return 1
        
        # Generate learning content
        result = generate_learning_content(params)
        if result is None:
            return 1
        
        # å¦‚æœæ˜¯brainstorm_onlyæ¨¡å¼ï¼Œä¸åˆ›å»ºæ–‡ä»¶
        if params.get("brainstorm_only", False):
            print("Brainstorming completed!")
            return 0
        
        # åˆ›å»ºæ–‡ä»¶
        print("\nğŸ“ Creating tutorial files...")
        success = create_learning_files_from_responses(
            params, 
            result['tutorial_response'], 
            result['question_response'], 
            result['prompts_and_responses']
        )
        
        if success:
            print("File creation completed!")
            return 0
        else:
            print("Error:  File creation failed")
            return 1
    
    except Exception as e:
        print(f"Error: Error during runtime: {e}")
        return 1


if __name__ == "__main__":
    sys.exit(main())