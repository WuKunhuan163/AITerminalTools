#!/usr/bin/env python3
"""
Unit tests for LEARN tool
"""

import os
import sys
import tempfile
import json
import re
from pathlib import Path

# Add parent directory to path to import the module
sys.path.insert(0, str(Path(__file__).parent.parent))

from _UNITTEST.base_test import BaseTest, APITest, LongRunningTest


class TestLearn(BaseTest):
    """Test cases for LEARN tool"""

    def setUp(self):
        super().setUp()
        self.learn_script = self.get_bin_path('LEARN')
        self.learn_py = self.get_python_path('LEARN.py')

    def test_learn_script_exists(self):
        """Test that LEARN script exists"""
        self.assertTrue(self.learn_script.exists())

    def test_learn_py_exists(self):
        """Test that LEARN.py exists"""
        self.assertTrue(self.learn_py.exists())

    def test_learn_direct_mode_missing_output_dir(self):
        """Test LEARN direct mode requires output directory"""
        result = self.assertCommandFail([
            sys.executable, str(self.learn_py), 
            "Pythonç¼–ç¨‹", "--mode", "Advanced", "--style", "Witty"
        ])
        self.assertIn('required', result.stderr)

    def test_learn_help_output(self):
        """Test LEARN help output"""
        result = self.assertCommandSuccess([
            sys.executable, str(self.learn_py), "--help"
        ])
        self.assertIn('LEARN', result.stderr)


class TestLearnContentQuality(LongRunningTest):
    """Content quality tests for LEARN tool with different input types"""
    
    TEST_TIMEOUT = 360  # 6åˆ†é’Ÿè¶…æ—¶ï¼Œè¶³å¤Ÿå¤„ç†å¤æ‚çš„PDFå’Œæœç´¢ä»»åŠ¡
    
    def setUp(self):
        super().setUp()
        self.learn_py = self.get_python_path('LEARN.py')
        self.test_data_dir = Path(__file__).parent / "_DATA"
        self.test_md = self.test_data_dir / "extracted_paper.md"
        self.test_pdf = self.test_data_dir / "test_extract_paper.pdf"
    
    def _extract_keywords_from_content(self, content, expected_keywords):
        """æ£€æŸ¥å†…å®¹ä¸­æ˜¯å¦åŒ…å«æœŸæœ›çš„å…³é”®è¯ï¼Œæ”¯æŒé€šé…åŒ¹é…"""
        found_keywords = []
        missing_keywords = []
        
        content_lower = content.lower()
        
        for keyword_group in expected_keywords:
            # æ”¯æŒé€šé…æœºåˆ¶ï¼šå¦‚æœæ˜¯åˆ—è¡¨ï¼Œåˆ™ä¸ºå¤šé€‰ä¸€ï¼›å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œåˆ™ç²¾ç¡®åŒ¹é…
            if isinstance(keyword_group, list):
                # å¤šé€‰ä¸€åŒ¹é…
                found = False
                for variant in keyword_group:
                    if variant.lower() in content_lower:
                        found_keywords.append(f"{variant}(from {keyword_group})")
                        found = True
                        break
                if not found:
                    missing_keywords.append(f"Any of {keyword_group}")
            else:
                # ç²¾ç¡®åŒ¹é…
                if keyword_group.lower() in content_lower:
                    found_keywords.append(keyword_group)
                else:
                    missing_keywords.append(keyword_group)
                    
        return found_keywords, missing_keywords
    
    def _validate_tutorial_structure(self, content):
        """éªŒè¯æ•™ç¨‹çš„åŸºæœ¬ç»“æ„"""
        required_sections = ['#', '##', '###']  # è‡³å°‘è¦æœ‰æ ‡é¢˜ç»“æ„
        
        has_headers = any(section in content for section in required_sections)
        has_substantial_content = len(content) > 1000  # è‡³å°‘1000å­—ç¬¦
        # æ›´å®½æ¾çš„ç¤ºä¾‹æ£€æŸ¥ï¼ŒåŒ…æ‹¬æ›´å¤šå¯èƒ½çš„ç¤ºä¾‹æ ‡è®°
        example_markers = ['ä¾‹å¦‚', 'example', 'ç¤ºä¾‹', '```', 'ä¸¾ä¾‹', 'æ¯”å¦‚', 'å¦‚ï¼š', 'ä¾‹ï¼š', 
                          'Example', 'Instance', 'æ¡ˆä¾‹', 'å®ä¾‹', 'case', 'Case']
        has_examples = any(marker in content for marker in example_markers)
        
        return {
            'has_headers': has_headers,
            'has_substantial_content': has_substantial_content,
            'has_examples': has_examples,
            'content_length': len(content)
        }
    
    def _validate_questions_structure(self, content):
        """éªŒè¯é—®é¢˜çš„åŸºæœ¬ç»“æ„"""
        question_markers = ['?', 'ï¼Ÿ', 'é—®é¢˜', 'ç»ƒä¹ ', 'Question', 'Exercise']
        
        has_questions = any(marker in content for marker in question_markers)
        has_substantial_content = len(content) > 500  # è‡³å°‘500å­—ç¬¦
        
        # è®¡ç®—é—®é¢˜æ•°é‡ï¼ˆç®€å•ä¼°è®¡ï¼‰
        question_count = content.count('?') + content.count('ï¼Ÿ')
        
        return {
            'has_questions': has_questions,
            'has_substantial_content': has_substantial_content,
            'question_count': question_count,
            'content_length': len(content)
        }

    def test_01_markdown_input_quality(self):
        """æµ‹è¯•åŸºäºMarkdownæ–‡ä»¶è¾“å…¥çš„å†…å®¹è´¨é‡"""
        if not self.test_md.exists():
            self.skipTest("Test markdown file not found")
        
        # è¯»å–markdownå†…å®¹ç¡®å®šæœŸæœ›çš„å…³é”®è¯
        with open(self.test_md, 'r', encoding='utf-8') as f:
            md_content = f.read()
        
        # åŸºäºmarkdownå†…å®¹å®šä¹‰æœŸæœ›çš„å…³é”®è¯ï¼ˆ16ä¸ªï¼Œæ”¯æŒé€šé…åŒ¹é…ï¼‰
        expected_keywords = [
            ["GaussianObject", "Gaussian Object"],  # é€šé…ï¼šåŸåæˆ–åˆ†å¼€å†™
            ["3D", "ä¸‰ç»´", "ç«‹ä½“"],  # é€šé…ï¼šè‹±æ–‡æˆ–ä¸­æ–‡
            ["é‡å»º", "reconstruction", "é‡æ„"],  # é€šé…ï¼šä¸­è‹±æ–‡
            ["é«˜æ–¯", "Gaussian", "gauss"],  # é€šé…ï¼šä¸­è‹±æ–‡å’Œå°å†™
            ["Splatting", "splat", "æ³¼æº…"],  # é€šé…ï¼šåŸè¯æˆ–ç›¸å…³è¯
            ["è§†è§‰", "vision", "visual"],  # é€šé…ï¼šä¸­è‹±æ–‡
            ["æ·±åº¦å­¦ä¹ ", "deep learning", "æœºå™¨å­¦ä¹ "],  # é€šé…ï¼šç›¸å…³æ¦‚å¿µ
            ["è®¡ç®—æœºè§†è§‰", "computer vision", "CV"],  # é€šé…ï¼šå…¨ç§°æˆ–ç¼©å†™
            ["ç¥ç»ç½‘ç»œ", "neural network", "ç½‘ç»œ"],  # é€šé…ï¼šå…¨ç§°æˆ–ç®€ç§°
            ["ç®—æ³•", "algorithm", "æ–¹æ³•"],  # é€šé…ï¼šä¸­è‹±æ–‡
            ["æ¸²æŸ“", "render", "rendering"],  # é€šé…ï¼šä¸­è‹±æ–‡
            ["æ¨¡å‹", "model", "å»ºæ¨¡"],  # é€šé…ï¼šç›¸å…³è¯
            ["ä¼˜åŒ–", "optimization", "optimize"],  # é€šé…ï¼šåè¯æˆ–åŠ¨è¯
            ["å›¾åƒ", "image", "picture"],  # é€šé…ï¼šåŒä¹‰è¯
            ["è´¨é‡", "quality", "é«˜è´¨é‡"],  # é€šé…ï¼šç›¸å…³è¯
            ["æ•ˆæœ", "result", "performance"]  # é€šé…ï¼šæ•ˆæœç›¸å…³
        ]
        
        temp_base = Path("/tmp/test_learn_markdown")
        temp_base.mkdir(exist_ok=True)
        
        with tempfile.TemporaryDirectory(dir=temp_base) as temp_dir:
            # è¿è¡ŒLEARNç”Ÿæˆæ•™ç¨‹
            result = self.assertCommandSuccess([
                sys.executable, str(self.learn_py),
                "-o", temp_dir, "-m", "ä¸­çº§", "-s", "å®ä¾‹ä¸°å¯Œ",
                "--paper", str(self.test_md),
                "3Dé«˜æ–¯æº…å°„åŸºç¡€æ•™ç¨‹"
            ], timeout=180)  # 3åˆ†é’Ÿè¶…æ—¶ï¼Œç›´æ¥è¯»å–md
            
            # æ£€æŸ¥ç”Ÿæˆçš„æ–‡ä»¶
            tutorial_file = Path(temp_dir) / "tutorial.md"
            question_file = Path(temp_dir) / "question.md"
            
            self.assertTrue(tutorial_file.exists(), "Tutorial file not generated")
            self.assertTrue(question_file.exists(), "Question file not generated")
            
            # éªŒè¯æ•™ç¨‹å†…å®¹è´¨é‡
            with open(tutorial_file, 'r', encoding='utf-8') as f:
                tutorial_content = f.read()
            
            tutorial_validation = self._validate_tutorial_structure(tutorial_content)
            self.assertTrue(tutorial_validation['has_headers'], "Tutorial lacks proper header structure")
            self.assertTrue(tutorial_validation['has_substantial_content'], 
                          f"Tutorial too short: {tutorial_validation['content_length']} chars")
            
            # ç¤ºä¾‹æ£€æŸ¥æ”¹ä¸ºè­¦å‘Šï¼Œä¸ä½œä¸ºå¤±è´¥æ¡ä»¶
            if not tutorial_validation['has_examples']:
                print("âš ï¸  Warning: Tutorial may lack examples")
            
            # æ£€æŸ¥å…³é”®è¯è¦†ç›–
            found_keywords, missing_keywords = self._extract_keywords_from_content(
                tutorial_content, expected_keywords)
            
            coverage_ratio = len(found_keywords) / len(expected_keywords)
            
            # è¾“å‡ºè¯¦ç»†çš„å…³é”®è¯åˆ†æ
            print(f"ğŸ” å…³é”®è¯åˆ†æ:")
            print(f"   âœ… æ‰¾åˆ°çš„å…³é”®è¯ ({len(found_keywords)}/{len(expected_keywords)}): {found_keywords}")
            print(f"   âŒ ç¼ºå¤±çš„å…³é”®è¯: {missing_keywords}")
            
            self.assertGreaterEqual(coverage_ratio, 0.75,  # è¦æ±‚è‡³å°‘12/16 = 75%
                             f"Low keyword coverage: {coverage_ratio:.2f} ({len(found_keywords)}/{len(expected_keywords)})")
            
            # éªŒè¯é—®é¢˜å†…å®¹è´¨é‡
            with open(question_file, 'r', encoding='utf-8') as f:
                question_content = f.read()
            
            question_validation = self._validate_questions_structure(question_content)
            self.assertTrue(question_validation['has_questions'], "Questions file lacks question markers")
            self.assertTrue(question_validation['has_substantial_content'],
                          f"Questions too short: {question_validation['content_length']} chars")
            self.assertGreater(question_validation['question_count'], 5, 
                             f"Too few questions: {question_validation['question_count']}")
            
            print(f"âœ… Markdown test - Tutorial: {tutorial_validation['content_length']} chars, "
                  f"Questions: {question_validation['question_count']} questions, "
                  f"Keywords: {len(found_keywords)}/{len(expected_keywords)}")

    def test_02_pdf_input_quality(self):
        """æµ‹è¯•åŸºäºPDFæ–‡ä»¶è¾“å…¥çš„å†…å®¹è´¨é‡"""
        if not self.test_pdf.exists():
            self.skipTest("Test PDF file not found")
        
        # åŸºäºPDFé¢„æœŸå†…å®¹å®šä¹‰å…³é”®è¯ï¼ˆ16ä¸ªï¼Œæ”¯æŒé€šé…åŒ¹é…ï¼‰
        expected_keywords = [
            ["GaussianObject", "Gaussian Object"],
            ["3D", "ä¸‰ç»´", "ç«‹ä½“"],
            ["é‡å»º", "reconstruction", "é‡æ„"],
            ["é«˜æ–¯", "Gaussian", "gauss"],
            ["Splatting", "splat"],
            ["è®¡ç®—æœºè§†è§‰", "computer vision", "CV"],
            ["æ·±åº¦å­¦ä¹ ", "deep learning"],
            ["ç¥ç»ç½‘ç»œ", "neural network"],
            ["ç®—æ³•", "algorithm", "æ–¹æ³•"],
            ["æ¸²æŸ“", "render", "rendering"],
            ["æ¨¡å‹", "model", "å»ºæ¨¡"],
            ["ä¼˜åŒ–", "optimization"],
            ["å›¾åƒ", "image", "å›¾ç‰‡"],
            ["è§†å›¾", "view", "è§†è§’"],
            ["è´¨é‡", "quality", "é«˜è´¨é‡"],
            ["æ€§èƒ½", "performance", "æ•ˆæœ"]
        ]
        
        temp_base = Path("/tmp/test_learn_pdf")
        temp_base.mkdir(exist_ok=True)
        
        with tempfile.TemporaryDirectory(dir=temp_base) as temp_dir:
            # è¿è¡ŒLEARNç”Ÿæˆæ•™ç¨‹ï¼ˆä½¿ç”¨PDFè¾“å…¥ï¼‰
            result = self.assertCommandSuccess([
                sys.executable, str(self.learn_py),
                "-o", temp_dir, "-m", "åˆå­¦è€…", "-s", "è¯¦ç»†æ·±å…¥",
                "--pdf", str(self.test_pdf),
                "PDFè®ºæ–‡å­¦ä¹ æ•™ç¨‹"
            ], timeout=180)  # PDFå¤„ç†éœ€è¦æ›´é•¿æ—¶é—´
            
            # æ£€æŸ¥ç”Ÿæˆçš„æ–‡ä»¶
            tutorial_file = Path(temp_dir) / "tutorial.md"
            question_file = Path(temp_dir) / "question.md"
            
            self.assertTrue(tutorial_file.exists(), "Tutorial file not generated")
            self.assertTrue(question_file.exists(), "Question file not generated")
            
            # éªŒè¯å†…å®¹è´¨é‡
            with open(tutorial_file, 'r', encoding='utf-8') as f:
                tutorial_content = f.read()
            
            tutorial_validation = self._validate_tutorial_structure(tutorial_content)
            self.assertTrue(tutorial_validation['has_substantial_content'],
                          f"PDF tutorial too short: {tutorial_validation['content_length']} chars")
            
            # æ£€æŸ¥å…³é”®è¯è¦†ç›–
            found_keywords, missing_keywords = self._extract_keywords_from_content(
                tutorial_content, expected_keywords)
            
            coverage_ratio = len(found_keywords) / len(expected_keywords)
            
            # è¾“å‡ºè¯¦ç»†çš„å…³é”®è¯åˆ†æ
            print(f"ğŸ” å…³é”®è¯åˆ†æ:")
            print(f"   âœ… æ‰¾åˆ°çš„å…³é”®è¯ ({len(found_keywords)}/{len(expected_keywords)}): {found_keywords}")
            print(f"   âŒ ç¼ºå¤±çš„å…³é”®è¯: {missing_keywords}")
            
            self.assertGreaterEqual(coverage_ratio, 0.75,  # è¦æ±‚è‡³å°‘12/16 = 75%
                             f"Low keyword coverage: {coverage_ratio:.2f} ({len(found_keywords)}/{len(expected_keywords)})")
            
            print(f"âœ… PDF test - Tutorial: {tutorial_validation['content_length']} chars, "
                  f"Keywords: {len(found_keywords)}/{len(expected_keywords)} ({coverage_ratio:.1%})")

    def test_03_url_input_quality(self):
        """æµ‹è¯•åŸºäºURLè¾“å…¥çš„å†…å®¹è´¨é‡"""
        # ä½¿ç”¨ä¸€ä¸ªå·²çŸ¥çš„arXivè®ºæ–‡URL
        test_url = "https://arxiv.org/pdf/2106.02613.pdf"
        expected_keywords = [
            ["neural", "ç¥ç»"],
            ["network", "ç½‘ç»œ"],
            ["target", "ç›®æ ‡"],
            ["regularization", "æ­£åˆ™åŒ–"],
            ["æ·±åº¦å­¦ä¹ ", "deep learning"],
            ["ç¥ç»ç½‘ç»œ", "neural network"],
            ["æœºå™¨å­¦ä¹ ", "machine learning", "ML"],
            ["ç®—æ³•", "algorithm"],
            ["ä¼˜åŒ–", "optimization", "optimize"],
            ["è®­ç»ƒ", "training", "train"],
            ["æ¨¡å‹", "model"],
            ["å‡½æ•°", "function"],
            ["å‚æ•°", "parameter", "param"],
            ["æŸå¤±", "loss"],
            ["æ¢¯åº¦", "gradient"],
            ["æ€§èƒ½", "performance", "æ•ˆæœ"]
        ]
        
        # ä½¿ç”¨/tmpç›®å½•é¿å…æ±¡æŸ“æµ‹è¯•ç›®å½•
        temp_base = Path("/tmp/test_learn_url")
        temp_base.mkdir(exist_ok=True)
        
        with tempfile.TemporaryDirectory(dir=temp_base) as temp_dir:
            # è¿è¡ŒLEARNç”Ÿæˆæ•™ç¨‹ï¼ˆä½¿ç”¨URLè¾“å…¥ï¼‰
            # é¢„æœŸï¼šä¸‹è½½+extract=3åˆ†é’Ÿï¼Œ3æ¬¡OpenRouterè°ƒç”¨=3åˆ†é’Ÿï¼Œæ€»è®¡6åˆ†é’Ÿ
            result = self.assertCommandSuccess([
                sys.executable, str(self.learn_py),
                "-o", temp_dir, "-m", "ä¸­çº§", "-s", "ç®€æ´æ˜äº†",
                "--url", test_url,
                "ç¥ç»ç½‘ç»œæ­£åˆ™åŒ–æŠ€æœ¯"
            ], timeout=240)  # 4åˆ†é’Ÿè¶…æ—¶
            
            # æ£€æŸ¥ç”Ÿæˆçš„æ–‡ä»¶
            tutorial_file = Path(temp_dir) / "tutorial.md"
            question_file = Path(temp_dir) / "question.md"
            
            self.assertTrue(tutorial_file.exists(), "Tutorial file not generated")
            self.assertTrue(question_file.exists(), "Question file not generated")
            
            # éªŒè¯å†…å®¹è´¨é‡
            with open(tutorial_file, 'r', encoding='utf-8') as f:
                tutorial_content = f.read()
            
            tutorial_validation = self._validate_tutorial_structure(tutorial_content)
            self.assertTrue(tutorial_validation['has_substantial_content'],
                          f"URL tutorial too short: {tutorial_validation['content_length']} chars")
            
            # æ£€æŸ¥å…³é”®è¯è¦†ç›–
            found_keywords, missing_keywords = self._extract_keywords_from_content(
                tutorial_content, expected_keywords)
            
            coverage_ratio = len(found_keywords) / len(expected_keywords)
            
            # è¾“å‡ºè¯¦ç»†çš„å…³é”®è¯åˆ†æ
            print(f"ğŸ” å…³é”®è¯åˆ†æ:")
            print(f"   âœ… æ‰¾åˆ°çš„å…³é”®è¯ ({len(found_keywords)}/{len(expected_keywords)}): {found_keywords}")
            print(f"   âŒ ç¼ºå¤±çš„å…³é”®è¯: {missing_keywords}")
            
            self.assertGreaterEqual(coverage_ratio, 0.75,  # è¦æ±‚è‡³å°‘12/16 = 75%
                             f"Low keyword coverage: {coverage_ratio:.2f} ({len(found_keywords)}/{len(expected_keywords)})")
            
            print(f"âœ… URL test - Tutorial: {tutorial_validation['content_length']} chars, "
                  f"Keywords: {len(found_keywords)}/{len(expected_keywords)} ({coverage_ratio:.1%})")

    def test_04_description_input_quality(self):
        """æµ‹è¯•åŸºäºæè¿°è¾“å…¥çš„å†…å®¹è´¨é‡"""
        # æ›´æ˜ç¡®æŒ‡å‘æ€§çš„æè¿°
        description = "åŒç›®ç«‹ä½“è§†è§‰ä¸­çš„æ·±åº¦ä¼°è®¡ç®—æ³•ç ”ç©¶ï¼ŒåŒ…æ‹¬ç›¸æœºæ ‡å®šã€ç‰¹å¾åŒ¹é…ã€è§†å·®è®¡ç®—å’Œä¸‰ç»´é‡å»ºæŠ€æœ¯"
        expected_keywords = [
            ["ç«‹ä½“è§†è§‰", "stereo vision", "åŒç›®è§†è§‰"],  # æ ¸å¿ƒæ¦‚å¿µ1
            ["æ·±åº¦ä¼°è®¡", "depth estimation", "depth"],  # æ ¸å¿ƒæ¦‚å¿µ2
            ["è§†å·®", "disparity", "parallax"],  # æ ¸å¿ƒæŠ€æœ¯1
            ["åŒç›®", "binocular", "stereo"],  # æ ¸å¿ƒæŠ€æœ¯2
            ["ä¸‰ç»´é‡å»º", "3D reconstruction", "ä¸‰ç»´"],  # æ ¸å¿ƒç›®æ ‡
            ["ç›¸æœºæ ‡å®š", "camera calibration", "æ ‡å®š"],  # å…³é”®æ­¥éª¤1
            ["åŒ¹é…ç®—æ³•", "matching", "ç‰¹å¾åŒ¹é…"],  # å…³é”®æ­¥éª¤2
            ["è®¡ç®—æœºè§†è§‰", "computer vision", "CV"],  # å­¦ç§‘é¢†åŸŸ
            ["å›¾åƒå¤„ç†", "image processing", "å›¾åƒ"],  # ç›¸å…³æŠ€æœ¯1
            ["å‡ ä½•", "geometry", "å‡ ä½•å­¦"],  # ç›¸å…³ç†è®º1
            ["ç®—æ³•", "algorithm", "æ–¹æ³•"],  # é€šç”¨æŠ€æœ¯è¯
            ["åƒç´ ", "pixel", "ç‚¹"],  # åŸºç¡€æ¦‚å¿µ1
            ["åæ ‡", "coordinate", "ä½ç½®"],  # åŸºç¡€æ¦‚å¿µ2
            ["å˜æ¢", "transform", "è½¬æ¢"],  # åŸºç¡€æ“ä½œ
            ["çŸ©é˜µ", "matrix", "çŸ©é˜µè¿ç®—"],  # æ•°å­¦å·¥å…·
            ["ç²¾åº¦", "accuracy", "å‡†ç¡®åº¦"]  # æ€§èƒ½æŒ‡æ ‡
        ]
        
        # ä½¿ç”¨/tmpç›®å½•é¿å…æ±¡æŸ“æµ‹è¯•ç›®å½•
        temp_base = Path("/tmp/test_learn_desc")
        temp_base.mkdir(exist_ok=True)
        
        with tempfile.TemporaryDirectory(dir=temp_base) as temp_dir:
            # è¿è¡ŒLEARNç”Ÿæˆæ•™ç¨‹ï¼ˆä½¿ç”¨æè¿°æœç´¢ï¼‰
            # é¢„æœŸï¼š1æ¬¡æŒ‡ä»¤ç”Ÿæˆ+search+1æ¬¡ç»“æœéªŒè¯=3åˆ†é’Ÿï¼Œ3æ¬¡OpenRouterè°ƒç”¨=6åˆ†é’Ÿï¼Œæ€»è®¡9åˆ†é’Ÿ
            result = self.assertCommandSuccess([
                sys.executable, str(self.learn_py),
                "-o", temp_dir, "-m", "ä¸“å®¶", "-s", "ç†è®ºå¯¼å‘",
                "--description", description,
                "--negative", "åŒ»å­¦ åŒ»ç–—",  # æ’é™¤ä¸ç›¸å…³å†…å®¹
                "ç«‹ä½“è§†è§‰æ·±åº¦ä¼°è®¡ä¸“ä¸šæ•™ç¨‹"
            ], timeout=360)  # 6åˆ†é’Ÿè¶…æ—¶
            
            # æ£€æŸ¥ç”Ÿæˆçš„æ–‡ä»¶
            tutorial_file = Path(temp_dir) / "tutorial.md"
            question_file = Path(temp_dir) / "question.md"
            
            self.assertTrue(tutorial_file.exists(), "Tutorial file not generated")
            self.assertTrue(question_file.exists(), "Question file not generated")
            
            # éªŒè¯å†…å®¹è´¨é‡
            with open(tutorial_file, 'r', encoding='utf-8') as f:
                tutorial_content = f.read()
            
            tutorial_validation = self._validate_tutorial_structure(tutorial_content)
            self.assertTrue(tutorial_validation['has_substantial_content'],
                          f"Description tutorial too short: {tutorial_validation['content_length']} chars")
            
            # æ£€æŸ¥å…³é”®è¯è¦†ç›–
            found_keywords, missing_keywords = self._extract_keywords_from_content(
                tutorial_content, expected_keywords)
            
            coverage_ratio = len(found_keywords) / len(expected_keywords)
            
            # è¾“å‡ºè¯¦ç»†çš„å…³é”®è¯åˆ†æ
            print(f"ğŸ” å…³é”®è¯åˆ†æ:")
            print(f"   âœ… æ‰¾åˆ°çš„å…³é”®è¯ ({len(found_keywords)}/{len(expected_keywords)}): {found_keywords}")
            print(f"   âŒ ç¼ºå¤±çš„å…³é”®è¯: {missing_keywords}")
            
            self.assertGreaterEqual(coverage_ratio, 0.5,  # è¦æ±‚è‡³å°‘8/16 = 50%ï¼Œä½“ç°æŒ‡å‘æ€§
                             f"Low keyword coverage: {coverage_ratio:.2f} ({len(found_keywords)}/{len(expected_keywords)})")
            
            print(f"âœ… Description test - Tutorial: {tutorial_validation['content_length']} chars, "
                  f"Keywords: {len(found_keywords)}/{len(expected_keywords)} ({coverage_ratio:.1%})")

    def test_05_brainstorm_only_quality(self):
        """æµ‹è¯•--brainstorm-onlyæ¨¡å¼çš„å†…å®¹è´¨é‡"""
        topic = "3D Gaussian Splattingåœ¨å®æ—¶æ¸²æŸ“ä¸­çš„åº”ç”¨"
        expected_keywords = [
            ["3D", "ä¸‰ç»´", "ç«‹ä½“"],
            ["Gaussian", "é«˜æ–¯", "gauss"],
            ["Splatting", "splat", "æ³¼æº…"],
            ["å®æ—¶æ¸²æŸ“", "real-time rendering", "å®æ—¶"],
            ["æ¸²æŸ“", "render", "rendering"],
            ["å›¾å½¢å­¦", "graphics", "è®¡ç®—æœºå›¾å½¢"],
            ["è®¡ç®—æœºå›¾å½¢", "computer graphics", "CG"],
            ["GPU", "æ˜¾å¡", "å›¾å½¢å¤„ç†"],
            ["ä¼˜åŒ–", "optimization", "æ€§èƒ½ä¼˜åŒ–"],
            ["æ€§èƒ½", "performance", "æ•ˆç‡"],
            ["ç®—æ³•", "algorithm", "æ–¹æ³•"],
            ["è´¨é‡", "quality", "å“è´¨"],
            ["é€Ÿåº¦", "speed", "å¿«é€Ÿ"],
            ["æŠ€æœ¯", "technology", "tech"],
            ["åº”ç”¨", "application", "app"],
            ["æ•ˆæœ", "effect", "ç»“æœ"]
        ]
        
        temp_base = Path("/tmp/test_learn_brainstorm")
        temp_base.mkdir(exist_ok=True)
        
        with tempfile.TemporaryDirectory(dir=temp_base) as temp_dir:
            # è¿è¡ŒLEARN brainstorm-onlyæ¨¡å¼
            result = self.assertCommandSuccess([
                sys.executable, str(self.learn_py),
                "-o", temp_dir, "-m", "ä¸­çº§", "-s", "å®ä¾‹ä¸°å¯Œ",
                "--brainstorm-only", topic
            ], timeout=60)
            
            # brainstorm-onlyæ¨¡å¼ä¸ç”Ÿæˆæ–‡ä»¶ï¼Œæ£€æŸ¥è¾“å‡ºå†…å®¹
            output_content = result.stdout
            
            # éªŒè¯brainstormå†…å®¹è´¨é‡
            self.assertIn("å¤´è„‘é£æš´", output_content)
            self.assertGreater(len(output_content), 500, "Brainstorm output too short")
            
            # æ£€æŸ¥å…³é”®è¯è¦†ç›–
            found_keywords, missing_keywords = self._extract_keywords_from_content(
                output_content, expected_keywords)
            
            coverage_ratio = len(found_keywords) / len(expected_keywords)
            
            # è¾“å‡ºè¯¦ç»†çš„å…³é”®è¯åˆ†æ
            print(f"ğŸ” å…³é”®è¯åˆ†æ:")
            print(f"   âœ… æ‰¾åˆ°çš„å…³é”®è¯ ({len(found_keywords)}/{len(expected_keywords)}): {found_keywords}")
            print(f"   âŒ ç¼ºå¤±çš„å…³é”®è¯: {missing_keywords}")
            
            self.assertGreaterEqual(coverage_ratio, 0.68,  # è¦æ±‚è‡³å°‘11/16 = 68%ï¼Œå®é™…è¡¨ç°è‰¯å¥½
                             f"Low keyword coverage in brainstorm: {coverage_ratio:.2f} ({len(found_keywords)}/{len(expected_keywords)})")
            
            print(f"âœ… Brainstorm test - Output: {len(output_content)} chars, "
                  f"Keywords: {len(found_keywords)}/{len(expected_keywords)} ({coverage_ratio:.1%})")

    def test_06_description_general_topic_quality(self):
        """æµ‹è¯•åŸºäºæè¿°è¾“å…¥çš„é€šç”¨ä¸»é¢˜ï¼ˆéè®ºæ–‡ï¼‰å†…å®¹è´¨é‡"""
        # æ˜ç¡®æŒ‡å‘é€šç”¨ä¸»é¢˜è€Œéç‰¹å®šè®ºæ–‡çš„æè¿°
        description = "æœºå™¨å­¦ä¹ ä¸­çš„ç›‘ç£å­¦ä¹ ç®—æ³•ï¼ŒåŒ…æ‹¬å†³ç­–æ ‘ã€æ”¯æŒå‘é‡æœºå’Œç¥ç»ç½‘ç»œçš„åŸºæœ¬åŸç†ä¸åº”ç”¨"
        expected_keywords = [
            ["æœºå™¨å­¦ä¹ ", "machine learning", "ML"],
            ["ç›‘ç£å­¦ä¹ ", "supervised learning"],
            ["ç®—æ³•", "algorithm", "æ–¹æ³•"],
            ["å†³ç­–æ ‘", "decision tree"],
            ["æ”¯æŒå‘é‡æœº", "SVM", "support vector machine"],
            ["ç¥ç»ç½‘ç»œ", "neural network"],
            ["åˆ†ç±»", "classification"],
            ["å›å½’", "regression"],
            ["è®­ç»ƒ", "training", "train"],
            ["ç‰¹å¾", "feature"],
            ["æ•°æ®", "data", "dataset"],
            ["æ¨¡å‹", "model"],
            ["é¢„æµ‹", "prediction", "predict"],
            ["å‡†ç¡®ç‡", "accuracy"],
            ["è¿‡æ‹Ÿåˆ", "overfitting"],
            ["æ³›åŒ–", "generalization"]
        ]
        
        temp_base = Path("/tmp/test_learn_general")
        temp_base.mkdir(exist_ok=True)
        
        with tempfile.TemporaryDirectory(dir=temp_base) as temp_dir:
            result = self.assertCommandSuccess([
                sys.executable, str(self.learn_py),
                "-o", temp_dir, "-m", "ä¸­çº§", "-s", "å®ä¾‹ä¸°å¯Œ",
                "--description", description,
                "æœºå™¨å­¦ä¹ ç›‘ç£ç®—æ³•æ•™ç¨‹"
            ], timeout=360)  # 6åˆ†é’Ÿè¶…æ—¶
            
            # éªŒè¯æ–‡ä»¶ç”Ÿæˆ
            tutorial_file = Path(temp_dir) / "tutorial.md"
            question_file = Path(temp_dir) / "question.md"
            
            self.assertTrue(tutorial_file.exists(), "Tutorial file not generated")
            self.assertTrue(question_file.exists(), "Question file not generated")
            
            # éªŒè¯å†…å®¹è´¨é‡
            with open(tutorial_file, 'r', encoding='utf-8') as f:
                tutorial_content = f.read()
            
            tutorial_validation = self._validate_tutorial_structure(tutorial_content)
            self.assertTrue(tutorial_validation['has_substantial_content'],
                          f"General topic tutorial too short: {tutorial_validation['content_length']} chars")
            
            # æ£€æŸ¥å…³é”®è¯è¦†ç›–
            found_keywords, missing_keywords = self._extract_keywords_from_content(
                tutorial_content, expected_keywords)
            
            coverage_ratio = len(found_keywords) / len(expected_keywords)
            
            print(f"ğŸ” å…³é”®è¯åˆ†æ:")
            print(f"   âœ… æ‰¾åˆ°çš„å…³é”®è¯ ({len(found_keywords)}/{len(expected_keywords)}): {found_keywords}")
            print(f"   âŒ ç¼ºå¤±çš„å…³é”®è¯: {missing_keywords}")
            
            self.assertGreaterEqual(coverage_ratio, 0.5,  # é€šç”¨ä¸»é¢˜è¦æ±‚8/16 = 50%
                             f"Low keyword coverage: {coverage_ratio:.2f} ({len(found_keywords)}/{len(expected_keywords)})")
            
            print(f"âœ… General Topic test - Tutorial: {tutorial_validation['content_length']} chars, "
                  f"Keywords: {len(found_keywords)}/{len(expected_keywords)} ({coverage_ratio:.1%})")

    def test_07a_at_reference_file_not_found(self):
        """æµ‹è¯•@ç¬¦å·å¼•ç”¨ä¸å­˜åœ¨çš„æ–‡ä»¶ - åº”è¯¥å¿«é€Ÿç»“æŸ"""
        import time
        start_time = time.time()
        
        temp_base = Path("/tmp/test_learn_at_reference_error")
        temp_base.mkdir(exist_ok=True)
        
        with tempfile.TemporaryDirectory(dir=temp_base) as temp_dir:
            # ä½¿ç”¨@ç¬¦å·å¼•ç”¨ä¸å­˜åœ¨çš„æ–‡ä»¶
            nonexistent_file = "/tmp/nonexistent_paper.md"
            description = f'å­¦ä¹ ä¸å­˜åœ¨çš„è®ºæ–‡ @"{nonexistent_file}"'
            
            # è¿™ä¸ªå‘½ä»¤åº”è¯¥å¤±è´¥ï¼Œå› ä¸ºæ–‡ä»¶ä¸å­˜åœ¨
            result = self.assertCommandFail([
                sys.executable, str(self.learn_py),
                "-o", temp_dir, "-m", "åˆå­¦è€…", "-s", "ç®€æ´æ˜äº†",
                "--brainstorm-only",  # åªåšå¤´è„‘é£æš´ï¼Œé¿å…ä¸‹è½½
                "--description", description,
                "æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨çš„@å¼•ç”¨"
            ], timeout=1)  # 1ç§’è¶…æ—¶ï¼Œåº”è¯¥å¾ˆå¿«ç»“æŸ
            
            # éªŒè¯æ‰§è¡Œæ—¶é—´
            execution_time = time.time() - start_time
            self.assertLess(execution_time, 1, f"@ç¬¦å·å¼•ç”¨ä¸å­˜åœ¨æ–‡ä»¶æµ‹è¯•è€—æ—¶è¿‡é•¿: {execution_time:.1f}ç§’")
            
            # éªŒè¯æ–‡ä»¶ä¸å­˜åœ¨çš„é”™è¯¯å¤„ç†
            error_found = (
                "æ–‡ä»¶ä¸å­˜åœ¨" in result.stderr or "@ç¬¦å·å¼•ç”¨çš„æ–‡ä»¶ä¸å­˜åœ¨" in result.stderr or
                "æ–‡ä»¶ä¸å­˜åœ¨" in result.stdout or "@ç¬¦å·å¼•ç”¨çš„æ–‡ä»¶ä¸å­˜åœ¨" in result.stdout
            )
            self.assertTrue(
                error_found,
                f"æœªæ‰¾åˆ°é¢„æœŸçš„é”™è¯¯ä¿¡æ¯ï¼Œstderr: {result.stderr}, stdout: {result.stdout}"
            )
            print(f"âœ… @ç¬¦å·å¼•ç”¨æ–‡ä»¶ä¸å­˜åœ¨æµ‹è¯•é€šè¿‡ - è€—æ—¶: {execution_time:.1f}ç§’")

    def test_07b_at_reference_single_paper_absolute_path(self):
        """æµ‹è¯•@ç¬¦å·å¼•ç”¨å•ä¸ªè®ºæ–‡ï¼ˆç»å¯¹è·¯å¾„ï¼‰ - å†…å®¹è´¨é‡éªŒè¯"""
        paper1_path = self.test_data_dir / "extracted_paper.md"
        if not paper1_path.exists():
            self.skipTest("extracted_paper.md not found")
            
        # é¢„æœŸå…³é”®è¯ï¼ˆåŸºäºGaussianObjectè®ºæ–‡ï¼‰
        expected_keywords = [
            ["GaussianObject", "Gaussian Object"],
            ["3D", "ä¸‰ç»´", "ç«‹ä½“"],
            ["é‡å»º", "reconstruction", "é‡æ„"],
            ["é«˜æ–¯", "Gaussian", "gauss"],
            ["Splatting", "splat"],
            ["è§†è§‰", "visual", "vision"],
            ["è´¨é‡", "quality", "é«˜è´¨é‡"],
            ["ç®—æ³•", "algorithm", "æ–¹æ³•"],
            ["æŠ€æœ¯", "technology", "technique"],
            ["åŸç†", "principle", "theory"],
            ["æ·±åº¦å­¦ä¹ ", "deep learning"],
            ["è®¡ç®—æœºè§†è§‰", "computer vision"],
            ["ç‚¹äº‘", "point cloud"],
            ["æ¸²æŸ“", "render", "rendering"],
            ["æ¨¡å‹", "model"],
            ["ä¼˜åŒ–", "optimization"]
        ]
        
        temp_base = Path("/tmp/test_learn_at_single_abs")
        temp_base.mkdir(exist_ok=True)
        
        with tempfile.TemporaryDirectory(dir=temp_base) as temp_dir:
            # ä½¿ç”¨@ç¬¦å·å¼•ç”¨æ–‡ä»¶å†…å®¹ï¼ˆç»å¯¹è·¯å¾„ï¼‰
            description = f'æ·±å…¥å­¦ä¹ GaussianObjectçš„3Dé‡å»ºæŠ€æœ¯åŸç†å’Œæ–¹æ³• @"{paper1_path.absolute()}"'
            
            result = self.assertCommandSuccess([
                sys.executable, str(self.learn_py),
                "-o", temp_dir, "-m", "ä¸­çº§", "-s", "è¯¦ç»†æ·±å…¥",
                "--context",  # contextæ¨¡å¼è·³è¿‡brainstormingï¼Œç›´æ¥ç”Ÿæˆæ•™ç¨‹
                "--description", description,
                "åŸºäºGaussianObjectè®ºæ–‡çš„3Dé‡å»ºå­¦ä¹ "
            ], timeout=120)  # 2åˆ†é’Ÿè¶…æ—¶ï¼Œéœ€è¦ç”Ÿæˆå®Œæ•´æ•™ç¨‹
            
            # éªŒè¯å†…å®¹è´¨é‡
            found_keywords, missing_keywords = self._extract_keywords_from_content(
                result.stdout, expected_keywords)
            
            coverage_ratio = len(found_keywords) / len(expected_keywords)
            
            print(f"ğŸ” å…³é”®è¯åˆ†æ:")
            print(f"   âœ… æ‰¾åˆ°çš„å…³é”®è¯ ({len(found_keywords)}/{len(expected_keywords)}): {found_keywords}")
            print(f"   âŒ ç¼ºå¤±çš„å…³é”®è¯: {missing_keywords}")
            
            # è¦æ±‚è‡³å°‘75%çš„å…³é”®è¯è¦†ç›–
            self.assertGreaterEqual(coverage_ratio, 0.75,
                             f"å…³é”®è¯è¦†ç›–ç‡ä¸è¶³: {coverage_ratio:.2f} ({len(found_keywords)}/{len(expected_keywords)})")
            
            # éªŒè¯@ç¬¦å·å¼•ç”¨åŠŸèƒ½
            self.assertTrue(
                "æ£€æµ‹åˆ°@æ–‡ä»¶å¼•ç”¨" in result.stdout or "Contextæ¨¡å¼" in result.stdout,
                "æœªæ‰¾åˆ°@æ–‡ä»¶å¼•ç”¨æˆ–Contextæ¨¡å¼çš„ç›¸å…³ä¿¡æ¯"
            )
            
            # éªŒè¯ç”Ÿæˆäº†æ•™ç¨‹æ–‡ä»¶
            tutorial_file = Path(temp_dir) / "tutorial.md"
            question_file = Path(temp_dir) / "question.md"
            self.assertTrue(tutorial_file.exists(), "Tutorialæ–‡ä»¶æœªç”Ÿæˆ")
            self.assertTrue(question_file.exists(), "Questionæ–‡ä»¶æœªç”Ÿæˆ")
            
            print(f"âœ… @ç¬¦å·å¼•ç”¨å•è®ºæ–‡ï¼ˆç»å¯¹è·¯å¾„ï¼‰æµ‹è¯•é€šè¿‡ - å…³é”®è¯è¦†ç›–ç‡: {coverage_ratio:.1%}")

    def test_07c_at_reference_single_paper_relative_path(self):
        """æµ‹è¯•@ç¬¦å·å¼•ç”¨å•ä¸ªè®ºæ–‡ï¼ˆç›¸å¯¹è·¯å¾„ï¼‰"""
        # ä½¿ç”¨ç›¸å¯¹è·¯å¾„
        paper1_relative = "_UNITTEST/_DATA/extracted_paper2.md"
        
        temp_base = Path("/tmp/test_learn_at_single_rel")
        temp_base.mkdir(exist_ok=True)
        
        with tempfile.TemporaryDirectory(dir=temp_base) as temp_dir:
            # ä½¿ç”¨@ç¬¦å·å¼•ç”¨æ–‡ä»¶å†…å®¹ï¼ˆç›¸å¯¹è·¯å¾„ï¼‰
            description = f'å­¦ä¹ AutoPartGençš„è‡ªå›å½’3Déƒ¨ä»¶ç”ŸæˆæŠ€æœ¯ @"{paper1_relative}"'
            
            result = self.assertCommandSuccess([
                sys.executable, str(self.learn_py),
                "-o", temp_dir, "-m", "ä¸­çº§", "-s", "è¯¦ç»†æ·±å…¥", 
                "--context",  # contextæ¨¡å¼è·³è¿‡brainstormingï¼Œç›´æ¥ç”Ÿæˆæ•™ç¨‹
                "--description", description,
                "åŸºäºAutoPartGenè®ºæ–‡çš„3Déƒ¨ä»¶ç”Ÿæˆå­¦ä¹ "
            ], timeout=120)  # 2åˆ†é’Ÿè¶…æ—¶ï¼Œéœ€è¦ç”Ÿæˆå®Œæ•´æ•™ç¨‹
            
            # éªŒè¯@ç¬¦å·å¼•ç”¨åŠŸèƒ½å’Œå†…å®¹è´¨é‡
            self.assertTrue(
                "å±•å¼€æ–‡ä»¶å¼•ç”¨" in result.stdout or "æ£€æµ‹åˆ°æ–‡ä»¶å¼•ç”¨" in result.stdout,
                "æœªæ‰¾åˆ°æ–‡ä»¶å¼•ç”¨å¤„ç†çš„ç›¸å…³ä¿¡æ¯"
            )
            
            # éªŒè¯ç”Ÿæˆçš„å†…å®¹åŒ…å«ç›¸å…³æ¦‚å¿µ
            self.assertIn("AutoPartGen", result.stdout)
            self.assertIn("è‡ªå›å½’", result.stdout) or self.assertIn("autoregressive", result.stdout)
            self.assertIn("éƒ¨ä»¶", result.stdout) or self.assertIn("part", result.stdout)
            
            print("âœ… @ç¬¦å·å¼•ç”¨å•è®ºæ–‡ï¼ˆç›¸å¯¹è·¯å¾„ï¼‰æµ‹è¯•é€šè¿‡")

    def test_07d_at_reference_double_papers_comparison(self):
        """æµ‹è¯•@ç¬¦å·å¼•ç”¨åŒè®ºæ–‡æ¯”è¾ƒ"""
        paper1_path = self.test_data_dir / "extracted_paper.md"
        paper2_path = self.test_data_dir / "extracted_paper2.md"
        
        if not paper1_path.exists() or not paper2_path.exists():
            self.skipTest("Test papers not found")
            
        temp_base = Path("/tmp/test_learn_at_double")
        temp_base.mkdir(exist_ok=True)
        
        with tempfile.TemporaryDirectory(dir=temp_base) as temp_dir:
            # ä½¿ç”¨@ç¬¦å·å¼•ç”¨ä¸¤ä¸ªæ–‡ä»¶è¿›è¡Œæ¯”è¾ƒ
            description = f'æ¯”è¾ƒåˆ†æGaussianObjectå’ŒAutoPartGenä¸¤ç§3Dç”ŸæˆæŠ€æœ¯çš„å¼‚åŒç‚¹ï¼Œé‡ç‚¹å…³æ³¨å®ƒä»¬çš„æ–¹æ³•è®ºã€åº”ç”¨åœºæ™¯å’ŒæŠ€æœ¯ä¼˜åŠ¿ @"{paper1_path}" @"{paper2_path}"'
            
            result = self.assertCommandSuccess([
                sys.executable, str(self.learn_py),
                "-o", temp_dir, "-m", "ä¸“å®¶", "-s", "ç†è®ºå¯¼å‘",
                "--context",  # contextæ¨¡å¼è·³è¿‡brainstormingï¼Œç›´æ¥ç”Ÿæˆæ•™ç¨‹
                "--description", description,
                "GaussianObject vs AutoPartGen æŠ€æœ¯å¯¹æ¯”åˆ†æ"
            ], timeout=120)  # 2åˆ†é’Ÿè¶…æ—¶ï¼Œéœ€è¦ç”Ÿæˆå®Œæ•´æ•™ç¨‹
            
            # éªŒè¯@ç¬¦å·å¼•ç”¨åŠŸèƒ½
            self.assertTrue(
                "å±•å¼€æ–‡ä»¶å¼•ç”¨" in result.stdout or "æ£€æµ‹åˆ°æ–‡ä»¶å¼•ç”¨" in result.stdout,
                "æœªæ‰¾åˆ°æ–‡ä»¶å¼•ç”¨å¤„ç†çš„ç›¸å…³ä¿¡æ¯"
            )
            
            # éªŒè¯ç”Ÿæˆçš„å†…å®¹åŒ…å«ä¸¤ç¯‡è®ºæ–‡çš„å…³é”®æ¦‚å¿µ
            gaussian_concepts = ["GaussianObject", "Gaussian", "é«˜æ–¯", "Splatting"]
            autopart_concepts = ["AutoPartGen", "è‡ªå›å½’", "autoregressive", "éƒ¨ä»¶", "part"]
            comparison_concepts = ["æ¯”è¾ƒ", "å¯¹æ¯”", "å¼‚åŒ", "difference", "comparison", "vs"]
            
            found_gaussian = any(concept in result.stdout for concept in gaussian_concepts)
            found_autopart = any(concept in result.stdout for concept in autopart_concepts)
            found_comparison = any(concept in result.stdout for concept in comparison_concepts)
            
            self.assertTrue(found_gaussian, "åº”è¯¥åŒ…å«GaussianObjectç›¸å…³æ¦‚å¿µ")
            self.assertTrue(found_autopart, "åº”è¯¥åŒ…å«AutoPartGenç›¸å…³æ¦‚å¿µ") 
            self.assertTrue(found_comparison, "åº”è¯¥åŒ…å«æ¯”è¾ƒåˆ†æç›¸å…³æ¦‚å¿µ")
            
            # è¯„ä¼°å†…å®¹è´¨é‡ - æ£€æŸ¥æ˜¯å¦åŒ…å«æŠ€æœ¯å¯¹æ¯”çš„å…³é”®è¦ç´ 
            quality_indicators = [
                "æ–¹æ³•", "method", "æŠ€æœ¯", "technology", "approach",
                "åº”ç”¨", "application", "åœºæ™¯", "scenario",
                "ä¼˜åŠ¿", "advantage", "ç‰¹ç‚¹", "feature", "benefit",
                "3D", "ç”Ÿæˆ", "generation", "é‡å»º", "reconstruction"
            ]
            
            found_quality_indicators = [indicator for indicator in quality_indicators 
                                      if indicator in result.stdout]
            quality_ratio = len(found_quality_indicators) / len(quality_indicators)
            
            print(f"ğŸ” åŒè®ºæ–‡æ¯”è¾ƒè´¨é‡åˆ†æ:")
            print(f"   âœ… æ‰¾åˆ°çš„è´¨é‡æŒ‡æ ‡ ({len(found_quality_indicators)}/{len(quality_indicators)}): {found_quality_indicators}")
            print(f"   è´¨é‡æ¯”ä¾‹: {quality_ratio:.1%}")
            
            # è¦æ±‚è‡³å°‘åŒ…å«50%çš„è´¨é‡æŒ‡æ ‡
            self.assertGreaterEqual(quality_ratio, 0.5, 
                             f"åŒè®ºæ–‡æ¯”è¾ƒè´¨é‡ä¸è¶³: {quality_ratio:.2f}")
            
            print("âœ… @ç¬¦å·å¼•ç”¨åŒè®ºæ–‡æ¯”è¾ƒæµ‹è¯•é€šè¿‡")

    def test_07h_context_option(self):
        """æµ‹è¯•--contexté€‰é¡¹åŠŸèƒ½"""
        # åˆ›å»ºåŒ…å«ç‰¹å®šé¢†åŸŸçŸ¥è¯†çš„æµ‹è¯•å†…å®¹
        test_context = """æ·±åº¦å¼ºåŒ–å­¦ä¹ åœ¨æ¸¸æˆAIä¸­çš„åº”ç”¨

1. æ ¸å¿ƒæ¦‚å¿µ
- Qå­¦ä¹ ç®—æ³•æ˜¯å¼ºåŒ–å­¦ä¹ çš„åŸºç¡€
- æ·±åº¦Qç½‘ç»œ(DQN)ç»“åˆäº†æ·±åº¦å­¦ä¹ å’ŒQå­¦ä¹ 
- ç­–ç•¥æ¢¯åº¦æ–¹æ³•ç›´æ¥ä¼˜åŒ–ç­–ç•¥å‡½æ•°
- Actor-Criticæ–¹æ³•ç»“åˆä»·å€¼å‡½æ•°å’Œç­–ç•¥å‡½æ•°

2. æŠ€æœ¯æŒ‘æˆ˜
- æ ·æœ¬æ•ˆç‡é—®é¢˜ï¼šéœ€è¦å¤§é‡è®­ç»ƒæ•°æ®
- ç¨³å®šæ€§é—®é¢˜ï¼šè®­ç»ƒè¿‡ç¨‹å®¹æ˜“ä¸ç¨³å®š
- æ³›åŒ–èƒ½åŠ›ï¼šå¦‚ä½•åœ¨æ–°ç¯å¢ƒä¸­è¡¨ç°è‰¯å¥½

3. åº”ç”¨æ¡ˆä¾‹
- AlphaGoï¼šå›´æ£‹æ¸¸æˆAIçªç ´
- OpenAI Fiveï¼šDOTA2æ¸¸æˆAI
- AlphaStarï¼šæ˜Ÿé™…äº‰éœ¸IIæ¸¸æˆAI
"""
        
        # é¢„æœŸå…³é”®è¯
        expected_keywords = [
            ["å¼ºåŒ–å­¦ä¹ ", "reinforcement learning"],
            ["æ¸¸æˆAI", "game AI"],
            ["Qå­¦ä¹ ", "Q-learning"],
            ["æ·±åº¦Qç½‘ç»œ", "DQN"],
            ["ç­–ç•¥æ¢¯åº¦", "policy gradient"],
            ["Actor-Critic"],
            ["æ ·æœ¬æ•ˆç‡", "sample efficiency"],
            ["ç¨³å®šæ€§", "stability"],
            ["æ³›åŒ–", "generalization"],
            ["AlphaGo"],
            ["OpenAI Five"],
            ["AlphaStar"],
            ["æ·±åº¦å­¦ä¹ ", "deep learning"],
            ["ç®—æ³•", "algorithm"],
            ["è®­ç»ƒ", "training"],
            ["ä¼˜åŒ–", "optimization"]
        ]
        
        temp_base = Path("/tmp/test_learn_context")
        temp_base.mkdir(exist_ok=True)
        
        with tempfile.TemporaryDirectory(dir=temp_base) as temp_dir:
            # æµ‹è¯•--contexté€‰é¡¹
            result = self.assertCommandSuccess([
                sys.executable, str(self.learn_py),
                "-o", temp_dir, "-m", "é«˜çº§", "-s", "ç†è®ºå¯¼å‘",
                "--context",
                "--description", test_context,
                "æ·±åº¦å¼ºåŒ–å­¦ä¹ æ¸¸æˆAIä¸“ä¸šæ•™ç¨‹"
            ], timeout=120)  # éœ€è¦ç”Ÿæˆå®Œæ•´æ•™ç¨‹ï¼Œæ—¶é—´ä¼šæ›´é•¿
            
            # éªŒè¯å†…å®¹è´¨é‡
            found_keywords, missing_keywords = self._extract_keywords_from_content(
                result.stdout, expected_keywords)
            
            coverage_ratio = len(found_keywords) / len(expected_keywords)
            
            print(f"ğŸ” å…³é”®è¯åˆ†æ:")
            print(f"   âœ… æ‰¾åˆ°çš„å…³é”®è¯ ({len(found_keywords)}/{len(expected_keywords)}): {found_keywords}")
            print(f"   âŒ ç¼ºå¤±çš„å…³é”®è¯: {missing_keywords}")
            
            # è¦æ±‚è‡³å°‘85%çš„å…³é”®è¯è¦†ç›–ï¼ˆç›´æ¥contextåº”è¯¥æœ€å‡†ç¡®ï¼‰
            self.assertGreaterEqual(coverage_ratio, 0.85,
                             f"å…³é”®è¯è¦†ç›–ç‡ä¸è¶³: {coverage_ratio:.2f} ({len(found_keywords)}/{len(expected_keywords)})")
            
            # éªŒè¯--contextæ¨¡å¼å¯ç”¨
            self.assertIn("Contextæ¨¡å¼", result.stdout)
            
            # éªŒè¯æ²¡æœ‰è¿›è¡Œå®é™…çš„è®ºæ–‡æœç´¢å’Œä¸‹è½½
            self.assertNotIn("ğŸ” æœç´¢è®ºæ–‡:", result.stdout)
            self.assertNotIn("ğŸ“¥ ä¸‹è½½è®ºæ–‡:", result.stdout)
            self.assertNotIn("ğŸ¤– æ­£åœ¨ä¼˜åŒ–æœç´¢æŸ¥è¯¢", result.stdout)
            
            # éªŒè¯è·³è¿‡äº†brainstorming
            self.assertIn("è·³è¿‡å¤´è„‘é£æš´æ­¥éª¤", result.stdout)
            
            # éªŒè¯ç”Ÿæˆäº†æ•™ç¨‹æ–‡ä»¶
            tutorial_file = Path(temp_dir) / "tutorial.md"
            question_file = Path(temp_dir) / "question.md"
            self.assertTrue(tutorial_file.exists(), "Tutorialæ–‡ä»¶æœªç”Ÿæˆ")
            self.assertTrue(question_file.exists(), "Questionæ–‡ä»¶æœªç”Ÿæˆ")
            
            print(f"âœ… --contexté€‰é¡¹æµ‹è¯•é€šè¿‡ - å…³é”®è¯è¦†ç›–ç‡: {coverage_ratio:.1%}")

    def test_07i_context_brainstorm_only_mutual_exclusion(self):
        """æµ‹è¯•--contextå’Œ--brainstorm-onlyçš„äº’æ–¥æ€§"""
        temp_base = Path("/tmp/test_learn_mutex")
        temp_base.mkdir(exist_ok=True)
        
        with tempfile.TemporaryDirectory(dir=temp_base) as temp_dir:
            # å°è¯•åŒæ—¶ä½¿ç”¨--contextå’Œ--brainstorm-onlyï¼Œåº”è¯¥å¤±è´¥
            result = self.assertCommandFail([
                sys.executable, str(self.learn_py),
                "-o", temp_dir, "-m", "ä¸­çº§", "-s", "è¯¦ç»†æ·±å…¥",
                "--context", "--brainstorm-only",  # è¿™ä¸¤ä¸ªé€‰é¡¹äº’æ–¥
                "--description", "æµ‹è¯•äº’æ–¥æ€§æ£€æŸ¥",
                "æµ‹è¯•ä¸»é¢˜"
            ], timeout=10)  # åº”è¯¥å¾ˆå¿«å¤±è´¥
            
            # éªŒè¯é”™è¯¯ä¿¡æ¯
            error_found = (
                "äº’æ–¥" in result.stderr or "ä¸èƒ½åŒæ—¶ä½¿ç”¨" in result.stderr or
                "äº’æ–¥" in result.stdout or "ä¸èƒ½åŒæ—¶ä½¿ç”¨" in result.stdout
            )
            self.assertTrue(
                error_found,
                f"æœªæ‰¾åˆ°é¢„æœŸçš„äº’æ–¥é”™è¯¯ä¿¡æ¯ï¼Œstderr: {result.stderr}, stdout: {result.stdout}"
            )
            
            print("âœ… --contextå’Œ--brainstorm-onlyäº’æ–¥æ€§æµ‹è¯•é€šè¿‡")

    def test_07e_at_reference_prompt_cleaning(self):
        """æµ‹è¯•@ç¬¦å·å¼•ç”¨æ–‡ä»¶æ—¶å‘ç»™OpenRouterçš„promptä¸åŒ…å«placeholderå’Œå›¾ç‰‡id"""
        # åˆ›å»ºåŒ…å«placeholderå’Œå›¾ç‰‡idçš„æµ‹è¯•æ–‡ä»¶
        test_content = """# æµ‹è¯•è®ºæ–‡

## ä»‹ç»
è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•è®ºæ–‡ã€‚

[placeholder: image_001]

![æµ‹è¯•å›¾ç‰‡](images/abcd1234567890abcdef1234567890abcdef1234567890abcdef1234567890ab.jpg)

## æ–¹æ³•
[image: formula_001]

ä¸€äº›æ­£å¸¸å†…å®¹ã€‚

[table: table_001]

abcd1234567890abcdef1234567890abcdef1234567890abcdef1234567890ab

[message: å›¾ç‰‡å¤„ç†å¤±è´¥]

æ›´å¤šæ­£å¸¸å†…å®¹ã€‚
"""
        
        temp_base = Path("/tmp/test_learn_at_prompt_clean")
        temp_base.mkdir(exist_ok=True)
        
        try:
            # åˆ›å»ºæµ‹è¯•æ–‡ä»¶
            test_file = temp_base / "test_paper_with_placeholders.md"
            with open(test_file, 'w', encoding='utf-8') as f:
                f.write(test_content)
            
            with tempfile.TemporaryDirectory(dir=temp_base) as temp_dir:
                # ä½¿ç”¨@ç¬¦å·å¼•ç”¨åŒ…å«placeholderçš„æ–‡ä»¶
                description = f'åˆ†æè¿™ç¯‡æµ‹è¯•è®ºæ–‡çš„å†…å®¹ @"{test_file}"'
                
                result = self.assertCommandSuccess([
                    sys.executable, str(self.learn_py),
                    "-o", temp_dir, "-m", "åˆå­¦è€…", "-s", "ç®€æ´æ˜äº†",
                    "--context",
                    "--description", description,
                    "æµ‹è¯•placeholderæ¸…ç†"
                ], timeout=30)
                
                # éªŒè¯è¾“å‡ºä¸­ä¸åŒ…å«placeholderç›¸å…³å†…å®¹
                output = result.stdout
                
                # æ£€æŸ¥ä¸åº”è¯¥åŒ…å«çš„å†…å®¹ï¼ˆä»åŸå§‹æ–‡ä»¶ä¸­çš„å†…å®¹ï¼‰
                forbidden_patterns = [
                    "image_001", "formula_001", "table_001",  # ç‰¹å®šçš„placeholder ID
                    "abcd1234567890abcdef1234567890abcdef1234567890abcdef1234567890ab",  # å›¾ç‰‡hash
                    "[image:", "[formula:", "[table:", "[message:",  # æ–¹æ‹¬å·æ ‡è®°
                    "images/abcd", "å›¾ç‰‡å¤„ç†å¤±è´¥"  # å…·ä½“çš„å›¾ç‰‡è·¯å¾„å’Œé”™è¯¯ä¿¡æ¯
                ]
                
                found_forbidden = []
                for pattern in forbidden_patterns:
                    if pattern.lower() in output.lower():
                        found_forbidden.append(pattern)
                
                self.assertEqual([], found_forbidden, 
                               f"OpenRouter promptåŒ…å«äº†åº”è¯¥è¢«æ¸…ç†çš„å†…å®¹: {found_forbidden}")
                
                # éªŒè¯åº”è¯¥åŒ…å«çš„æ­£å¸¸å†…å®¹
                self.assertIn("æ­£å¸¸å†…å®¹", output)
                self.assertIn("æµ‹è¯•è®ºæ–‡", output)
                
                print("âœ… @ç¬¦å·å¼•ç”¨promptæ¸…ç†æµ‹è¯•é€šè¿‡ - æ‰€æœ‰placeholderå’Œå›¾ç‰‡idå·²è¢«æ¸…ç†")
                
        finally:
            # æ¸…ç†æµ‹è¯•æ–‡ä»¶
            if test_file.exists():
                test_file.unlink()

    def test_07f_at_reference_pdf_support(self):
        """æµ‹è¯•@ç¬¦å·å¼•ç”¨PDFæ–‡ä»¶æ”¯æŒ - å†…å®¹è´¨é‡éªŒè¯"""
        # æ£€æŸ¥æ˜¯å¦æœ‰æµ‹è¯•PDFæ–‡ä»¶
        test_pdf = self.test_data_dir / "test_extract_paper.pdf"
        if not test_pdf.exists():
            self.skipTest("Test PDF file not found")
        
        # é¢„æœŸå…³é”®è¯ï¼ˆåŸºäºPDFå†…å®¹ï¼‰
        expected_keywords = [
            ["GaussianObject", "Gaussian Object"],
            ["3D", "ä¸‰ç»´"],
            ["é‡å»º", "reconstruction"],
            ["é«˜æ–¯", "Gaussian"],
            ["Splatting", "splat"],
            ["è´¨é‡", "quality"],
            ["ç®—æ³•", "algorithm"],
            ["æŠ€æœ¯", "technology"],
            ["è§†è§‰", "visual"],
            ["æ¨¡å‹", "model"],
            ["æ¸²æŸ“", "render"],
            ["ä¼˜åŒ–", "optimization"],
            ["æ·±åº¦å­¦ä¹ ", "deep learning"],
            ["è®¡ç®—æœºè§†è§‰", "computer vision"],
            ["ç‚¹äº‘", "point cloud"],
            ["é‡æ„", "reconstruct"]
        ]
        
        temp_base = Path("/tmp/test_learn_at_pdf")
        temp_base.mkdir(exist_ok=True)
        
        with tempfile.TemporaryDirectory(dir=temp_base) as temp_dir:
            # ä½¿ç”¨@ç¬¦å·å¼•ç”¨PDFæ–‡ä»¶
            description = f'å­¦ä¹ è¿™ä¸ªPDFè®ºæ–‡çš„ä¸»è¦å†…å®¹ @"{test_pdf}"'
            
            result = self.assertCommandSuccess([
                sys.executable, str(self.learn_py),
                "-o", temp_dir, "-m", "åˆå­¦è€…", "-s", "ç®€æ´æ˜äº†",
                "--context",
                "--description", description,
                "æµ‹è¯•PDF@å¼•ç”¨"
            ], timeout=90)  # PDFå¤„ç†éœ€è¦æ—¶é—´ï¼Œä½†åº”è¯¥æ¯”ä¹‹å‰å¿«
            
            # éªŒè¯å†…å®¹è´¨é‡
            found_keywords, missing_keywords = self._extract_keywords_from_content(
                result.stdout, expected_keywords)
            
            coverage_ratio = len(found_keywords) / len(expected_keywords)
            
            print(f"ğŸ” å…³é”®è¯åˆ†æ:")
            print(f"   âœ… æ‰¾åˆ°çš„å…³é”®è¯ ({len(found_keywords)}/{len(expected_keywords)}): {found_keywords}")
            print(f"   âŒ ç¼ºå¤±çš„å…³é”®è¯: {missing_keywords}")
            
            # è¦æ±‚è‡³å°‘60%çš„å…³é”®è¯è¦†ç›–ï¼ˆPDFå¤„ç†å¯èƒ½æœ‰æŸå¤±ï¼‰
            self.assertGreaterEqual(coverage_ratio, 0.6,
                             f"å…³é”®è¯è¦†ç›–ç‡ä¸è¶³: {coverage_ratio:.2f} ({len(found_keywords)}/{len(expected_keywords)})")
            
            # éªŒè¯PDFå¤„ç†æ¶ˆæ¯
            self.assertIn("æ­£åœ¨è§£æPDFæ–‡ä»¶", result.stdout)
            self.assertIn("ä½¿ç”¨basicå¼•æ“", result.stdout)
            
            # éªŒè¯@ç¬¦å·å¼•ç”¨åŠŸèƒ½
            self.assertTrue(
                "æ£€æµ‹åˆ°@æ–‡ä»¶å¼•ç”¨" in result.stdout or "Contextæ¨¡å¼" in result.stdout,
                "æœªæ‰¾åˆ°@æ–‡ä»¶å¼•ç”¨æˆ–Contextæ¨¡å¼çš„ç›¸å…³ä¿¡æ¯"
            )
            
            print(f"âœ… @ç¬¦å·å¼•ç”¨PDFæ–‡ä»¶æµ‹è¯•é€šè¿‡ - å…³é”®è¯è¦†ç›–ç‡: {coverage_ratio:.1%}")

    def test_07g_at_reference_txt_support(self):
        """æµ‹è¯•@ç¬¦å·å¼•ç”¨TXTæ–‡ä»¶æ”¯æŒ - å†…å®¹è´¨é‡éªŒè¯"""
        # åˆ›å»ºæµ‹è¯•TXTæ–‡ä»¶
        test_content = """æ·±åº¦å­¦ä¹ åŸºç¡€æ•™ç¨‹

ç¬¬ä¸€ç« ï¼šç¥ç»ç½‘ç»œç®€ä»‹
ç¥ç»ç½‘ç»œæ˜¯æœºå™¨å­¦ä¹ çš„é‡è¦åˆ†æ”¯ã€‚

ç¬¬äºŒç« ï¼šåå‘ä¼ æ’­ç®—æ³•
åå‘ä¼ æ’­æ˜¯ç¥ç»ç½‘ç»œè®­ç»ƒçš„æ ¸å¿ƒç®—æ³•ã€‚

ç¬¬ä¸‰ç« ï¼šä¼˜åŒ–æ–¹æ³•
å¸¸ç”¨çš„ä¼˜åŒ–æ–¹æ³•åŒ…æ‹¬SGDã€Adamç­‰ã€‚
"""
        
        # é¢„æœŸå…³é”®è¯ï¼ˆåŸºäºTXTå†…å®¹ï¼‰
        expected_keywords = [
            ["æ·±åº¦å­¦ä¹ ", "deep learning"],
            ["ç¥ç»ç½‘ç»œ", "neural network"],
            ["æœºå™¨å­¦ä¹ ", "machine learning"],
            ["åå‘ä¼ æ’­", "backpropagation"],
            ["ç®—æ³•", "algorithm"],
            ["ä¼˜åŒ–", "optimization"],
            ["SGD"],
            ["Adam"],
            ["è®­ç»ƒ", "training"],
            ["æ ¸å¿ƒ", "core"],
            ["æ–¹æ³•", "method"],
            ["åŸºç¡€", "basic", "fundamental"],
            ["æ•™ç¨‹", "tutorial"],
            ["æŠ€æœ¯", "technology"],
            ["å­¦ä¹ ", "learning"],
            ["ç½‘ç»œ", "network"]
        ]
        
        temp_base = Path("/tmp/test_learn_at_txt")
        temp_base.mkdir(exist_ok=True)
        
        try:
            # åˆ›å»ºæµ‹è¯•TXTæ–‡ä»¶
            test_file = temp_base / "test_tutorial.txt"
            with open(test_file, 'w', encoding='utf-8') as f:
                f.write(test_content)
            
            with tempfile.TemporaryDirectory(dir=temp_base) as temp_dir:
                # ä½¿ç”¨@ç¬¦å·å¼•ç”¨TXTæ–‡ä»¶
                description = f'åŸºäºè¿™ä¸ªæ•™ç¨‹å†…å®¹è¿›è¡Œæ·±å…¥å­¦ä¹  @"{test_file}"'
                
                result = self.assertCommandSuccess([
                    sys.executable, str(self.learn_py),
                    "-o", temp_dir, "-m", "ä¸­çº§", "-s", "è¯¦ç»†æ·±å…¥",
                    "--context",
                    "--description", description,
                    "åŸºäºTXTæ–‡ä»¶çš„å­¦ä¹ "
                ], timeout=30)  # åº”è¯¥å¾ˆå¿«
                
                # éªŒè¯å†…å®¹è´¨é‡
                found_keywords, missing_keywords = self._extract_keywords_from_content(
                    result.stdout, expected_keywords)
                
                coverage_ratio = len(found_keywords) / len(expected_keywords)
                
                print(f"ğŸ” å…³é”®è¯åˆ†æ:")
                print(f"   âœ… æ‰¾åˆ°çš„å…³é”®è¯ ({len(found_keywords)}/{len(expected_keywords)}): {found_keywords}")
                print(f"   âŒ ç¼ºå¤±çš„å…³é”®è¯: {missing_keywords}")
                
                # è¦æ±‚è‡³å°‘80%çš„å…³é”®è¯è¦†ç›–ï¼ˆTXTæ–‡ä»¶å¤„ç†æœ€å‡†ç¡®ï¼‰
                self.assertGreaterEqual(coverage_ratio, 0.8,
                                 f"å…³é”®è¯è¦†ç›–ç‡ä¸è¶³: {coverage_ratio:.2f} ({len(found_keywords)}/{len(expected_keywords)})")
                
                # éªŒè¯@ç¬¦å·å¼•ç”¨åŠŸèƒ½
                self.assertTrue(
                    "æ£€æµ‹åˆ°@æ–‡ä»¶å¼•ç”¨" in result.stdout or "Contextæ¨¡å¼" in result.stdout,
                    "æœªæ‰¾åˆ°@æ–‡ä»¶å¼•ç”¨æˆ–Contextæ¨¡å¼çš„ç›¸å…³ä¿¡æ¯"
                )
                
                print(f"âœ… @ç¬¦å·å¼•ç”¨TXTæ–‡ä»¶æµ‹è¯•é€šè¿‡ - å…³é”®è¯è¦†ç›–ç‡: {coverage_ratio:.1%}")
                
        finally:
            # æ¸…ç†æµ‹è¯•æ–‡ä»¶
            if test_file.exists():
                test_file.unlink()

            print("âœ… @ç¬¦å·å¼•ç”¨åŒè®ºæ–‡æ¯”è¾ƒæµ‹è¯•é€šè¿‡")

    def test_08_file_override_handling(self):
        """æµ‹è¯•æ–‡ä»¶è¦†ç›–å¤„ç†çš„ä¸åŒæ¨¡å¼"""
        base_output = Path("/tmp/test_learn_override")
        base_output.mkdir(exist_ok=True)
        
        try:
            # æµ‹è¯•1ï¼šé»˜è®¤æ¨¡å¼ï¼ˆåº”è¯¥è¦†ç›–ï¼‰
            target_dir = base_output / "test_default"
            target_dir.mkdir(exist_ok=True)
            
            # åˆ›å»ºå·²å­˜åœ¨çš„æ–‡ä»¶
            (target_dir / "tutorial.md").write_text("existing tutorial")
            (target_dir / "question.md").write_text("existing questions")
            
            result = self.assertCommandSuccess([
                sys.executable, str(self.learn_py),
                "-o", str(target_dir), "-m", "åˆå­¦è€…", "-s", "ç®€æ´æ˜äº†",
                "--brainstorm-only",
                "é»˜è®¤æ¨¡å¼æµ‹è¯•ä¸»é¢˜"
            ], timeout=60)
            
            self.assertIn("å¤´è„‘é£æš´", result.stdout)
            self.assertIn("é»˜è®¤æ¨¡å¼", result.stdout)
            print("âœ… é»˜è®¤æ¨¡å¼æµ‹è¯•é€šè¿‡")
            
            # æµ‹è¯•2ï¼šno-override-materialæ¨¡å¼ï¼ˆåº”è¯¥è‡ªåŠ¨é‡å‘½åï¼‰
            target_dir2 = base_output / "test_no_override"
            target_dir2.mkdir(exist_ok=True)
            
            # åˆ›å»ºå·²å­˜åœ¨çš„æ–‡ä»¶
            (target_dir2 / "tutorial.md").write_text("existing tutorial")
            
            result2 = self.assertCommandSuccess([
                sys.executable, str(self.learn_py),
                "-o", str(target_dir2), "-m", "åˆå­¦è€…", "-s", "ç®€æ´æ˜äº†",
                "--no-override-material", "--brainstorm-only",
                "è‡ªåŠ¨é‡å‘½åæµ‹è¯•ä¸»é¢˜"
            ], timeout=60)
            
            self.assertIn("å¤´è„‘é£æš´", result2.stdout)
            # æ£€æŸ¥æ˜¯å¦åˆ›å»ºäº†é‡å‘½åç›®å½•
            renamed_dirs = [d for d in base_output.iterdir() if d.name.startswith("test_no_override_")]
            auto_rename_worked = len(renamed_dirs) > 0 or "è‡ªåŠ¨é‡å‘½å" in result2.stdout
            
            print(f"âœ… è‡ªåŠ¨é‡å‘½åæµ‹è¯•é€šè¿‡ - é‡å‘½åç›®å½•æ•°: {len(renamed_dirs)}")
            
        finally:
            # æ¸…ç†æµ‹è¯•ç›®å½•
            import shutil
            if base_output.exists():
                shutil.rmtree(base_output)




class TestLearnAPI(APITest):
    """API tests for LEARN tool that require longer timeouts"""

    def setUp(self):
        super().setUp()
        self.learn_py = self.get_python_path('LEARN.py')

    def test_learn_direct_mode_with_output_dir(self):
        """Test LEARN direct mode with output directory"""
        result = self.assertCommandSuccess([
            sys.executable, str(self.learn_py), 
            "Pythonç¼–ç¨‹", "--mode", "Advanced", "--style", "Witty", 
            "--output-dir", "/tmp/test-learn"
        ])
        self.assertIn('æ­£åœ¨ç”Ÿæˆå­¦ä¹ å†…å®¹ç»“æ„', result.stdout)

    def test_learn_basic_functionality(self):
        """Test basic LEARN functionality"""
        result = self.assertCommandSuccess([
            sys.executable, str(self.learn_py),
            "æµ‹è¯•ä¸»é¢˜", "--mode", "Beginner", "--output-dir", "/tmp/test"
        ])
        self.assertIn('æ­£åœ¨ç”Ÿæˆå­¦ä¹ å†…å®¹ç»“æ„', result.stdout)

    def test_learn_paper_mode(self):
        """Test LEARN paper mode"""
        # Create a dummy PDF file
        dummy_pdf = "/tmp/test_paper.pdf"
        with open(dummy_pdf, 'w') as f:
            f.write("dummy pdf content")
        
        try:
            result = self.assertCommandSuccess([
                sys.executable, str(self.learn_py),
                dummy_pdf, "--mode", "Beginner", "--output-dir", "/tmp/test"
            ])
            self.assertIn('æ­£åœ¨ç”Ÿæˆå­¦ä¹ å†…å®¹ç»“æ„', result.stdout)
        finally:
            # Clean up
            if os.path.exists(dummy_pdf):
                os.remove(dummy_pdf)

    def test_learn_pdf_mode(self):
        """Test LEARN --pdf mode"""
        # Create a dummy PDF file
        dummy_pdf = "/tmp/test_paper.pdf"
        with open(dummy_pdf, 'w') as f:
            f.write("dummy pdf content")
        
        try:
            result = self.run_subprocess([
                sys.executable, str(self.learn_py),
                "--pdf", dummy_pdf, "--output", "/tmp/test", "æµ‹è¯•PDFå¤„ç†"
            ])
            # Should attempt to process PDF (may fail due to dummy content, but should try)
            self.assertIn('ç›´æ¥å¤„ç†PDFæ–‡ä»¶', result.stdout)
        finally:
            # Clean up
            if os.path.exists(dummy_pdf):
                os.remove(dummy_pdf)

    def test_learn_gen_command(self):
        """Test LEARN --gen-command feature"""
        result = self.run_subprocess([
            sys.executable, str(self.learn_py),
            "--gen-command", "æˆ‘æƒ³å­¦ä¹ æ·±åº¦å­¦ä¹ åŸºç¡€"
        ])
        # Should generate a LEARN command
        self.assertIn('LEARN', result.stdout)

    def test_learn_file_reference_parsing(self):
        """Test file reference parsing functionality"""
        # This test checks if the file reference parsing logic exists
        # We can't easily test the full functionality without actual files
        try:
            import LEARN
            # Check if the file reference functions exist
            self.assertTrue(hasattr(LEARN, 'expand_file_references') or 
                          'expand_file_references' in dir(LEARN))
        except ImportError:
            self.skipTest("LEARN module not available for direct import")


if __name__ == '__main__':
    import unittest
    unittest.main() 