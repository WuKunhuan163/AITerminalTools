problem of generating images or videos from text, and it is solved by learning a prior, or conditional distribution, from billions of data samples. However, data of this size is unavailable for 3D objects. Authors address this problem by involving 2D image or video generators in the 3D generation process. We distinguish two main camps: multi-view direct and single-view latent 3D generation.

Multi-view 3D Generation. In multi-view 3D generation, one asks the image generator to do most of the lifting, generating several views of the 3D objects, and thus simplifying extracting a 3D object from them. First, this was done using Score Distillation Sampling (SDS) [41], an idea explored extensively in follow-ups like GET3D [13], ProlificDreamer [52], Dream Gaussians [49], Lucid Dreamer [9] which seek to achieve multi-view consistency via iterative (and slow) optimization of a radiance field (NeRF [43] or 3DGS [23]). A significant innovation, pioneered by UpFusion [22], 3DiM [53], Zero-1-to-3 [35] and MVDream [44], was to fine-tune the image generator to directly produce multiple consistent views of the object. By making the image generator more 3D aware, 3D reconstruction becomes simpler, as noted in InstantMesh [55], GRM [56], and others [54].

Single-view Latent 3D Generation. The alternative approach is to start from a single image of the object and directly reconstruct the 3D object from it. Because single-view reconstruction is extremely ambiguous, this requires to learn a reconstruction function. This was the path taken, for example, by LRM [19] and others [17, 47]. However, their deterministic reconstruction model cannot cope well with this ambiguity and often produces blurry outputs. Much better results were recently obtained by switching to stochastic 3D reconstruction based on latent diffusion. Some of the best single-image 3D reconstructors are based on the 3DShapeToVecSet [62] latent representation (also similar to Michelangelo’s [64]). Building on it, CLAY [63], DreamCraft3D [46], CraftsMan [29], TripoSG [30], and others [10, 57, 60] are able to generate highly detailed and accurate 3D shapes. We build on this representation as well and show that it also supports compositionality very effectively.

Composable 3D Generation. Approaches to composable 3D generation typically start by decomposing objects into constituent parts. One common strategy represents objects as mixtures of primitives, often without semantic labels. For instance, SIF [16] models object occupancy using mixtures of 3D Gaussians. LDIF [15] represents shapes as a set of local deep implicit functions (DIFs), spatially arranged and blended using a template of Gaussian primitives. Methods such as Neural Template [20] and SPAGHETTI [1] achieve decompositions through auto-decoding. SALAD [28] utilizes SPAGHETTI for diffusion-based generation. PartNeRF [50] expands this concept by employing mixtures of NeRFs. NeuForm [31] and DifFacto [38] specifically target part-level controllability. DBW [37] uses textured superquadrics to decompose scenes. In contrast, another research direction emphasizes explicitly semantic parts. PartSLIP [34] and PartSLIP++ [67] segment objects into semantic components from point clouds using vision-language models. Part123 [33] adapts techniques from scene-level approaches like Contrastive Lift [2] to reconstruct object parts. PartSDF [48] learns latent codes for parts using an auto-decoder and then uses SALAD for part prediction. Comboverse [8] leverages single-view inpainting model and 3D generator for composable 3D generation with spatial-aware SDS optimization. HoloPart [58], a recent work, starts from the shell of a 3D object and a part-level segmentation for it and performs 3D amodal part completion.

The work most related to ours is PartGen [5]. This squarely sits on the ‘multi-view direct’ camp (see above). It uses multi-view diffusion models for segmentation and completion of compositional 3D objects from diverse modalities.

3D Segmentation. 3D parts can be obtained by segmenting a 3D object (although the resulting parts will generally be incomplete). Some approaches for semantic 3D segmentation such as [65, 27, 51, 24, 2] used neural fields to ‘fuse’ 2D semantic features in 3D. Contrastive Lift [2] introduced a slow-fast contrastive clustering scheme for 3D instance segmentation. Recent methods such as [25, 61, 42, 3] integrate SAM [26] and often CLIP to model multi-scale concepts, where LangSplat explicitly encodes scale information and N2F2 learns to bind concepts to scales automatically. Neural Part Priors [4] used learned priors for test-time decomposition. Additionally, efforts to develop 3D ‘foundation’ models [66, 7] are enabling zero-shot point cloud segmentation across diverse domains.

# 3 Method

Let x → R3 be a 3D object given by a surface embedded in R3. We assume that the object is compositional, meaning that it can be expressed as the union x = !Kk=1 x(k) of K disjoint parts