# Analysis Report for: GaussianObject: High-Quality 3D Object Reconstruction from Four Views with Gaussian Splatting

```yaml
Source PDF: GaussianObject High-Quality 3D Object Reconstruction from Four Views with Gaussian Splatting.pdf
Author(s): 
Analysis Mode: academic
Layout Mode: arxiv
API Called: True
```

---


## ğŸ“„ Page 4

[Main Text]
where ğ‘tex and ğ‘img denote the text and image conditioning respectively, and ğœ–ğœƒis the Gaussian noise inferred by the diffusion model with parameter ğœƒ, Â¯ğ›¼1:ğ‘‡âˆˆ(0, 1]ğ‘‡is a decreasing sequence associated with the noise-adding process.

Given a sparse collection of ğ‘reference images ğ‘‹ref = {ğ‘¥ğ‘–}ğ‘ ğ‘–=1, captured within a 360â—¦range and encompassing one object, along with the corresponding camera intrinsics2 ğ¾ref = {ğ‘˜ğ‘–}ğ‘ ğ‘–=1, extrinsics Î ref = {ğœ‹ğ‘–}ğ‘ ğ‘–=1 and masks ğ‘€ref = {ğ‘šğ‘–}ğ‘ ğ‘–=1 of the object, our target is to obtain a 3D representation G, which can achieve photo-realistic rendering ğ‘¥= G(ğœ‹|{ğ‘¥ğ‘–, ğœ‹ğ‘–,ğ‘šğ‘–}ğ‘ ğ‘–=1) from any viewpoint. To achieve this, we employ the 3DGS model for its simplicity for structure priors embedding and fast rendering capabilities. The process begins with initializing 3D Gaussians using a visual hull [Laurentini 1994], followed by optimization with floater elimination, enhancing the structure of Gaussians. Then we design self-generating strategies to supply sufficient image pairs for constructing a Gaussian repair model, which is used to rectify incomplete object information. The overall framework is shown in Fig. 2.

Sparse views, especially for only 4 images, provide limited 3D information for reconstruction. In this case, SfM points, which are the key for 3DGS initialization, are often absent. Besides, insufficient multi-view consistency leads to ambiguity among shape and appearance, resulting in many floaters during reconstruction. We propose two techniques to initially optimize the 3D Gaussian representation, which take full advantage of structure priors from the limited views and result in a satisfactory outline of the object. Initialization with Visual Hull. To better leverage object structure information from limited reference images, we utilize the view frustums and object masks to create a visual hull as a geometric scaffold for initializing our 3D Gaussians. Compared with the limited number of SfM points in extremely sparse settings, the visual hull provides more structure priors that help build multiview consistency by excluding unreasonable Gaussian distributions. The cost of the visual hull is just several masks derived from sparse 360â—¦images, which can be easily acquired using current segmentation models such as SAM [Kirillov et al. 2023]. Specifically, points are randomly initialized within the visual hull using rejection sampling: we project uniformly sampled random 3D points onto image planes and retain those within the intersection of all image-space masks. Point colors are averaged from bilinearly interpolated pixel colors across reference image projections. Then we transform these 3D points into 3D Gaussians. For each point, we assign its position as ğœ‡and convert its color into ğ‘ â„. The mean distance between adjacent points forms the scale ğ‘ , while the rotation ğ‘is set to a unit quaternion as default. The opacity ğœis initialized to a constant value. This 4 â€¢ Yang, Li, et al.

image conditioning by integrating a network structure similar to the diffusion model, optimized with the loss function:

3.2 Overall Framework 3.3 Initial Optimization with Structure Priors 2Given that the camera intrinsics are known and fixed, we exclude them from the rendering function for simplicity.

Lğ¶ğ‘œğ‘›ğ‘‘= Eğ‘0,ğ‘¡,ğœ–[âˆ¥ğœ–ğœƒ(âˆšÂ¯ğ›¼ğ‘¡ğ‘0 + âˆš1 âˆ’Â¯ğ›¼ğ‘¡ğœ–,ğ‘¡,ğ‘tex,ğ‘img) âˆ’ğœ–âˆ¥2 2], (1) Combining visual hull initialization and floater elimination significantly enhances 3DGS performance for NVS in sparse 360â—¦contexts. While the fidelity of our reconstruction is generally passable, Gğ‘ ğ‘ƒGaussians for each element in Gğ‘. We then establish a normative range by computing the mean and standard deviation of these distances. Based on statistical analysis, we exclude Gaussians whose mean neighbor distances exceed the adaptive threshold ğœ= mean + ğœ†ğ‘’std. This thresholding process is repeated periodically throughout optimization, where ğœ†ğ‘’is linearly decreased to 0 to refine the scene representation progressively. Initial Optimization The optimization of Gğ‘incorporates color, mask, and monocular depth losses. The color loss combines L1 and D-SSIM losses from 3D Gaussian Splatting:

where ğ‘¥is the rendering and ğ‘¥ref is the corresponding reference image. A binary cross entropy (BCE) loss [Jadon 2020] is applied as mask loss:

where ğ‘šdenotes the object mask. A shift and scale invariant depth loss is utilized to guide geometry:

where ğ·âˆ—and ğ·âˆ— pred are per-frame rendered depths and monocularly estimated depths [Bhat et al. 2023] respectively. The depth values are computed following a normalization strategy [Ranftl et al. 2020]:

where ğ‘€denotes the number of valid pixels. The overall loss combines these components:

where ğœ†SSIM, ğœ†m, and ğœ†d control the magnitude of each term. Thanks to the efficient initialization, our training speed is remarkably fast. It only takes 1 minute to train a coarse Gaussian representation Gğ‘ at a resolution of 779 Ã— 520.

initialization strategy relies on the initial masks. Despite potential inaccuracies in these masks or unrepresented concavities by the visual hull, we observed that subsequent optimization processes reliably yield high-quality reconstructions. Floater Elimination. While the visual hull builds a coarse estimation of the object geometry, it often contains regions that do not belong to the object due to the inadequate coverage of reference images. These regions usually appear to be floaters, damaging the quality of novel view synthesis. These floaters are problematic as the optimization process struggles to adjust them due to insufficient observational data regarding their position and appearance. To mitigate this issue, we utilize the statistical distribution of distances among the 3D Gaussians to distinguish the primary object and the floaters. This is implemented by the K-Nearest Neighbors (KNN) algorithm, which calculates the average distance to the nearest âˆš 3.4 Gaussian Repair Model Setup Lref = (1 âˆ’ğœ†SSIM)L1 + ğœ†SSIMLDâˆ’SSIM + ğœ†mLm + ğœ†dLd, (6) L1 = âˆ¥ğ‘¥âˆ’ğ‘¥refâˆ¥1, LD-SSIM = 1 âˆ’SSIM(ğ‘¥,ğ‘¥ref), (2) Lm = âˆ’(ğ‘šref logğ‘š+ (1 âˆ’ğ‘šref) log(1 âˆ’ğ‘š)), (3) ğ·âˆ—= ğ·âˆ’median(ğ·) 1 ğ‘€ Ãğ‘€ ğ‘–=1 |ğ·âˆ’median(ğ·)| , (5) Ld = âˆ¥ğ·âˆ—âˆ’ğ·âˆ— predâˆ¥1, (4)

[Publication Info]
ACM Trans. Graph., Vol. 43, No. 6, Article . Publication date: December 2024.

## ğŸ“„ Page 5

[Main Text]
where ğ‘tex denotes an object-specific language prompt, defined as â€œa photo of [V],â€ as per Dreambooth [Ruiz et al. 2023]. Specifically, we inject LoRA layers into the text encoder, image condition branch, and U-Net for fine-tuning. Please refer to the Appendix for details.

still suffers in regions that are poorly observed, regions with occlusion, or even unobserved regions. These challenges loom over the completeness of the reconstruction, like the sword of Damocles. To mitigate these issues, we introduce a Gaussian repair model R designed to correct the aberrant distribution of Gğ‘. Our R takes corrupted rendered images ğ‘¥â€²(Gğ‘, ğœ‹nov) as input and outputs photorealistic and high-fidelity images Ë†ğ‘¥. This image repair capability can be used to refine the 3D Gaussians, leading to learning better structure and appearance details. Sufficient data pairs are essential for training R but are rare in existing datasets. To this end, we adopt two main strategies for generating adequate image pairs, i.e., leave-one-out training and adding 3D noises. For leave-one-out training, we build ğ‘subsets from the ğ‘input images, each containing ğ‘âˆ’1 reference images and 1 left-out image ğ‘¥out. Then we train ğ‘3DGS models with reference images of these subsets, termed as {Gğ‘–ğ‘}ğ‘âˆ’1 ğ‘–=0 . After specific iterations, we use the left-out image ğ‘¥out to continue training each Gaussian model {Gğ‘–ğ‘}ğ‘âˆ’1 ğ‘–=0 into { Ë†Gğ‘–ğ‘}ğ‘âˆ’1 ğ‘–=0 . Throughout this process, the rendered images from the left-out view at different iterations are stored to form the image pairs along with left-out image ğ‘¥out for training the repair model. Note that training these left-out models costs little, with less than ğ‘minutes in total. The other strategy is to add 3D noises ğœ–ğ‘ onto Gaussian attributes. The ğœ–ğ‘ are derived from the mean ğœ‡Î” and variance ğœÎ” of attribute differences between {Gğ‘–ğ‘}ğ‘âˆ’1 ğ‘–=0 and { Ë†Gğ‘–ğ‘}ğ‘âˆ’1 ğ‘–=0 . This allows us to render more degraded images ğ‘¥â€²(Gğ‘(ğœ–ğ‘ ), ğœ‹ref) at all reference views from the created noisy Gaussians, resulting in extensive image pairs (ğ‘‹â€²,ğ‘‹ref). We inject LoRA weights and fine-tune a pre-trained ControlNet [Zhang et al. 2023b] using the generated image pairs as our Gaussian repair model. The training procedure is shown in Fig. 3. The loss function, based on Eq. 1, is defined as:

Ltune = Eğ‘¥ref,ğ‘¡,ğœ–,ğ‘¥â€² h âˆ¥(ğœ–ğœƒ(ğ‘¥ref ğ‘¡,ğ‘¡,ğ‘¥â€²,ğ‘tex) âˆ’ğœ–)âˆ¥2 2 i , (7) GaussianObject: High-Quality 3D Object Reconstruction from Four Views with Gaussian Splatting â€¢ 5 which is similar to SDEdit [Meng et al. 2022]. We then generate a sample Ë†ğ‘¥ğ‘—from R by running DDIM sampling [Song et al. 2021] over ğ‘˜= âŒŠ50 Â· ğ‘¡ where E and D are from the VAE model used by the diffusion model. The distances from ğœ‹ğ‘—to Î ğ‘Ÿğ‘’ğ‘“is used to weight the reliability of Ë†ğ‘¥ğ‘—, guiding the optimization with a loss function:

where P is an estimated coarse point cloud of the scene, and Ë†Î ref, Ë†ğ¾ref are the predicted camera poses and intrinsics of ğ‘‹ref, respectively. For CF-GaussianObject, we modify the intrinsic recovery module within DUSt3R, allowing ğ‘¥ğ‘–âˆˆğ‘‹ref to share the same intrinsic Ë†ğ¾. This adaption enables the retrieval of P, Ë†Î ref, and Ë†ğ¾. Besides, After training R, we distill its target object priors into Gğ‘to refine its rendering quality. The object information near the reference views is abundant. This observation motivates designing distance as a criterion in identifying views that need rectification, leading to distance-aware sampling. Specifically, we establish an elliptical path aligned with the training views and focus on a central point. Arcs near Î ğ‘Ÿğ‘’ğ‘“, where we assume Gğ‘renders high-quality images, form the reference path. The other arcs, yielding renderings, need to be rectified and define the repair path, as depicted in Fig. 4. In each iteration, novel viewpoints, ğœ‹ğ‘—âˆˆÎ nov, are randomly sampled among the repair path. For each ğœ‹ğ‘—, we render the corresponding image ğ‘¥ğ‘—(Gğ‘, ğœ‹ğ‘—), encode it to be E(ğ‘¥ğ‘—) by the latent diffusion encoder E and pass E(ğ‘¥ğ‘—) to the image conditioning branch of R. Simultaneously, a cloned E(ğ‘¥ğ‘—) is disturbed into a noisy latent ğ‘§ğ‘¡:

3.5 Gaussian Repair with Distance-Aware Sampling Here, ğ¿ğ‘denotes the perceptual similarity metric LPIPS [Zhang et al. 2018], ğ‘¤(ğ‘¡) is a noise-level modulated weighting function from DreamFusion [Poole et al. 2023], ğœ†(ğœ‹ğ‘—) denotes a distancebased weighting function, and ğ‘‘max is the maximal distance among neighboring reference viewpoints. To ensure coherence between 3D Gaussians and reference images, we continue training Gğ‘with Lref during the whole Gaussian repair procedure.

3.6 COLMAP-Free GaussianObject (CF-GaussianObject) Current SOTA sparse view reconstruction methods rely on precise camera parameters, including intrinsics and poses, obtained through an SfM pipeline with dense input, limiting their usability in daily applications. This process can be cumbersome and unreliable in sparse-view scenarios where matched features are insufficient for accurate reconstruction. To overcome this limitation, we introduce an advanced sparse matching model, DUSt3R [Wang et al. 2024a], into GaussianObject to enable COLMAP-free sparse 360â—¦reconstruction. Given reference input images ğ‘‹ref, DUSt3R is formulated as:

Lrep = Eğœ‹ğ‘—,ğ‘¡  ğ‘¤(ğ‘¡)ğœ†(ğœ‹ğ‘—) âˆ¥ğ‘¥ğ‘—âˆ’Ë†ğ‘¥ğ‘—âˆ¥1 + âˆ¥ğ‘¥ğ‘—âˆ’Ë†ğ‘¥ğ‘—âˆ¥2 + ğ¿ğ‘(ğ‘¥ğ‘—, Ë†ğ‘¥ğ‘—) , ğ‘§ğ‘¡= âˆšÂ¯ğ›¼ğ‘¡E(ğ‘¥ğ‘—) + âˆš1 âˆ’Â¯ğ›¼ğ‘¡ğœ–, where ğœ–âˆ¼N (0, ğ¼),ğ‘¡âˆˆ[0,ğ‘‡], (8) where ğœ†(ğœ‹ğ‘—) = 2 Â· minğ‘ ğ‘–=1(âˆ¥ğœ‹ğ‘—âˆ’ğœ‹ğ‘–âˆ¥2) ğ‘‡âŒ‹steps and forwarding the diffusion decoder D:

P, Ë†Î ref, Ë†ğ¾ref = DUSt3R(ğ‘‹ref), (11) Ë†ğ‘¥ğ‘—= D(DDIM(ğ‘§ğ‘¡, E(ğ‘¥ğ‘—))), (9) ğ‘‘max . (10)

[Figure]
Screenshot Path: /Users/wukunhuan/.local/bin/pdf_extractor/pdf_extractor_data/images/figure_screenshot_f2cf31f8.png
Associated Text: Fig. 3. Illustration of Gaussian repair model setup. First, we add Gaussian noise ğœ–to a reference image ğ‘¥ref to form a noisy image. Next, this noisy image along with ğ‘¥refâ€™s corresponding degraded image ğ‘¥â€² are passed to a pre-trained fixed ControlNet with learnable LoRA layers to predict a noise distribution ğœ–ğœƒ. We use the differences among ğœ–and ğœ–ğœƒto fine-tune the parameters in LoRA layers.
Analysis: 
== Description Starts Here ==
*[APIè°ƒç”¨å¤±è´¥ï¼šæ‰€æœ‰é…ç½®çš„APIå¯†é’¥éƒ½æ— æ³•æˆåŠŸè·å–å›å¤ã€‚]*
=== Description Ends Here ===


[Publication Info]
ACM Trans. Graph., Vol. 43, No. 6, Article . Publication date: December 2024.