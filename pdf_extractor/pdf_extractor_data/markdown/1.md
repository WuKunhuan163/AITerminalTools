# GaussianObject: High-Quality 3D Object Reconstruction from Four Views with Gaussian Splatting

CHEN YANG∗, MoE Key Lab of Artificial Intelligence, AI Institute, SJTU, China   
SIKUANG LI∗, MoE Key Lab of Artificial Intelligence, AI Institute, SJTU, China   
JIEMIN FANG†, Huawei Inc., China   
RUOFAN LIANG, University of Toronto, Canada   
LINGXI XIE, Huawei Inc., China   
XIAOPENG ZHANG, Huawei Inc., China   
WEI SHEN‡, MoE Key Lab of Artificial Intelligence, AI Institute, SJTU, China   
QI TIAN, Huawei Inc., China

![](images/2d9ad8e3e84e0ceabbd9075f3232ca7fcb1bd2d1ea36a413088970c612d06809.jpg)  
Fig. 1. We introduce GaussianObject, a framework capable of reconstructing high-quality 3D objects from only 4 images with Gaussian splatting. Gaus sianObject demonstrates superior performance over previous state-of-the-art (SOTA) methods on challenging objects.

Reconstructing and rendering 3D objects from highly sparse views is of critical importance for promoting applications of 3D vision techniques and improving user experience. However, images from sparse views only contain very limited 3D information, leading to two significant challenges: 1) Difficulty in building multi-view consistency as images for matching are too few; 2) Partially omitted or highly compressed object information as view coverage is insufficient. To tackle these challenges, we propose GaussianObject, a framework to represent and render the 3D object with Gaussian splatting that achieves high rendering quality with only 4 input images. We first introduce techniques of visual hull and floater elimination, which explicitly inject structure priors into the initial optimization process to help build multi-view consistency, yielding a coarse 3D Gaussian representation. Then we construct a Gaussian repair model based on diffusion models to supplement the omitted object information, where Gaussians are further refined. We design a self-generating strategy to obtain image pairs for training the repair model. We further design a COLMAP-free variant, where pre-given accurate camera poses are not required, which achieves competitive quality and facilitates wider applications. GaussianObject is evaluated on several challenging datasets, including MipNeRF360, OmniObject3D, OpenIllumination, and our-collected unposed images, achieving superior performance from only four views and significantly outperforming previous SOTA methods. Our demo is available at https:// gaussianobject.github.io/ , and the code has been released at https:// github.com/ GaussianObject/ GaussianObject.

# CCS Concepts: • Computing methodologies → Reconstruction; Rendering; Point-based models.

Additional Key Words and Phrases: Sparse view reconstruction, 3D Gaussian Splatting, ControlNet, Visual hull, Novel view synthesis

# ACM Reference Format:

Chen Yang, Sikuang Li, Jiemin Fang, Ruofan Liang, Lingxi Xie, Xiaopeng Zhang, Wei Shen, and Qi Tian. 2024. GaussianObject: High-Quality 3D Object

Reconstruction from Four Views with Gaussian Splatting. ACM Trans. Graph. 43, 6 (December 2024), 28 pages. https://doi.org/10.1145/3687759

# 1 INTRODUCTION

Reconstructing and rendering 3D objects from 2D images has been a long-standing and important topic, which plays critical roles in a vast range of real-life applications. One key factor that impedes users, especially ones without expert knowledge, from widely using these techniques is that usually dozens of multi-view images need to be captured, which is cumbersome and sometimes impractical. Efficiently reconstructing high-quality 3D objects from highly sparse captured images is of great value for expediting downstream applications such as 3D asset creation for game/movie production and AR/VR products.

In recent years, a series of methods [Guangcong et al. 2023; Jain et al. 2021; Niemeyer et al. 2022; Shi et al. 2024b; Song et al. 2023b; Yang et al. 2023; Zhou and Tulsiani 2023; Zhu et al. 2024] have been proposed to reduce reliance on dense captures. However, it is still challenging to produce high-quality 3D objects when the views become extremely sparse, e.g. only 4 images in a 360◦ range, as shown in Fig. 1. We delve into the task of sparse-view reconstruction and discover two main challenges behind it. The first one lies in the difficulty of building multi-view consistency from highly sparse input. The 3D representation is easy to overfit the input images and degrades into fragmented pixel patches of training views without reasonable structures. The other challenge is that with sparse captures in a 360◦ range, some content of the object can be inevitably omitted or severely compressed when observed from extreme views1. The omitted or compressed information is impossible or hard to be reconstructed in 3D only from the input images.

To tackle the aforementioned challenges, we introduce GaussianObject, a novel framework designed to reconstruct high-quality 3D objects from as few as 4 input images. We choose 3D Gaussian splatting (3DGS) [Kerbl et al. 2023] as the basic representation as it is fast and, more importantly, explicit enough. Benefiting from its point-like structure, we design several techniques for introducing object structure priors, e.g. the basic/rough geometry of the object, to help build multi-view consistency, including visual hull [Laurentini 1994] to locate Gaussians within the object outline and floater elimination to remove outliers. To erase artifacts caused by omitted or highly compressed object information, we propose a Gaussian repair model driven by 2D large diffusion models [Rombach et al. 2022], translating corrupted rendered images into high-fidelity ones. As normal diffusion models lack the ability to repair corrupted images, we design self-generating strategies to construct image pairs to tune the diffusion models, including rendering images from leaveone-out training models and adding 3D noises to Gaussian attributes. Images generated from the repair model can be used to refine the 3D Gaussians optimized with structure priors, where the rendering quality can be further improved. To further extend GaussianObject to practical applications, we introduce a COLMAP-free variant of GaussianObject (CF-GaussianObject), which achieves competitive reconstruction performance on challenging datasets with only four input images without inputting accurate camera parameters.

Our contributions are summarized as follows:

• We optimize 3D Gaussians from highly sparse views using explicit structure priors, introducing techniques of visual hull for initialization and floater elimination for training. • We propose a Gaussian repair model based on diffusion models to remove artifacts caused by omitted or highly compressed information, where the rendering quality can be further improved. • The overall framework GaussianObject consistently outperforms current SOTA methods on several challenging real-world datasets, both qualitatively and quantitatively. A COLMAP-free variant is further presented for wider applications, weakening the requirement of accurate camera poses.

# 2 RELATED WORK

Vanilla NeRF struggles in sparse settings. Techniques like Deng et al. [2022]; Roessle et al. [2022]; Somraj et al. [2024, 2023]; Somraj and Soundararajan [2023] use Structure from Motion (SfM) [Schönberger and Frahm 2016] derived visibility or depth and mainly focus on closely aligned views. Xu et al. [2022] uses ground truth depth maps, which are costly to obtain in real-world images. Some methods [Guangcong et al. 2023; Song et al. 2023b] estimate depths with monocular depth estimation models [Ranftl et al. 2021, 2022] or sensors, but these are often too coarse. Jain et al. [2021] uses a vision-language model [Radford et al. 2021] for unseen view rendering, but the semantic consistency is too high-level to guide lowlevel reconstruction. Shi et al. [2024b] combines a deep image prior with factorized NeRF, effectively capturing overall appearance but missing fine details in input views. Priors based on information theory [Kim et al. 2022], continuity [Niemeyer et al. 2022], symmetry [Seo et al. 2023], and frequency regularization [Song et al. 2023a; Yang et al. 2023] are only effective for specific scenarios, limiting their further applications. Besides, there are some methods [Jang and Agapito 2024; Jiang et al. 2024; Xu et al. 2024c; Zou et al. 2024] that employ Vision Transformer (ViT) [Dosovitskiy et al. 2021] to reduce the requirements for constructing NeRFs and Gaussians.

The recent progress in diffusion models has spurred notable advancements in 3D applications. Dreamfusion [Poole et al. 2023] proposes Score Distillation Sampling (SDS) for distilling NeRFs with 2D priors from a pre-trained diffusion model for 3D object generation from text prompts. It has been further refined for textto-3D [Chen et al. 2023; Lin et al. 2023; Metzer et al. 2023; Shi et al. 2024a; Tang et al. 2024b; Wang et al. 2023a,b; Yi et al. 2024] and 3D/4D editing [Haque et al. 2023; Shao et al. 2024] by various studies, demonstrating the versatility of 2D diffusion models in 3D contexts. Burgess et al. [2024]; Chan et al. [2023]; Liu et al. [2023c]; Müller et al. [2024]; Pan et al. [2024]; Zhu and Zhuang [2024] have adapted these methods for 3D generation and view synthesis from a single image, while they often have strict input requirements and can produce overly saturated images. In sparse reconstruction, approaches like DiffusioNeRF [Wynn and Turmukhambetov 2023], SparseFusion [Zhou and Tulsiani 2023], Deceptive-NeRF [Liu et al. 2023b], ReconFusion [Wu et al. 2024] and CAT3D [Gao et al. 2024] integrate diffusion models with NeRFs. Recently, Large reconstruction models (LRMs) [Hong et al. 2024; Li et al. 2024; Tang et al. 2024a; Wang et al. 2024b; Wei et al. 2024; Weng et al. 2023; Xu et al. 2024a,b; Zhang et al. 2024] also achieve 3D reconstruction from highly sparse views. Though effective in generating images fast, these methods encounter issues with large pretraining, strict requirements on view distribution and object location, and difficulty in handling real-world captures.

![](images/62e8284b8ab9a7f950b12b9750c11c062ab28f92d2e70b5709637e0a039b7293.jpg)  
Fig. 2. Overview of GaussianObject. (a) We initialize 3D Gaussians by constructing a visual hull with camera parameters and masked images, which are optimized with Lref and refined through floater elimination. (b) We use a novel ‘leave-one-out’ strategy and add 3D noise to Gaussians to generate corrupted Gaussian renderings. These renderings, paired with their corresponding reference images, facilitate the training of the Gaussian repair model employing Ltune. For details please refer to Fig. 3. (c) Once trained, the Gaussian repair model is frozen and used to correct views that need to be rectified. These views are identified through distance-aware sampling. The repaired images and reference images are used to further optimize 3D Gaussians with Lrep and Lref.

While 3DGS shows strong power in novel view synthesis, it struggles with sparse 360◦ views similar to NeRF. Inspired by few-shot NeRFs, methods [Charatan et al. 2024; Chung et al. 2023; Paliwal et al. 2024; Xiong et al. 2023; Zhu et al. 2024] have been developed for sparse 360◦ reconstruction. However, they still severely rely on the SfM points. Our GaussianObject proposes structure-prior-aided Gaussian initialization to tackle this issue, drastically reducing the required input views to only 4, a significant improvement compared with over 20 views required by FSGS [Zhu et al. 2024].

# 3 METHOD

The subsequent sections detail the methodology: Sec. 3.1 reviews foundational techniques; Sec. 3.2 introduces our overall framework; Sec. 3.3 describes how we apply the structure priors for initial optimization; Sec. 3.4 details the setup of our Gaussian repair model; Sec. 3.5 illustrates the repair of 3D Gaussians using this model and Sec. 3.6 elucidates the COLMAP-free version of GaussianObject. To facilitate a better understanding, all key mathematical symbols and their corresponding meanings are listed in Table 1.

# 3.1 Preliminary

3D Gaussian Splatting. 3D Gaussian Splatting [Kerbl et al. 2023] represents 3D scenes with 3D Gaussians. Each 3D Gaussian is composed of the center location ??, rotation quaternion ??, scaling vector ??, opacity , and spherical harmonic (SH) coefficients . Thus, a sceneis parameterized as a set of Gaussians G = {???? : ????, ????, ????, ????, ??ℎ?? }????=1. ControlNet. Diffusion models are generative models that sample from a data distribution ??(??0), beginning with Gaussian noise ?? and using various sampling schedulers. They operate by reversing a discrete-time stochastic noise addition process {???? }????=0 with a diffusion model ???? (???? −1|???? ) trained to approximate ??(???? −1|???? ), where ?? ∈ [0,?? ] is the noise level and ?? is the learnable parameters. Substituting ??0 with its latent code ??0 from a Variational Autoencoder (VAE) [Kingma and Welling 2014] leads to the development of Latent Diffusion Models (LDM) [Rombach et al. 2022]. ControlNet [Zhang et al. 2023a] further enhances the generative process with additional image conditioning by integrating a network structure similar to the diffusion model, optimized with the loss function:

Table 1. List of Key Mathematical Symbols   

<html><body><table><tr><td>Symbol</td><td>Meaning</td></tr><tr><td>xref=</td><td>Reference images</td></tr><tr><td>={ki Kref</td><td>Intrinsics of Xref</td></tr><tr><td>Kref</td><td>Estimated intrinsics of Xref</td></tr><tr><td>K</td><td>Estimated shared intrinsics of Xref</td></tr><tr><td>mef=(）</td><td>Extrinsics of Xref</td></tr><tr><td>Inov</td><td>Extrinsics of viewpoints in repair path</td></tr><tr><td>Iref</td><td>Estimated extrinsics of Xref</td></tr><tr><td>Mmref =(mi）1</td><td>Masks of Xref</td></tr><tr><td>μ</td><td>Center location of Gaussian</td></tr><tr><td>q</td><td>Rotation quaternion of Gaussian</td></tr><tr><td>S</td><td>Scale vector of Gaussian</td></tr><tr><td>0</td><td>Opacity of Gaussian</td></tr><tr><td>sh</td><td>Spherical harmonic coefficients of Gaussian</td></tr><tr><td>Gc</td><td>Coarse 3D Gaussians</td></tr><tr><td>R</td><td>Diffusion based Gaussian repair model</td></tr><tr><td>8</td><td>Latent diffusion encoder of R</td></tr><tr><td>D</td><td>Latent diffusion decoder of R</td></tr><tr><td>x&#x27;</td><td>Degraded rendering</td></tr><tr><td>X</td><td>Image repaired by R</td></tr><tr><td>Es</td><td>3D Noise added to attributes of Gc</td></tr><tr><td>E</td><td>2D Gaussian noise for fine-tuning</td></tr><tr><td></td><td>2D Noise predicted by R</td></tr><tr><td>ctex</td><td>Object-specific language prompt</td></tr><tr><td>p</td><td>Coarse point cloud predicted byDUSt3R</td></tr></table></body></html>

![](images/333ac67e908acadb10d5c94ea89c1ce08648098ed00ee783ee2592e034e94baa.jpg)

where ??tex and ??img denote the text and image conditioning respectively, and ???? is the Gaussian noise inferred by the diffusion model with parameter ??, ??¯1:?? ∈ (0, 1]?? is a decreasing sequence associated with the noise-adding process.

# 3.2 Overall Framework

Given a sparse collection of ?? reference images ?? ref = {???? }????=1,captured within a 360◦ range and encompassing one object, along with the corresponding camera intrinsics2 ??ref = {???? }????=1, extrinsics Πref = {???? }????=1 and masks ?? ref = {???? }????=1 of the object, our target is to obtain a 3D representation G, which can achieve photo-realistic rendering ?? = G (?? |{????, ????, ???? }????=1) from any viewpoint. To achieve this, we employ the 3DGS model for its simplicity for structure priors embedding and fast rendering capabilities. The process begins with initializing 3D Gaussians using a visual hull [Laurentini 1994], followed by optimization with floater elimination, enhancing the structure of Gaussians. Then we design self-generating strategies to supply sufficient image pairs for constructing a Gaussian repair model, which is used to rectify incomplete object information. The overall framework is shown in Fig. 2.

# 3.3 Initial Optimization with Structure Priors

Sparse views, especially for only 4 images, provide limited 3D information for reconstruction. In this case, SfM points, which are the key for 3DGS initialization, are often absent. Besides, insufficient multi-view consistency leads to ambiguity among shape and appearance, resulting in many floaters during reconstruction. We propose two techniques to initially optimize the 3D Gaussian representation, which take full advantage of structure priors from the limited views and result in a satisfactory outline of the object.

Initialization with Visual Hull. To better leverage object structure information from limited reference images, we utilize the view frustums and object masks to create a visual hull as a geometric scaf fold for initializing our 3D Gaussians. Compared with the limited number of SfM points in extremely sparse settings, the visual hull provides more structure priors that help build multiview consistency by excluding unreasonable Gaussian distributions. The cost of the visual hull is just several masks derived from sparse 360◦ images, which can be easily acquired using current segmentation models such as SAM [Kirillov et al. 2023]. Specifically, points are randomly initialized within the visual hull using rejection sampling: we project uniformly sampled random 3D points onto image planes and retain those within the intersection of all image-space masks. Point colors are averaged from bilinearly interpolated pixel colors across reference image projections. Then we transform these 3D points into 3D Gaussians. For each point, we assign its position as ?? and convert its color into ??ℎ. The mean distance between adjacent points forms the scale ??, while the rotation ?? is set to a unit quaternion as default. The opacity ?? is initialized to a constant value. This initialization strategy relies on the initial masks. Despite potential inaccuracies in these masks or unrepresented concavities by the visual hull, we observed that subsequent optimization processes reliably yield high-quality reconstructions.

Floater Elimination. While the visual hull builds a coarse estimation of the object geometry, it often contains regions that do not belong to the object due to the inadequate coverage of reference images. These regions usually appear to be floaters, damaging the quality of novel view synthesis. These floaters are problematic as the optimization process struggles to adjust them due to insufficient observational data regarding their position and appearance.

To mitigate this issue, we utilize the statistical distribution of distances among the 3D Gaussians to distinguish the primary object and the floaters. This is implemented by the K-Nearest Neighbors (KNN) algorithm, which calculates the average distance to the nearest ?? Gaussians for each element in G?? . We then establish a normative range by computing the mean and standard deviation of these distances. Based on statistical analysis, we exclude Gaussians whose mean neighbor distances exceed the adaptive threshold ?? = mean + ???? std. This thresholding process is repeated periodically throughout optimization, where ???? is linearly decreased to 0 to refine the scene representation progressively.

Initial Optimization The optimization of G?? incorporates color, mask, and monocular depth losses. The color loss combines L1 and D-SSIM losses from 3D Gaussian Splatting:

![](images/13027b8d78f50ae600e53e0a542adadb3a9b9d27c594c2c249223b4dc3d78193.jpg)

where ?? is the rendering and ??ref is the corresponding reference image. A binary cross entropy (BCE) loss [Jadon 2020] is applied as mask loss:

![](images/5b395e2179b5d6301bcf4738d9aa67ee1cdce0ca72edbf95022343fb96405f0d.jpg)

where ?? denotes the object mask. A shift and scale invariant depth loss is utilized to guide geometry:

![](images/f195ec2a58fa7a8ca50f68ec71712973bb62d87321acbb8f92ad5bb6368a3a2d.jpg)

where ??∗ and ??∗ are per-frame rendered depths and monocularly pred estimated depths [Bhat et al. 2023] respectively. The depth values are computed following a normalization strategy [Ranftl et al. 2020]:

![](images/0affe0ab782e7d3d50753527ee5fdd00337b9b3ead9ffb1a339d30e90b970a9e.jpg)

where ?? denotes the number of valid pixels. The overall loss combines these components:

![](images/c712a1674ed858d848eafae0803774f41927bd11916ebd89bc5a09d0c84edfaa.jpg)

where ??SSIM, ??m, and ??d control the magnitude of each term. Thanks to the efficient initialization, our training speed is remarkably fast. It only takes 1 minute to train a coarse Gaussian representation G?? at a resolution of 779 × 520.

# 3.4 Gaussian Repair Model Setup

Combining visual hull initialization and floater elimination significantly enhances 3DGS performance for NVS in sparse 360◦ contexts. While the fidelity of our reconstruction is generally passable, G?? still suffers in regions that are poorly observed, regions with occlusion, or even unobserved regions. These challenges loom over the completeness of the reconstruction, like the sword of Damocles.

![](images/683dc7261b6963940c025abcb26b91f59818d0e24cd6b34780a338f53e3997f5.jpg)  
Fig. 3. Illustration of Gaussian repair model setup. First, we add Gaussian noise ?? to a reference image ??ref to form a noisy image. Next, this noisy image along with ??ref’s corresponding degraded image ??′ are passed to a pre-trained fixed ControlNet with learnable LoRA layers to predict a noise distribution ???? . We use the differences among ?? and ???? to fine-tune the parameters in LoRA layers.

To mitigate these issues, we introduce a Gaussian repair model R designed to correct the aberrant distribution of G?? . Our R takes corrupted rendered images ??′ (G??, ??nov) as input and outputs photorealistic and high-fidelity images ??ˆ. This image repair capability can be used to refine the 3D Gaussians, leading to learning better structure and appearance details.

Sufficient data pairs are essential for training R but are rare in existing datasets. To this end, we adopt two main strategies for generating adequate image pairs, i.e., leave-one-out training and adding 3D noises. For leave-one-out training, we build ?? subsets from the ?? input images, each containing ?? − 1 reference images and 1 left-out image ??out. Then we train ?? 3DGS models with reference images of these subsets, termed as {G???? }?? −1??=0 . After specific iterations, we use the left-out image ??out to continue training each Gaussian model {G???? }?? −1??=0 into {Gˆ???? }?? −1??=0 . Throughout this process, are stored to form the image pairs along with left-out image ??out for training the repair model. Note that training these left-out models costs little, with less than ?? minutes in total. The other strategy is to add 3D noises ???? onto Gaussian attributes. The ???? are derived from the mean Δ and variance Δ of attribute differences between{G???? }?? −1??=0 and {Gˆ???? }?? −1??=0 . This allows us to render more degraded Gaussians, resulting in extensive image pairs (?? ′, ?? ref).

We inject LoRA weights and fine-tune a pre-trained ControlNet [Zhang et al. 2023b] using the generated image pairs as our Gaussian repair model. The training procedure is shown in Fig. 3. The loss function, based on Eq. 1, is defined as:

![](images/f0f1981595496154a977d1d473925d5779c76f5895396d167014c27ceceec5a3.jpg)

where ??tex denotes an object-specific language prompt, defined as “a photo of [V],” as per Dreambooth [Ruiz et al. 2023]. Specifically, we inject LoRA layers into the text encoder, image condition branch, and U-Net for fine-tuning. Please refer to the Appendix for details.

# 3.5 Gaussian Repair with Distance-Aware Sampling

After training R, we distill its target object priors into G?? to refine its rendering quality. The object information near the reference views is abundant. This observation motivates designing distance as a criterion in identifying views that need rectification, leading to distance-aware sampling.

Specifically, we establish an elliptical path aligned with the training views and focus on a central point. Arcs near Π???? ?? , where we assume G?? renders high-quality images, form the reference path. The other arcs, yielding renderings, need to be rectified and define the repair path, as depicted in Fig. 4. In each iteration, novel viewpoints, ???? ∈ Πnov, are randomly sampled among the repair path. For each ???? , we render the corresponding image ?? ?? (G??, ???? ), encode it to be E (?? ?? ) by the latent diffusion encoder E and pass E (?? ?? ) to the image conditioning branch of R. Simultaneously, a cloned E (?? ?? ) is disturbed into a noisy latent ???? :

![](images/672b3cc6420ec31ffc1dd508c1c844f7ec2ae65ec23038ef5a1a3bdd293c28eb.jpg)

which is similar to SDEdit [Meng et al. 2022]. We then generate a sample ??ˆ?? from R by running DDIM sampling [Song et al. 2021] over ?? = ⌊50 · ???? ⌋ steps and forwarding the diffusion decoder D:

![](images/bceee5b5ed8a11a69160998623ca39033ec7820a87c1ab5035ce82ef91d499f9.jpg)

where E and D are from the VAE model used by the diffusion model. The distances from ?? ?? to Π???? ?? is used to weight the reliability of ??ˆ?? , guiding the optimization with a loss function:

![](images/f5126d6b236f7d10c021d150db5ab87ffe366842b749a3c5a2b25af69fdbf537.jpg)

Here, ???? denotes the perceptual similarity metric LPIPS [Zhang et al. 2018], ?? (??) is a noise-level modulated weighting function from DreamFusion [Poole et al. 2023], ??(???? ) denotes a distancebased weighting function, and ??max is the maximal distance among neighboring reference viewpoints. To ensure coherence between 3D Gaussians and reference images, we continue training G?? with Lref during the whole Gaussian repair procedure.

# 3.6 COLMAP-Free GaussianObject (CF-GaussianObject)

Current SOTA sparse view reconstruction methods rely on precise camera parameters, including intrinsics and poses, obtained through an SfM pipeline with dense input, limiting their usability in daily applications. This process can be cumbersome and unreliable in sparse-view scenarios where matched features are insufficient for accurate reconstruction.

To overcome this limitation, we introduce an advanced sparse matching model, DUSt3R [Wang et al. 2024a], into GaussianObject to enable COLMAP-free sparse 360◦ reconstruction. Given reference input images ?? ref, DUSt3R is formulated as:

![](images/4fdbf958599fdfbe437a1c6620538d3b20c816f24d529a4d86589e3317793e5e.jpg)

where P is an estimated coarse point cloud of the scene, and Πˆ ref, ??ˆ ref are the predicted camera poses and intrinsics of ?? ref, respectively. For CF-GaussianObject, we modify the intrinsic recovery module within DUSt3R, allowing ???? ∈ ?? ref to share the same intrinsic ??ˆ . This adaption enables the retrieval of P, Πˆ ref, and ??ˆ . Besides, we apply structural priors with a visual hull to P to initialize 3D Gaussians. After initialization, we optimize Πˆ ref and the initialized 3D Gaussians using ?? ref and depth maps rendered from P simultaneously. Besides, we introduce a regularization loss to constrain deviations from Πˆ ref, enhancing the robustness of the optimization. After optimization, the 3D Gaussians and camera parameters are used for constructing the Gaussian repair model and Gaussian repairing process as described in Sec. 3.4 and Sec. 3.5. Refer to the Appendix for more details.

![](images/42d81142fed6eaeefbeae4abd57bf10130bf635a0bd5bf353e6077e65ae625d2.jpg)  
Fig. 4. Illustration of our distance-aware sampling. Blue and red indicate the reference and repair path, respectively.

# 4 EXPERIMENTS

# 4.1 Implementation Details

Our framework, illustrated in Fig. 2, is based on 3DGS [Kerbl et al. 2023] and threestudio [Guo et al. 2023]. The 3DGS model is trained for 10k iterations in the initial optimization, with periodic floater elimination every 500 iterations. The monocular depth for Ld is predicted by ZoeDepth [Bhat et al. 2023]. We use a ControlNetTile [Zhang et al. 2023b] model based on stable diffusion v1.5 [Rombach et al. 2022] as our repair model’s backbone. LoRA [Hu et al. 2022] weights, injected into the text-encoder and transformer blocks using minLoRA [Chang 2023], are trained for 1800 steps at a LoRA rank of 64 and a learning rate of 10−3. G?? is trained for another 4k iterations during distance-aware sampling. For the first 2800 iterations, optimization involves both a reference image and a repaired novel view image, with the weight of Lrep progressively decayed from 1.0 to 0.1. The final 1200-step training only involves reference views. The whole process of GaussianObject takes about 30 minutes on a GeForce RTX 3090 GPU for 4 input images at a 779 × 520 resolution. For more details, please refer to the Appendix.

# 4.2 Datasets

We evaluate GaussianObject on three datasets suited for sparseview 360◦ object reconstruction with varying input views, including Mip-NeRF360 [Barron et al. 2021], OmniObject3D [Wu et al. 2023], and OpenIllumination [Liu et al. 2023a]. Additionally, we use an iPhone 13 to capture four views of some daily-life objects to show the COLMAP-free performance. SAM [Kirillov et al. 2023] is used to obtain masks of the target objects.

# 4.3 Evaluation

Sparse 360◦ Reconstruction Performance. We evaluate the performance of GaussianObject against several reconstruction baselines, including the vanilla 3DGS [Kerbl et al. 2023] with random initialization and DVGO [Sun et al. 2022], and various few-view reconstruction models on the three datasets. Compared methods of RegNeRF [Niemeyer et al. 2022], DietNeRF [Jain et al. 2021], SparseNeRF [Guangcong et al. 2023], and ZeroRF [Shi et al. 2024b] utilize a variety of regularization techniques. Besides, FSGS [Zhu et al. 2024] is also built upon Gaussian splatting with SfM-point initialization. Note that we supply extra SfM points to FSGS so that it can work with the highly sparse 360◦ setting. Since camera pose estimation often suffers from scale and positional errors compared to ground truth, we adopt the evaluation used for COLMAP-free methods under dense view settings [Fu et al. 2024; Wang et al. 2021]. All models are trained using publicly released codes.

Table 2 and 3 present the view-synthesis performance of GaussianObject compared to existing methods on the MipNeRF360, OmniObject3D, and OpenIllumination datasets. Experiments show that GaussianObject consistently achieves SOTA results in all datasets, especially in the perceptual quality – LPIPS. Although GaussianObject is designed to address extremely sparse input views, it still outperforms other methods with more input views, i.e. 6 and 9, further proving the effectiveness. Notably, GaussianObject excels with as few as 4 views and significantly improves LPIPS over FSGS from 0.0951 to 0.0498 on MipNeRF360. This improvement is critical, as LPIPS is a key indicator of perceptual quality [Park et al. 2021].

Fig. 5 and Fig. 6 illustrate rendering results of various methods across different datasets with only 4 input views. We observe that GaussianObject achieves significantly better visual quality and fidelity than the competing models. We find that implicit representation based methods and random initialized 3DGS fail in extremely sparse settings, typically reconstructing objects as fragmented pixel patches. This confirms the effectiveness of integrating structure priors with explicit representations. Although ZeroRF exhibits competitive PSNR and SSIM on OpenIllumination, its renderings are blurred and lack details, as shown in Fig. 6. In contrast, GaussianObject demonstrates fine-detailed reconstruction. This superior perceptual quality highlights the effectiveness of the Gaussian repair model. It is highly suggested to refer to comprehensive video comparisons included in supplementary materials.

Comparison with LRMs. We further compare GaussianObject to recently popular LRM-like feed-forward reconstruction methods, i.e. LGM [Tang et al. 2024a] and TriplaneGaussian (TGS) [Zou et al. 2024] which are publicly available. The comparisons are shown in Table 4 on the challenging MipNeRF360 dataset. Given that TriplaneGaussian accommodates only a single image input, we feed it with frontal views of objects. LGM requires placing the target object at the world coordinate origin with cameras oriented towards it at an elevation of 0◦ and azimuths of 0◦, 90◦, 180◦, and 270◦. Therefore, we report two versions of LGM – LGM-4 which uses four sparse captures as input views directly, and LGM-1 which uses MVDream [Shi et al. 2024a] to generate images that comply with LGM’s setup requirements following its original manner. Results show that the strict requirements among input views significantly hinder the sparse reconstruction performance of LRM-like models with in-the-wild captures. In contrast, GaussianObject does not require extensive pre-training, has no restrictions on input views, and can reconstruct any complex object in daily life.

![](images/e4725e0aefb869ab84e2577dcba85be072e7bf4bf6d82b51b5d5138c6679f36b.jpg)  
Fig. 5. Qualitative examples on the MipNeRF360 and OmniObject3D dataset with 4 input views. Many methods fail to reach a coherent 3D representation, resulting in floaters and disjoint pixel patches. A pure white image indicates a total miss of the object by the corresponding method, usually caused by overfitting the input images.

Table 2. Comparisons with varying input views. LPIPS∗ = LPIPS × 102 throughout this paper. Best results are highlighted as 1st , 2nd and 3rd   

<html><body><table><tr><td rowspan="2"></td><td rowspan="2">Method</td><td colspan="3">LPIPS*PvN↑</td><td colspan="3"></td><td colspan="3"></td></tr><tr><td></td><td></td><td>SSIM ↑</td><td>LPIPS*↓</td><td>6-view↑</td><td>SSIM ↑</td><td>LPIPS*↓</td><td>9vie↑</td><td>SSIM ↑</td></tr><tr><td rowspan="12"></td><td>DVGO [Sun et al. 2022]</td><td>24.43</td><td>14.39</td><td>0.7912</td><td>26.67</td><td>14.30</td><td>0.7676</td><td>25.66</td><td>14.74</td><td>0.7842</td></tr><tr><td>3DGS [Kerbl et al. 2023]</td><td>10.80</td><td>20.31</td><td>0.8991</td><td>8.38</td><td>22.12</td><td>0.9134</td><td>6.42</td><td>24.29</td><td>0.9331</td></tr><tr><td>DietNeRF [Jain et al. 2021]</td><td>11.17</td><td>18.90</td><td>0.8971</td><td>6.96</td><td>22.03</td><td>0.9286</td><td>5.85</td><td>23.55</td><td>0.9424</td></tr><tr><td>RegNeRF[Niemeyer et al. 2022]</td><td>20.44</td><td>13.59</td><td>0.8476</td><td>20.72</td><td>13.41</td><td>0.8418</td><td>19.70</td><td>13.68</td><td>0.8517</td></tr><tr><td>FreeNeRF[Yang et al.2023]</td><td>16.83</td><td>13.71</td><td>0.8534</td><td>6.84</td><td>22.26</td><td>0.9332</td><td>5.51</td><td>27.66</td><td>0.9485</td></tr><tr><td>SparseNeRF [Guangcong et al. 2023]</td><td>17.76</td><td>12.83</td><td>0.8454</td><td>19.74</td><td>13.42</td><td>0.8316</td><td>21.56</td><td>14.36</td><td>0.8235</td></tr><tr><td>ZeroRF [Shi et al. 2024b]</td><td>19.88</td><td>14.17</td><td>0.8188</td><td>8.31</td><td>24.14</td><td>0.9211</td><td>5.34</td><td>27.78</td><td>0.9460</td></tr><tr><td>FSGS [Zhu et al. 2024]</td><td>9.51</td><td>21.07</td><td>0.9097</td><td>7.69</td><td>22.68</td><td>0.9264</td><td>6.06</td><td>25.31</td><td>0.9397</td></tr><tr><td>GaussianObject (Ours) CF-GaussianObject (Ours)</td><td>4.98</td><td>24.81</td><td>0.9350</td><td>3.63</td><td>27.00</td><td>0.9512</td><td>2.75</td><td>28.62</td><td>0.9638</td></tr><tr><td>DVGO [Sun et al. 2022] 3DGS [Kerbl et al. 2023]</td><td>8.47</td><td>21.39</td><td>0.9014</td><td>5.71</td><td>24.06</td><td>0.9269</td><td>5.50</td><td>24.39</td><td>0.9300</td></tr><tr><td rowspan="10"></td><td></td><td>14.48</td><td>17.14</td><td>0.8952</td><td>12.89</td><td>18.32</td><td>0.9142</td><td>11.49</td><td>19.26</td><td>0.9302</td></tr><tr><td>8.60</td><td>17.29</td><td>0.9299</td><td>7.74</td><td>18.29</td><td>0.9378</td><td>6.50</td><td></td><td>20.26</td><td>0.9483</td></tr><tr><td></td><td>18.56</td><td>0.9205</td><td>10.39</td><td>19.07</td><td></td><td></td><td></td><td></td><td>0.9258</td></tr><tr><td>DietNeRFl22)</td><td>11.64</td><td></td><td></td><td></td><td></td><td>0.9267</td><td>10.32</td><td>19.26</td><td></td></tr><tr><td>FreeNeRF[Yang et al.2023]</td><td>8.28</td><td>17.78</td><td>0.9402</td><td>7.32</td><td>19.02</td><td>0.9464</td><td>7.25</td><td>20.35</td><td>0.9467</td></tr><tr><td>SparseNeRF [Guangcong et al. 2023]</td><td>17.47</td><td>15.22</td><td>0.8921</td><td>21.71</td><td>15.86</td><td>0.8935</td><td>23.76</td><td>17.16</td><td>0.8947</td></tr><tr><td>ZeroRF [Shi et al. 2024b]</td><td>4.44</td><td>27.78</td><td>0.9615</td><td>3.11</td><td>31.94</td><td>0.9731</td><td>3.10</td><td>32.93</td><td>0.9747</td></tr><tr><td>FSGS [Zhu et al. 2024]</td><td>6.25</td><td>24.71</td><td>0.9545</td><td>6.05</td><td>26.36</td><td>0.9582</td><td>4.17</td><td>29.16</td><td>0.9695</td></tr><tr><td>GaussianObject (Ours)</td><td>2.07</td><td>30.89</td><td>0.9756</td><td>1.55</td><td>33.31</td><td>0.9821</td><td>1.20</td><td>35.49</td><td>0.9870</td></tr><tr><td>CF-GaussianObject (Ours)</td><td>2.62</td><td>28.51</td><td>0.9669</td><td>2.03</td><td>30.73</td><td>0.9738</td><td>2.08</td><td>31.23</td><td>0.9757</td></tr></table></body></html>

Table 3. Quantitative comparisons on the OpenIllumination dataset. Methods with † means the metrics are from the ZeroRF paper [Shi et al. 2024b].   

<html><body><table><tr><td rowspan="2">Method</td><td colspan="3">4-view</td><td colspan="3">6-view</td></tr><tr><td>LPIPS*↓</td><td></td><td></td><td>PSNR ↑ SSIM ↑|LPIPS*↓</td><td></td><td>PSNR ↑ SSIM ↑</td></tr><tr><td>DVGO</td><td>11.84</td><td>21.15</td><td>0.8973</td><td>8.83</td><td>23.79</td><td>0.9209</td></tr><tr><td>3DGS</td><td>30.08</td><td>11.50</td><td>0.8454</td><td>29.65</td><td>11.98</td><td>0.8277</td></tr><tr><td>DietNeRF†</td><td>10.66</td><td>23.09</td><td>0.9361</td><td>9.51</td><td>24.20</td><td>0.9401</td></tr><tr><td>RegNeRF†</td><td>47.31</td><td>11.61</td><td>0.6940</td><td>30.28</td><td>14.08</td><td>0.8586</td></tr><tr><td>FreeNeRF†</td><td>35.81</td><td>12.21</td><td>0.7969</td><td>35.15</td><td>11.47</td><td>0.8128</td></tr><tr><td>SparseNeRF</td><td>22.28</td><td>13.60</td><td>0.8808</td><td>26.30</td><td>12.80</td><td>0.8403</td></tr><tr><td>ZeroRFt</td><td>9.74</td><td>24.54</td><td>0.9308</td><td>7.96</td><td>26.51</td><td>0.9415</td></tr><tr><td>Ours</td><td>6.71</td><td>24.64</td><td>0.9354</td><td>5.44</td><td>26.54</td><td>0.9443</td></tr></table></body></html>

Performance of CF-GaussianObject. CF-GaussianObject is evaluated on the MipNeRF360 and OmniObject3D datasets, with results detailed in Table 2 and Fig. 5. Though CF-GaussianObject exhibits some performance degradation, it eliminates the need for precise camera parameters, significantly enhancing its practical utility. Its performance remains competitive compared to other SOTA methods that depend on accurate camera parameters. Notably, we observe that the performance degradation correlates with an increase in the number of input views, primarily due to declines in the accuracy of DUSt3R’s estimates as the number of views rises. As demonstrated in Fig. 7, comparative experiments on smartphone-captured images confirm the superior reconstruction capabilities and visual quality of CF-GaussianObject. More visualization of CF-GaussianObject can be found in our appendix and supplementary materials.

# 4.4 Ablation Studies

Key Components. We conduct a series of experiments to validate the effectiveness of each component. The following experiments are performed on MipNeRF360 with 4 input views, and averaged metric values are reported. We disable visual hull initialization, floater elimination, Gaussian repair model setup, and Gaussian repair process once at a time to verify their effectiveness. The Gaussian repair loss is further compared with the Score Distillation Sampling (SDS) loss [Poole et al. 2023], and the depth loss is ablated. The results, presented in Table 5 and Fig. 9, indicate that each element significantly contributes to performance, with their absence leading to a decline in results. Particularly, omitting visual hull initialization results in a marked decrease in performance. Gaussian repair model setup and the Gaussian repair process significantly enhance visual quality, and the absence of either results in a substantial decline in perceptual quality as shown in Fig. 8. Contrary to its effectiveness in text-to-3D or single image-to-3D, SDS results in unstable optimization and diminished performance in our context. The depth loss shows marginal promotion, mainly for LPIPS and SSIM. We apply it to enhance the robustness of our framework.

![](images/4d6ae5dada83fed73309551e76c63e7119c7bbe8c3de32fccb94de655c248003.jpg)  
Fig. 6. Qualitative results on the OpenIllumination dataset. Although ZeroRF shows competitive PSNR and SSIM, its renderings often appear blurred. While GaussianObject outperforms in restoring fine details, achieving a significant perceptual quality advantage.

![](images/6af5374918c1379fe35743fe9bc3d22b55c3ffc63c75c28a9e84ecbcac59e53b.jpg)  
Fig. 7. Qualitative results on our-collected images captured by an iPhone 13. We equip other SOTAs with camera parameters predicted by DUSt3R for fair comparison. The results demonstrate the superior performance of our CF-GaussianObject among casually captured images, with fine details and higher visual quality.

![](images/ed168aa155103ed2e3f1c6ff06388fc9469319b375ecfa91ad3fc2928df89a2e.jpg)  
Fig. 8. Importance of our Gaussian repair model setup. Without the Gaussian repair process or the finetuning of the ControlNet, the renderings exhibit noticeable artifacts and lack of details, particularly in areas with insufficient view coverage. Zoom in for better comparison.   
Fig. 10. Qualitative comparisons by ablating different Gaussian repair model setup methods. “MDepth” denotes the repair model with masked monocular depth estimation as the condition.

![](images/67cd62dd7e801c3dc916526af667a854cde8cbe44ac4fbc66e55c04c01cca79b.jpg)  
Fig. 9. Ablation study on different components. “VH” denotes for visual hull and “FE” is floater elimination. The “GT” image is from a test view.

Structure of Repair Model. Our repair model is designed to generate photo-realistic and 3D-consistent views of the target object. This is achieved by leave-one-out training and perturbing the attributes of 3D Gaussians to create image pairs for fine-tuning a pre-trained image-conditioned ControlNet. Similarities can be found in Dreambooth [Ruiz et al. 2023], which aims to generate specific subject images from limited inputs. To validate the efficacy of our design, we evaluate the samples generated by our Gaussian repair model and other alternative structures. The first is implemented with Dreambooth [Raj et al. 2023; Ruiz et al. 2023], which embeds target object priors with semantic modifications. To make the output corresponding to the target object, we utilize SDEdit [Meng et al. 2022] to guide the image generation. Inspired by Song et al. [2023b], the second introduces a monocular depth conditioning ControlNet [Zhang et al. 2023a], which is fine-tuned using data pair generation as in Sec. 3.4. We also assess the performance using masked depth conditioning. Furthermore, we consider Zero123-XL [Deitke et al. 2023; Liu et al. 2023c], a well-known single-image reconstruction model requiring object-centered input images with precise camera rotations and positions. Here, we manually align the coordinate system and select the closest image to the novel viewpoint as its reference.

Table 4. Quantitative comparisons with LRM-like methods on MipNeRF360.   

<html><body><table><tr><td colspan="2">Method</td><td>LPIPS*↓</td><td>PSNR ↑</td><td>SSIM ↑</td></tr><tr><td colspan="2">TGS [Zou et al. 2024]</td><td>9.14</td><td>18.07</td><td>0.9073</td></tr><tr><td colspan="2">LGM-4 [Tang et al. 2024a]</td><td>9.20</td><td>17.97</td><td>0.9071</td></tr><tr><td colspan="2">LGM-1 [Tang et al. 2024a]</td><td>9.13</td><td>17.46</td><td>0.9071</td></tr><tr><td colspan="2"> GaussianObject (Ours)</td><td>4.99</td><td>24.81</td><td>0.9350</td></tr><tr><td>Input</td><td colspan="2">Zero123-XL Dreambooth</td><td>MDepth</td><td>Ours</td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td>口</td><td></td><td></td><td></td><td></td></tr></table></body></html>

Table 5. Ablation study on key components.   

<html><body><table><tr><td>Method</td><td>LPIPS*↓</td><td>PSNR ↑</td><td>SSIM ↑</td></tr><tr><td>Ours w/o Visual Hull</td><td>12.72</td><td>15.95</td><td>0.8719</td></tr><tr><td>Ours w/o Floater Elimination</td><td>4.99</td><td>24.73</td><td>0.9346</td></tr><tr><td>Ours w/o Setup</td><td>5.53</td><td>24.28</td><td>0.9307</td></tr><tr><td>Ours w/o Gaussian Repair</td><td>5.55</td><td>24.37</td><td>0.9297</td></tr><tr><td>Ours w/o Depth Loss</td><td>5.09</td><td>24.84</td><td>0.9341</td></tr><tr><td>Ours w/ SDS [Poole et al. 2023]</td><td>6.07</td><td>22.42</td><td>0.9188</td></tr><tr><td>GaussianObject (Ours)</td><td>4.98</td><td>24.81</td><td>0.9350</td></tr></table></body></html>

The results, as shown in Table 6 and Fig. 10, reveal that semantic modifications proposed by Dreambooth alone fail in 3D-coherent synthesis. Monocular depth conditioning, whether with or without masks, despite some improvements, still struggles with depth roughness and artifacts. Zero123-XL, while generating visually acceptable images, the multi-view structure consistency is lacking. In contrast, our model excels in both 3D consistency and detail fidelity, outperforming others qualitatively and quantitatively.

![](images/4f6f6d4837d1e55193f00272fa0a72d48160c7acdd40b2c752e0497d1f99ff72.jpg)  
Fig. 11. Ablation on Training View Number. Experiments are conducted on scene kitchen in the MipNeRF360 dataset.

Table 6. Ablation study about alternatives of the Gaussian repair model.   

<html><body><table><tr><td>Method</td><td>LPIPS*↓</td><td>PSNR ↑</td><td>SSIM ↑</td></tr><tr><td>Zero123-XL [Liu et al. 2023c]</td><td>13.97</td><td>17.71</td><td>0.8921</td></tr><tr><td>Dreambooth [Ruiz et al. 2023]</td><td>6.58</td><td>21.85</td><td>0.9093</td></tr><tr><td>Depth Condition</td><td>7.00</td><td>21.87</td><td>0.9112</td></tr><tr><td>Depth Condition w/ Mask</td><td>6.87</td><td>21.92</td><td>0.9117</td></tr><tr><td>GaussianObject (Ours)</td><td>5.79</td><td>23.55</td><td>0.9220</td></tr></table></body></html>

Effect of View Numbers. We design experiments to evaluate the advantage of our method over different training views. As shown in Fig. 11, GaussianObject consistently outperforms vanilla 3DGS in varying numbers of training views. Besides, GaussianObject with 24 training views achieves performance comparable to that of 3DGS trained on all views (243).

# 4.5 Limitations and Future Work

GaussianObject demonstrates notable performance in sparse 360◦ ob ject reconstruction, yet several avenues for future research exist. In regions completely unobserved or insufficiently observed, our repair model may generate hallucinations, i.e., it may produce non-existent details, as shown in Fig. 12. However, these regions are inherently non-deterministic in information, and other methods also struggle in these areas. Additionally, due to the high sparsity level, our model is currently limited in capturing view-dependent effects. With such sparse data, our method cannot differentiate whether the appearance is view-dependent or inherent. Consequently, it ‘bakes in’ the view-dependent features (like reflected white light) onto the surface, resulting in an inability to display view-dependent appearance from novel viewpoints correctly and leading to some unintended artifacts as demonstrated in Fig. 13. Fine-tuning diffusion models with more view-dependent data may be a promising direction. Besides, integrating GaussianObject with surface reconstruction methods like 2DGS [Huang et al. 2024] and GOF [Yu et al. 2024] is a promising direction. Furthermore, CF-GaussianObject achieves competitive performance, but there is still a performance gap compared to precise camera parameters. An interesting exploration is to leverage confidence maps from matching methods for more accurate pose estimation.

![](images/8c5b199362ea7538e1f525305aa3951ad3a97b8509764ea0e4dfe7c569e4a957.jpg)  
Fig. 12. Hallucinations of non-existent details. GaussianObject may fabricate visually reasonable details in areas with little information. For instance, the hole in the stone vase is filled in.

![](images/4edf23de78f80bedade9e9628d7de04677faf669c945a7438bc5741c054af036.jpg)  
Fig. 13. Comparative visualization highlighting the challenge of reconstructing view-dependent appearance with only four input images.

# 5 CONCLUSION

In summary, GaussianObject is a novel framework designed for highquality 3D object reconstruction from extremely sparse 360◦ views, based on 3DGS with real-time rendering capabilities. We design two main methods to achieve this goal: structure-prior-aided optimization for facilitating the multi-view consistency construction and a Gaussian repair model to remove artifacts caused by omitted or highly compressed object information. We also provide a COLMAPfree version that can be easily applied in real life with competitive performance. We sincerely hope that GaussianObject can advance daily-life applications of reconstructing 3D objects, markedly reducing capture requirements and broadening application prospects.

# ACKNOWLEDGMENTS

This work was supported by the NSFC under Grant 62322604 and 62176159, and in part by the Shanghai Municipal Science and Technology Major Project under Grant 2021SHZDZX0102. The authors express gratitude to the anonymous reviewers for their valuable feedback and to Deyu Wang for his assistance with figure drawing and Blender support.

# REFERENCES

Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P. Srinivasan, and Peter Hedman. 2021. Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields. 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2021), 5460–5469.   
Shariq Farooq Bhat, Reiner Birkl, Diana Wofk, Peter Wonka, and Matthias Müller. 2023. Zoedepth: Zero-shot transfer by combining relative and metric depth. arXiv preprint arXiv:2302.12288 (2023).   
James Burgess, Kuan-Chieh Wang, and Serena Yeung. 2024. Viewpoint Textual Inversion: Unleashing Novel View Synthesis with Pretrained 2D Diffusion Models. ECCV (2024).   
Jiazhong Cen, Zanwei Zhou, Jiemin Fang, Chen Yang, Wei Shen, Lingxi Xie, Dongsheng Jiang, Xiaopeng Zhang, and Qi Tian. 2023. Segment Anything in 3D with NeRFs. In NeurIPS.   
Eric R Chan, Koki Nagano, Matthew A Chan, Alexander W Bergman, Jeong Joon Park, Axel Levy, Miika Aittala, Shalini De Mello, Tero Karras, and Gordon Wetzstein. 2023.