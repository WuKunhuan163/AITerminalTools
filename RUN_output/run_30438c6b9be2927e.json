{
  "success": true,
  "query": "deep learning",
  "max_results": 2,
  "total_papers_found": 2,
  "source_results": {
    "arxiv": 2,
    "google_scholar": 2,
    "semantic_scholar": 0
  },
  "papers": [
    {
      "title": "Towards Depth Foundation Model: Recent Trends in Vision-Based Depth\n  Estimation",
      "authors": [
        "Zhen Xu",
        "Hongyu Zhou",
        "Sida Peng",
        "Haotong Lin",
        "Haoyu Guo",
        "Jiahao Shao",
        "Peishan Yang",
        "Qinglin Yang",
        "Sheng Miao",
        "Xingyi He",
        "Yifan Wang",
        "Yue Wang",
        "Ruizhen Hu",
        "Yiyi Liao",
        "Xiaowei Zhou",
        "Hujun Bao"
      ],
      "abstract": "Depth estimation is a fundamental task in 3D computer vision, crucial for\napplications such as 3D reconstruction, free-viewpoint rendering, robotics,\nautonomous driving, and AR/VR technologies. Traditional methods relying on\nhardware sensors like LiDAR are often limited by high costs, low resolution,\nand environmental sensitivity, limiting their applicability in real-world\nscenarios. Recent advances in vision-based methods offer a promising\nalternative, yet they face challenges in generalization and stability due to\neither the low-capacity model architectures or the reliance on domain-specific\nand small-scale datasets. The emergence of scaling laws and foundation models\nin other domains has inspired the development of \"depth foundation models\":\ndeep neural networks trained on large datasets with strong zero-shot\ngeneralization capabilities. This paper surveys the evolution of deep learning\narchitectures and paradigms for depth estimation across the monocular, stereo,\nmulti-view, and monocular video settings. We explore the potential of these\nmodels to address existing challenges and provide a comprehensive overview of\nlarge-scale datasets that can facilitate their development. By identifying key\narchitectures and training strategies, we aim to highlight the path towards\nrobust depth foundation models, offering insights into their future research\nand applications.",
      "url": "http://arxiv.org/abs/2507.11540v1",
      "pdf_url": "http://arxiv.org/pdf/2507.11540v1.pdf",
      "publication_date": "2025-07-15",
      "venue": "arXiv preprint",
      "citation_count": null,
      "source": "arxiv"
    },
    {
      "title": "Streaming 4D Visual Geometry Transformer",
      "authors": [
        "Dong Zhuo",
        "Wenzhao Zheng",
        "Jiahe Guo",
        "Yuqi Wu",
        "Jie Zhou",
        "Jiwen Lu"
      ],
      "abstract": "Perceiving and reconstructing 4D spatial-temporal geometry from videos is a\nfundamental yet challenging computer vision task. To facilitate interactive and\nreal-time applications, we propose a streaming 4D visual geometry transformer\nthat shares a similar philosophy with autoregressive large language models. We\nexplore a simple and efficient design and employ a causal transformer\narchitecture to process the input sequence in an online manner. We use temporal\ncausal attention and cache the historical keys and values as implicit memory to\nenable efficient streaming long-term 4D reconstruction. This design can handle\nreal-time 4D reconstruction by incrementally integrating historical information\nwhile maintaining high-quality spatial consistency. For efficient training, we\npropose to distill knowledge from the dense bidirectional visual geometry\ngrounded transformer (VGGT) to our causal model. For inference, our model\nsupports the migration of optimized efficient attention operator (e.g.,\nFlashAttention) from the field of large language models. Extensive experiments\non various 4D geometry perception benchmarks demonstrate that our model\nincreases the inference speed in online scenarios while maintaining competitive\nperformance, paving the way for scalable and interactive 4D vision systems.\nCode is available at: https://github.com/wzzheng/StreamVGGT.",
      "url": "http://arxiv.org/abs/2507.11539v1",
      "pdf_url": "http://arxiv.org/pdf/2507.11539v1.pdf",
      "publication_date": "2025-07-15",
      "venue": "arXiv preprint",
      "citation_count": null,
      "source": "arxiv"
    }
  ],
  "timestamp": "2025-07-16T19:42:02.718895",
  "run_identifier": "30438c6b9be2927e",
  "command": "SEARCH_PAPER",
  "args": [
    "deep learning",
    "--max-results",
    "2"
  ],
  "output_file": "/Users/wukunhuan/.local/bin/RUN_output/run_30438c6b9be2927e.json",
  "duration": 5
}