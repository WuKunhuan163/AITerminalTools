{
  "success": true,
  "query": "machine learning",
  "max_results": 1,
  "total_papers_found": 1,
  "source_results": {
    "arxiv": 1,
    "google_scholar": 1,
    "semantic_scholar": 0
  },
  "papers": [
    {
      "title": "Towards Depth Foundation Model: Recent Trends in Vision-Based Depth\n  Estimation",
      "authors": [
        "Zhen Xu",
        "Hongyu Zhou",
        "Sida Peng",
        "Haotong Lin",
        "Haoyu Guo",
        "Jiahao Shao",
        "Peishan Yang",
        "Qinglin Yang",
        "Sheng Miao",
        "Xingyi He",
        "Yifan Wang",
        "Yue Wang",
        "Ruizhen Hu",
        "Yiyi Liao",
        "Xiaowei Zhou",
        "Hujun Bao"
      ],
      "abstract": "Depth estimation is a fundamental task in 3D computer vision, crucial for\napplications such as 3D reconstruction, free-viewpoint rendering, robotics,\nautonomous driving, and AR/VR technologies. Traditional methods relying on\nhardware sensors like LiDAR are often limited by high costs, low resolution,\nand environmental sensitivity, limiting their applicability in real-world\nscenarios. Recent advances in vision-based methods offer a promising\nalternative, yet they face challenges in generalization and stability due to\neither the low-capacity model architectures or the reliance on domain-specific\nand small-scale datasets. The emergence of scaling laws and foundation models\nin other domains has inspired the development of \"depth foundation models\":\ndeep neural networks trained on large datasets with strong zero-shot\ngeneralization capabilities. This paper surveys the evolution of deep learning\narchitectures and paradigms for depth estimation across the monocular, stereo,\nmulti-view, and monocular video settings. We explore the potential of these\nmodels to address existing challenges and provide a comprehensive overview of\nlarge-scale datasets that can facilitate their development. By identifying key\narchitectures and training strategies, we aim to highlight the path towards\nrobust depth foundation models, offering insights into their future research\nand applications.",
      "url": "http://arxiv.org/abs/2507.11540v1",
      "pdf_url": "http://arxiv.org/pdf/2507.11540v1.pdf",
      "publication_date": "2025-07-15",
      "venue": "arXiv preprint",
      "citation_count": null,
      "source": "arxiv"
    }
  ],
  "timestamp": "2025-07-16T20:55:53.908022",
  "run_identifier": "17cd68452a4cba16",
  "command": "SEARCH_PAPER",
  "args": [
    "machine learning",
    "--max-results",
    "1"
  ],
  "output_file": "/Users/wukunhuan/.local/bin/RUN_output/run_17cd68452a4cba16.json",
  "duration": 8
}