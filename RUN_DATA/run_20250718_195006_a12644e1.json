{
  "success": true,
  "content": "# 深度学习基础 (Advanced Level, Rigorous Style)\n\n## 目录\n1.  [引言](#引言)\n2.  [核心概念](#核心概念)\n    *   [反向传播算法 (Backpropagation Algorithm)](#反向传播算法-backpropagation-algorithm)\n    *   [梯度下降及其变种 (Gradient Descent and its Variants)](#梯度下降及其变种-gradient-descent-and-its-variants)\n    *   [卷积神经网络 (Convolutional Neural Networks, CNNs)](#卷积神经网络-convolutional-neural-networks-cnns)\n    *   [循环神经网络 (Recurrent Neural Networks, RNNs)](#循环神经网络-recurrent-neural-networks-rnns)\n3.  [学习路径指导](#学习路径指导)\n4.  [实践练习](#实践练习)\n5.  [精选资源推荐](#精选资源推荐)\n\n---\n\n## 1. 引言\n\n本教程旨在为具有一定数学和编程基础的学习者提供深度学习核心概念的严谨性介绍。我们将深入探讨神经网络中的关键算法、模型架构以及训练优化技术，为理解更复杂的深度学习模型和前沿研究奠定坚实基础。本教程假定读者熟悉线性代数、微积分、概率论基础，并具备Python编程能力。\n\n---\n\n## 2. 核心概念\n\n### 2.1 反向传播算法 (Backpropagation Algorithm)\n\n反向传播是训练人工神经网络的核心算法，它利用链式法则（Chain Rule）计算损失函数关于网络权重的梯度。\n\n**数学原理:**\n\n考虑一个神经网络，其输出为 $\\hat{y}$，真实值为 $y$。损失函数 $L$ 衡量了预测值与真实值之间的差异。反向传播的目标是计算 $\\frac{\\partial L}{\\partial W}$ 和 $\\frac{\\partial L}{\\partial b}$，其中 $W$ 和 $b$ 分别代表网络的权重和偏置。\n\n对于一个多层网络，假设第 $l$ 层有 $n_l$ 个神经元，其输出 $a^{(l)}$ 是上一层输出 $a^{(l-1)}$ 和该层参数 $W^{(l)}, b^{(l)}$ 的函数：\n$z^{(l)} = W^{(l)} a^{(l-1)} + b^{(l)}$\n$a^{(l)} = \\sigma(z^{(l)})$\n其中 $\\sigma$ 是激活函数。\n\n根据链式法则，损失函数 $L$ 对任意权重 $W_{ij}^{(l)}$ 的偏导数可以通过以下方式计算：\n$\\frac{\\partial L}{\\partial W_{ij}^{(l)}} = \\frac{\\partial L}{\\partial a^{(l)}} \\frac{\\partial a^{(l)}}{\\partial z^{(l)}} \\frac{\\partial z^{(l)}}{\\partial W_{ij}^{(l)}}$\n\n我们定义**误差项** $\\delta^{(l)} = \\frac{\\partial L}{\\partial z^{(l)}}$。则：\n$\\frac{\\partial a^{(l)}}{\\partial z^{(l)}} = \\sigma'(z^{(l)})$ (激活函数的导数)\n$\\frac{\\partial z^{(l)}}{\\partial W_{ij}^{(l)}} = a_j^{(l-1)}$ (上一层第 $j$ 个神经元的输出)\n\n因此，$\\frac{\\partial L}{\\partial W_{ij}^{(l)}} = \\delta_i^{(l)} a_j^{(l-1)}$。\n\n对于误差项 $\\delta^{(l)}$，我们可以通过**后向传播**（从输出层向前一层传递误差）来计算：\n$\\delta^{(l)} = \\frac{\\partial L}{\\partial z^{(l)}} = \\frac{\\partial L}{\\partial a^{(l)}} \\frac{\\partial a^{(l)}}{\\partial z^{(l)}} = \\left( \\frac{\\partial L}{\\partial z^{(l+1)}} \\frac{\\partial z^{(l+1)}}{\\partial a^{(l)}} \\right) \\sigma'(z^{(l)})$\n$\\frac{\\partial z^{(l+1)}}{\\partial a^{(l)}} = (W^{(l+1)})^T$\n\n所以，$\\delta^{(l)} = ((W^{(l+1)})^T \\delta^{(l+1)}) \\odot \\sigma'(z^{(l)})$，其中 $\\odot$ 表示逐元素乘法（Hadamard product）。\n\n**计算图视角:**\n\n反向传播可以被理解为在计算图上从输出节点（损失）向输入节点（权重）回溯计算导数的过程。\n\n**代码示例 (概念性，使用NumPy):**\n\n```python\nimport numpy as np\n\n# 假设 sigmoid 激活函数及其导数\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\ndef sigmoid_prime(x):\n    return sigmoid(x) * (1 - sigmoid(x))\n\n# 假设 MSE 损失函数及其导数\ndef mse_loss(y_true, y_pred):\n    return np.mean((y_true - y_pred)**2)\n\ndef mse_loss_prime(y_true, y_pred):\n    return 2 * (y_pred - y_true) / y_true.shape[0] # 均值求导\n\n# 简单网络结构: Input -> Hidden (Sigmoid) -> Output (Identity)\n# 假设输入 x, 权重 W1, b1 (隐藏层), W2, b2 (输出层)\n\n# 前向传播\n# hidden_layer_input = np.dot(x, W1) + b1\n# hidden_layer_output = sigmoid(hidden_layer_input)\n# output_layer_input = np.dot(hidden_layer_output, W2) + b2\n# predicted_output = output_layer_input # Identity activation\n\n# 反向传播\n# 1. 计算输出层误差\n# delta_output = mse_loss_prime(y_true, predicted_output) * identity_prime(output_layer_input)\n# identity_prime is 1. So, delta_output = mse_loss_prime(y_true, predicted_output)\n\n# 2. 计算输出层权重梯度\n# dW2 = np.dot(hidden_layer_output.T, delta_output)\n# db2 = np.sum(delta_output, axis=0, keepdims=True)\n\n# 3. 计算隐藏层误差\n# delta_hidden = np.dot(delta_output, W2.T) * sigmoid_prime(hidden_layer_input)\n\n# 4. 计算隐藏层权重梯度\n# dW1 = np.dot(x.T, delta_hidden)\n# db1 = np.sum(delta_hidden, axis=0, keepdims=True)\n```\n\n### 2.2 梯度下降及其变种 (Gradient Descent and its Variants)\n\n梯度下降是一种优化算法，用于最小化损失函数。它通过迭代地沿着损失函数梯度的反方向更新模型参数来寻找最优解。\n\n**基本梯度下降 (Batch Gradient Descent, BGD):**\n在每次参数更新时，使用整个训练数据集计算损失函数的梯度。\n$$ \\theta_{t+1} = \\theta_t - \\eta \\nabla L(\\theta_t) $$\n其中 $\\theta$ 是模型参数，$\\eta$ 是学习率。\n*   **优点:** 保证收敛到局部（或全局）最优解（对于凸函数）。\n*   **缺点:** 计算成本高，尤其对于大规模数据集；需要将整个数据集加载到内存。\n\n**随机梯度下降 (Stochastic Gradient Descent, SGD):**\n在每次参数更新时，仅使用一个随机选择的训练样本计算梯度。\n$$ \\theta_{t+1} = \\theta_t - \\eta \\nabla L(\\theta_t; x^{(i)}, y^{(i)}) $$\n*   **优点:** 计算效率高，内存需求小；引入噪声有助于跳出局部最优。\n*   **缺点:** 梯度估计方差大，导致收敛路径不稳定，可能无法精确收敛到最小值。\n\n**小批量梯度下降 (Mini-batch Gradient Descent):**\n在每次参数更新时，使用一小批（m个）随机选择的训练样本计算梯度。\n$$ \\theta_{t+1} = \\theta_t - \\eta \\nabla L(\\theta_t; x^{(i:i+m)}, y^{(i:i+m)}) $$\n*   **优点:** 结合了BGD的稳定性（相比SGD）和SGD的效率（相比BGD）；可以利用矩阵运算进行高效计算。\n*   **缺点:** 需要选择合适的批量大小 $m$。\n\n**动量法 (Momentum):**\n引入一个“动量”项，加速在相关方向上的更新，并减缓在不相关方向上的震荡。\n$$ v_{t+1} = \\gamma v_t + \\eta \\nabla L(\\theta_t) $$\n$$ \\theta_{t+1} = \\theta_t - v_{t+1} $$\n其中 $\\gamma$ 是动量系数（通常取0.9）。$v_t$ 是速度向量。\n\n**Nesterov 加速梯度 (Nesterov Accelerated Gradient, NAG):**\n在计算梯度前，先“向前看”一步。\n$$ v_{t+1} = \\gamma v_t + \\eta \\nabla L(\\theta_t - \\gamma v_t) $$\n$$ \\theta_{t+1} = \\theta_t - v_{t+1} $$\n\n**自适应学习率方法 (Adaptive Learning Rate Methods):**\n根据参数的历史梯度信息，为每个参数分配不同的学习率。\n\n*   **Adagrad:** 为频繁更新的参数降低学习率，为不频繁更新的参数提高学习率。\n    $$ G_t = G_{t-1} + \\nabla L(\\theta_t)^2 \\quad (\\text{element-wise square}) $$\n    $$ \\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{G_t + \\epsilon}} \\odot \\nabla L(\\theta_t) $$\n    其中 $\\epsilon$ 是一个很小的常数，防止除以零。\n    *   **缺点:** 学习率单调递减，可能过早停止学习。\n\n*   **RMSprop:** Adagrad 的改进，使用指数加权平均来计算平方梯度。\n    $$ E[g^2]_t = \\beta E[g^2]_{t-1} + (1-\\beta) \\nabla L(\\theta_t)^2 $$\n    $$ \\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{E[g^2]_t + \\epsilon}} \\odot \\nabla L(\\theta_t) $$\n    其中 $\\beta$ 是衰减率（通常取0.9）。\n\n*   **Adam (Adaptive Moment Estimation):** 结合了Momentum和RMSprop的优点，同时计算一阶矩（均值）和二阶矩（方差）的估计。\n    $$ m_t = \\beta_1 m_{t-1} + (1-\\beta_1) \\nabla L(\\theta_t) $$\n    $$ v_t = \\beta_2 v_{t-1} + (1-\\beta_2) \\nabla L(\\theta_t)^2 $$\n    $$ \\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}, \\quad \\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t} $$\n    $$ \\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{\\hat{v}_t} + \\epsilon} \\odot \\hat{m}_t $$\n    其中 $\\beta_1, \\beta_2$ 是超参数（通常取0.9, 0.999）。$\\hat{m}_t, \\hat{v}_t$ 是偏差校正后的估计。\n    *   **AdamW:** Adam 的改进，将权重衰减（L2正则化）从梯度计算中分离出来，效果更好。\n\n**代码示例 (PyTorch):**\n\n```python\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n# 假设有一个简单的模型\nmodel = nn.Linear(10, 1)\ncriterion = nn.MSELoss()\noptimizer_sgd = optim.SGD(model.parameters(), lr=0.01)\noptimizer_adam = optim.Adam(model.parameters(), lr=0.001)\n\n# 假设有输入数据 input_data 和目标数据 target_data\n# input_data.shape = (batch_size, 10)\n# target_data.shape = (batch_size, 1)\n\n# 训练循环 (以Adam为例)\n# for epoch in range(num_epochs):\n#     # 前向传播\n#     outputs = model(input_data)\n#     loss = criterion(outputs, target_data)\n#\n#     # 反向传播和优化\n#     optimizer_adam.zero_grad() # 清零之前的梯度\n#     loss.backward()           # 计算梯度\n#     optimizer_adam.step()     # 更新参数\n#\n#     if (epoch+1) % 100 == 0:\n#         print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n```\n\n### 2.3 卷积神经网络 (Convolutional Neural Networks, CNNs)\n\nCNNs 是专门为处理具有网格状拓扑结构的数据（如图像）设计的神经网络。其核心是卷积层和池化层。\n\n**卷积层 (Convolutional Layer):**\n通过卷积核（滤波器）在输入数据上滑动，执行卷积操作。\n*   **卷积核 (Kernel/Filter):** 一个小的权重矩阵，用于提取局部特征。\n*   **步长 (Stride):** 卷积核滑动的步长。\n*   **填充 (Padding):** 在输入数据的边缘添加零值，以控制输出特征图的尺寸，并允许卷积核在边缘进行更多次计算。\n\n**数学原理:**\n对于一个输入特征图 $I$ 和卷积核 $K$，输出特征图 $O$ 的计算（不考虑偏置和激活函数）为：\n$$ O(i, j) = \\sum_{m} \\sum_{n} I(i+m, j+n) \\cdot K(m, n) $$\n其中 $(i, j)$ 是输出特征图的坐标，$(m, n)$ 是卷积核的坐标。\n\n**感受野 (Receptive Field):**\nCNN中，一个输出特征图上的单个神经元所能“看到”的输入图像区域。随着网络层数的加深，神经元的感受野会逐渐增大，从而能够捕捉到更高级、更抽象的特征。\n\n**池化层 (Pooling Layer):**\n通常在卷积层之后使用，用于降低特征图的空间维度（降采样），减少计算量，并提供一定程度的平移不变性。\n*   **最大池化 (Max Pooling):** 选择区域内的最大值。\n*   **平均池化 (Average Pooling):** 计算区域内的平均值。\n\n**典型CNN架构:**\n*   **LeNet-5:** 最早的CNN之一，用于手写数字识别。\n*   **AlexNet:** 引入ReLU激活函数、Dropout、数据增强等技术，在ImageNet竞赛中取得突破。\n*   **VGGNet:** 使用更小的卷积核（3x3）堆叠，构建更深的网络。\n*   **GoogLeNet (Inception):** 引入Inception模块，并行使用不同大小的卷积核和池化，提高网络宽度和效率。\n*   **ResNet (Residual Network):** 引入残差连接（shortcut connections），有效解决了深度网络中的梯度消失问题，使得训练非常深的网络成为可能。\n\n**代码示例 (PyTorch):**\n\n```python\nimport torch\nimport torch.nn as nn\n\n# 定义一个卷积层\n# in_channels: 输入通道数 (例如，灰度图为1，彩色图为3)\n# out_channels: 输出通道数 (卷积核的数量)\n# kernel_size: 卷积核的尺寸 (例如，3表示3x3)\n# stride: 步长\n# padding: 填充\nconv_layer = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1)\n\n# 定义一个最大池化层\npool_layer = nn.MaxPool2d(kernel_size=2, stride=2)\n\n# 假设有一个输入张量 input_image\n# input_image.shape = (batch_size, in_channels, height, width)\n\n# forward pass\n# convolved_output = conv_layer(input_image)\n# pooled_output = pool_layer(convolved_output)\n\n# 尺寸计算示例:\n# Input: N x C_in x H_in x W_in\n# Conv2d output: N x C_out x H_out x W_out\n# H_out = floor((H_in + 2*padding - kernel_size) / stride) + 1\n# W_out = floor((W_in + 2*padding - kernel_size) / stride) + 1\n# MaxPool2d output: N x C x H_out x W_out (stride=kernel_size, padding=0)\n# H_out = floor((H_in - kernel_size) / stride) + 1\n# W_out = floor((W_in - kernel_size) / stride) + 1\n```\n\n### 2.4 循环神经网络 (Recurrent Neural Networks, RNNs)\n\nRNNs 适用于处理序列数据，其核心思想是引入“记忆”——隐藏状态（hidden state），使得网络能够利用过去的信息来影响当前时刻的计算。\n\n**基本 RNN 结构:**\n在每个时间步 $t$，RNN接收当前输入 $x^{(t)}$ 和前一时间步的隐藏状态 $h^{(t-1)}$，计算当前隐藏状态 $h^{(t)}$ 和输出 $y^{(t)}$。\n$h^{(t)} = \\sigma(W_{hh} h^{(t-1)} + W_{xh} x^{(t)} + b_h)$\n$y^{(t)} = W_{hy} h^{(t)} + b_y$\n其中 $W_{hh}, W_{xh}, W_{hy}$ 是网络参数，$\\sigma$ 是激活函数。\n\n**梯度消失/爆炸问题:**\n在反向传播过程中，由于重复乘以相同的权重矩阵（在时间维度上展开），梯度会随着时间步的增加而指数级地衰减（消失）或增长（爆炸），导致网络难以学习长期依赖关系。\n\n**长短期记忆网络 (Long Short-Term Memory, LSTM):**\nLSTM 通过引入门控机制（如遗忘门、输入门、输出门）和细胞状态（cell state）来解决梯度消失问题，能够有效地学习长期依赖。\n\n*   **细胞状态 (Cell State, $C_t$):** 贯穿整个序列的“记忆通道”。\n*   **遗忘门 (Forget Gate, $f_t$):** 决定从细胞状态中丢弃哪些信息。\n    $f_t = \\sigma(W_{f} [h_{t-1}, x_t] + b_f)$\n*   **输入门 (Input Gate, $i_t$):** 决定哪些新信息被存储到细胞状态。\n    $i_t = \\sigma(W_{i} [h_{t-1}, x_t] + b_i)$\n    $\\tilde{C}_t = \\tanh(W_{C} [h_{t-1}, x_t] + b_C)$ （候选值）\n*   **更新细胞状态:**\n    $C_t = f_t \\odot C_{t-1} + i_t \\odot \\tilde{C}_t$\n*   **输出门 (Output Gate, $o_t$):** 决定输出什么（基于细胞状态）。\n    $o_t = \\sigma(W_{o} [h_{t-1}, x_t] + b_o)$\n    $h_t = o_t \\odot \\tanh(C_t)$\n\n**门控循环单元 (Gated Recurrent Unit, GRU):**\nGRU 是 LSTM 的简化版本，它合并了细胞状态和隐藏状态，并使用两个门（更新门和重置门）。\n\n*   **更新门 (Update Gate, $z_t$):** 控制前一个隐藏状态有多少被保留，以及有多少新的候选隐藏状态被添加。\n    $z_t = \\sigma(W_z [h_{t-1}, x_t] + b_z)$\n*   **重置门 (Reset Gate, $r_t$):** 控制前一个隐藏状态在计算候选隐藏状态时被忽略的程度。\n    $r_t = \\sigma(W_r [h_{t-1}, x_t] + b_r)$\n*   **候选隐藏状态 ($\\tilde{h}_t$):**\n    $\\tilde{h}_t = \\tanh(W_h [r_t \\odot h_{t-1}, x_t] + b_h)$\n*   **更新隐藏状态:**\n    $h_t = (1 - z_t) \\odot h_{t-1} + z_t \\odot \\tilde{h}_t$\n\n**代码示例 (PyTorch):**\n\n```python\nimport torch\nimport torch.nn as nn\n\n# 定义一个LSTM层\n# input_size: 输入特征的维度\n# hidden_size: 隐藏状态的维度\n# num_layers: LSTM堆叠的层数\n# batch_first: 如果为True, 输入和输出的张量形状为 (batch, seq, feature)\nlstm_layer = nn.LSTM(input_size=10, hidden_size=20, num_layers=1, batch_first=True)\n\n# 假设有一个输入张量 input_sequence\n# input_sequence.shape = (batch_size, sequence_length, input_size)\n\n# 初始隐藏状态和细胞状态 (如果未提供，PyTorch会自动初始化为零)\n# h0 = torch.zeros(num_layers, batch_size, hidden_size)\n# c0 = torch.zeros(num_layers, batch_size, hidden_size)\n\n# forward pass\n# output, (hn, cn) = lstm_layer(input_sequence, (h0, c0))\n# output.shape = (batch_size, sequence_length, hidden_size)\n# hn.shape = (num_layers, batch_size, hidden_size) # 最后一个时间步的隐藏状态\n# cn.shape = (num_layers, batch_size, hidden_size) # 最后一个时间步的细胞状态\n```\n\n---\n\n## 3. 学习路径指导\n\n本教程建议按照以下顺序学习，以建立扎实的深度学习基础：\n\n1.  **数学与Python基础回顾:** 确保对线性代数（向量、矩阵运算）、微积分（导数、链式法则）以及NumPy有扎实的掌握。\n2.  **神经网络基础:** 理解感知器、激活函数、前向传播、损失函数。\n3.  **反向传播算法:** 深入理解链式法则在神经网络中的应用，这是所有后续学习的基础。\n4.  **优化算法:** 学习梯度下降及其各种变种（SGD, Adam等），理解它们如何影响训练过程和收敛性。\n5.  **模型构建与正则化:** 学习构建多层感知器 (MLP)，并掌握防止过拟合的技术（L1/L2正则化、Dropout、Batch Normalization）。\n6.  **卷积神经网络 (CNNs):** 学习卷积、池化操作的原理，理解感受野，并熟悉经典的CNN架构（如ResNet）。\n7.  **循环神经网络 (RNNs):** 学习RNN、LSTM、GRU的结构，理解它们如何处理序列数据以及如何解决梯度问题。\n8.  **注意力机制与Transformer (进阶):** 了解注意力机制如何增强模型能力，以及Transformer架构的革命性影响。\n9.  **迁移学习与预训练模型:** 学习如何利用已有的预训练模型加速学习和提升性能。\n\n---\n\n## 4. 实践练习\n\n1.  **手算反向传播:**\n    *   **题目:** 实现一个包含一个隐藏层（ReLU激活）和一个输出层（线性激活）的简单神经网络。给定一个输入样本、权重和偏置，手动计算损失函数（MSE）关于所有权重和偏置的梯度。\n    *   **要求:** 严格按照链式法则推导，并与NumPy实现进行对比验证。\n\n2.  **使用PyTorch/TensorFlow实现端到端训练:**\n    *   **任务:** 在MNIST数据集上训练一个MLP模型，实现二分类任务（区分数字0和1）。\n    *   **要求:**\n        *   构建一个包含至少一个隐藏层的MLP。\n        *   使用交叉熵损失函数。\n        *   尝试比较 `SGD` 和 `Adam` 优化器的训练效果（收敛速度、最终准确率）。\n        *   实现并应用 `Dropout` 来防止过拟合，并观察其对训练和验证集性能的影响。\n\n3.  **CNN图像分类与分析:**\n    *   **任务:** 在CIFAR-10数据集上训练一个基础的CNN模型（例如，模仿VGG的结构，包含几层卷积、池化和全连接层）。\n    *   **要求:**\n        *   实现模型结构。\n        *   使用数据增强技术（随机翻转、裁剪等），并分析其对模型泛化能力的影响。\n        *   可视化CNN的卷积核，尝试理解其学习到的特征。\n        *   分析模型在训练集和验证集上的损失和准确率曲线，诊断过拟合或欠拟合问题。\n\n---\n\n## 5. 精选资源推荐\n\n**书籍:**\n\n*   **《Deep Learning》** (Ian Goodfellow, Yoshua Bengio, Aaron Courville): 深度学习领域的权威著作，内容严谨，数学推导详尽。\n*   **《Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow》** (Aurélien Géron): 实践性极强，涵盖了从基础到高级的广泛主题，代码示例丰富。\n\n**在线课程:**\n\n*   **Stanford CS231n: Convolutional Neural Networks for Visual Recognition:** 图像处理领域的经典课程，提供公开的视频、笔记和作业。\n*   **Stanford CS224n: Natural Language Processing with Deep Learning:** NLP领域的权威课程，深入讲解了RNN、Attention和Transformer等模型。\n*   **fast.ai's Practical Deep Learning for Coders:** 强调代码优先和实践驱动的学习方法，非常适合快速上手并掌握实用的深度学习技术。\n\n**框架文档与教程:**\n\n*   **PyTorch Tutorials:** [https://pytorch.org/tutorials/](https://pytorch.org/tutorials/)\n*   **TensorFlow Tutorials:** [https://www.tensorflow.org/tutorials](https://www.tensorflow.org/tutorials)\n*   **Hugging Face Transformers Documentation:** [https://huggingface.co/docs/transformers/index](https://huggingface.co/docs/transformers/index) (对于NLP任务至关重要)\n\n**其他:**\n\n*   **arXiv.org:** 获取最新的研究论文。\n*   **Papers With Code:** 查找论文对应的代码实现。\n*   **Kaggle:** 参与竞赛，学习他人的Notebook，使用各种数据集。",
  "model": "google/gemini-2.5-flash-lite-preview-06-17",
  "usage": {
    "input_tokens": 7107,
    "output_tokens": 6271,
    "total_tokens": 13378
  },
  "cost": 0.0032191,
  "model_info": {
    "input_cost_per_1m": 0.1,
    "output_cost_per_1m": 0.4,
    "context_length": 1048576,
    "useable": true
  }
}