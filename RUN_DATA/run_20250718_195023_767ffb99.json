{
  "success": true,
  "content": "```markdown\n# 深度学习基础 - 练习题\n\n## 1. 基础知识测试 (Fundamental Knowledge)\n\n1.  **问题:** 在反向传播算法中，链式法则的核心作用是什么？\n    <details>\n    <summary>答案</summary>\n    链式法则用于计算复合函数的导数，在神经网络中，它使得我们能够逐层计算损失函数关于网络参数（权重和偏置）的梯度，从输出层向输入层传播误差信号。\n    </details>\n\n2.  **问题:** 简述小批量梯度下降 (Mini-batch Gradient Descent) 相较于随机梯度下降 (SGD) 和批量梯度下降 (Batch Gradient Descent) 的主要优势。\n    <details>\n    <summary>答案</summary>\n    小批量梯度下降在计算效率和收敛稳定性之间取得了平衡。相比SGD，它使用多个样本批次，降低了梯度估计的方差，使得收敛更稳定；相比BGD，它避免了在每次更新时使用整个数据集，减少了计算成本和内存需求，并且可以通过向量化操作提高效率。\n    </details>\n\n3.  **问题:** 在CNN中，卷积层的“步长 (Stride)”参数主要影响什么？\n    <details>\n    <summary>答案</summary>\n    步长参数控制着卷积核在输入特征图上滑动的间隔。较大的步长会导致输出特征图的空间维度（高度和宽度）减小得更快，从而减少计算量和模型参数，但可能丢失一些细节信息。\n    </details>\n\n## 2. 理解应用 (Comprehension and Application)\n\n1.  **问题:** 考虑一个具有两个隐藏层的MLP，其激活函数分别为ReLU和Sigmoid。若在反向传播过程中，某个隐藏层的输出梯度为零，这可能由什么原因引起？\n    <details>\n    <summary>答案</summary>\n    如果一个隐藏层的输出梯度为零，这可能意味着：\n    *   **Sigmoid饱和:** 如果该隐藏层的激活函数是Sigmoid，并且其输入值非常大（正向或负向），Sigmoid函数会趋于饱和（输出接近0或1），此时其导数非常接近零，导致梯度消失。\n    *   **ReLU的死亡节点:** 如果该隐藏层的激活函数是ReLU，并且输入为负，ReLU的输出将是零，其导数也为零。如果一个神经元持续接收负输入，它就可能成为“死亡ReLU”，无法再参与梯度更新。\n    *   **权重或前层梯度为零:** 即使激活函数导数不为零，如果前一层的梯度或者权重本身为零，也会导致该层的输出梯度为零。\n    </details>\n\n2.  **问题:** LSTM中的“遗忘门”和“输入门”在防止梯度消失方面扮演了什么角色？\n    <details>\n    <summary>答案</summary>\n    遗忘门和输入门通过对细胞状态 (Cell State) 进行选择性地读写，有效地缓解了梯度消失问题。\n    *   **遗忘门 (Forget Gate):** 允许LSTM选择性地“忘记”细胞状态中的旧信息。通过将遗忘门的输出与前一时间步的细胞状态相乘，可以控制信息流。当遗忘门输出接近1时，信息得以保留；当接近0时，信息被遗忘。这使得梯度能够更容易地“流过”不受旧信息干扰的路径。\n    *   **输入门 (Input Gate):** 控制新的信息如何被添加到细胞状态。它决定了哪些新的输入特征是重要的，并将它们与候选值（由tanh激活函数产生）结合起来。\n    通过这种门控机制，LSTM能够更精确地控制信息在时间序列中的传递和累积，从而在反向传播时，即使经过多个时间步，梯度也能够被保留下来，学习到长期依赖关系。\n    </details>\n\n3.  **问题:** 假设你在训练一个CNN模型用于图像识别，发现模型在训练集上表现极佳，但在验证集上表现很差。你采取了哪些策略来解决这个问题？请至少列举三种。\n    <details>\n    <summary>答案</summary>\n    这种情况表明模型发生了**过拟合**。可以采取以下策略：\n    1.  **增加训练数据:** 收集更多样化的训练样本，使模型学习到更具泛化能力的特征。\n    2.  **数据增强 (Data Augmentation):** 对现有训练数据进行随机变换（如翻转、裁剪、旋转、颜色抖动等），生成更多训练样本，增加数据的多样性。\n    3.  **正则化 (Regularization):**\n        *   **L1/L2正则化:** 在损失函数中加入 L1 或 L2 范数项，惩罚大的权重，限制模型的复杂度。\n        *   **Dropout:** 在训练过程中，随机“丢弃”一部分神经元及其连接，迫使网络学习冗余表示，减少对特定神经元的依赖。\n    4.  **Batch Normalization:** 在卷积层或全连接层之后、激活函数之前添加 Batch Normalization 层，可以稳定训练过程，并起到一定的正则化作用。\n    5.  **提前停止 (Early Stopping):** 监控模型在验证集上的性能，当性能不再提升或开始下降时，停止训练，保留在验证集上表现最好的模型。\n    6.  **简化模型:** 减少模型的层数、卷积核的数量或全连接层的节点数，降低模型的容量。\n    </details>\n\n## 3. 实践练习 (Practical Exercises)\n\n1.  **问题:** 使用PyTorch/TensorFlow实现一个包含一个隐藏层（使用ReLU激活）和一个输出层（使用Sigmoid激活）的二分类器。在模拟数据上进行训练，并观察学习率对收敛速度和最终性能的影响。\n    *   **要求:**\n        *   生成一个包含1000个样本、20个特征的合成数据集，特征之间存在一定的线性关系，并引入一些噪声。\n        *   构建MLP模型。\n        *   使用二元交叉熵损失函数。\n        *   分别尝试使用 `SGD` 优化器，学习率设置为 `0.1`, `0.01`, `0.001`，并记录训练100轮后的损失值和准确率。\n        *   使用 `Adam` 优化器，学习率设置为 `0.01`, `0.001`, `0.0001`，并记录训练100轮后的损失值和准确率。\n        *   简要分析不同学习率和优化器组合的效果。\n    <details>\n    <summary>参考实现思路</summary>\n    ```python\n    import torch\n    import torch.nn as nn\n    import torch.optim as optim\n    import numpy as np\n    from sklearn.model_selection import train_test_split\n    from sklearn.metrics import accuracy_score\n\n    # 1. 数据生成\n    np.random.seed(42)\n    n_samples = 1000\n    n_features = 20\n    X = np.random.randn(n_samples, n_features)\n    # 模拟一个线性关系，并加入噪声\n    true_weights = np.random.randn(n_features, 1)\n    true_bias = np.random.randn(1)\n    logits = X @ true_weights + true_bias\n    # 使用sigmoid生成概率，然后采样二元标签\n    probabilities = 1 / (1 + np.exp(-logits))\n    y = (probabilities > np.random.rand(n_samples, 1)).astype(int)\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    # 转换为PyTorch张量\n    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n    y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n    y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n\n    # 2. 模型定义\n    class SimpleMLP(nn.Module):\n        def __init__(self, input_size, hidden_size, output_size):\n            super(SimpleMLP, self).__init__()\n            self.layer_1 = nn.Linear(input_size, hidden_size)\n            self.relu = nn.ReLU()\n            self.layer_2 = nn.Linear(hidden_size, output_size)\n            self.sigmoid = nn.Sigmoid()\n\n        def forward(self, x):\n            x = self.layer_1(x)\n            x = self.relu(x)\n            x = self.layer_2(x)\n            x = self.sigmoid(x)\n            return x\n\n    # 3. 训练与评估函数\n    def train_and_evaluate(optimizer_name, lr, num_epochs=100):\n        model = SimpleMLP(input_size=n_features, hidden_size=64, output_size=1)\n        criterion = nn.BCELoss() # Binary Cross Entropy Loss\n\n        if optimizer_name == 'SGD':\n            optimizer = optim.SGD(model.parameters(), lr=lr)\n        elif optimizer_name == 'Adam':\n            optimizer = optim.Adam(model.parameters(), lr=lr)\n        else:\n            raise ValueError(\"Optimizer not supported\")\n\n        for epoch in range(num_epochs):\n            # Forward pass\n            outputs = model(X_train_tensor)\n            loss = criterion(outputs, y_train_tensor)\n\n            # Backward and optimize\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            if (epoch+1) % 20 == 0:\n                print(f'Optimizer: {optimizer_name}, LR: {lr}, Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n\n        # 评估\n        model.eval()\n        with torch.no_grad():\n            train_outputs = model(X_train_tensor)\n            train_predicted = (train_outputs > 0.5).float()\n            train_accuracy = accuracy_score(y_train_tensor.numpy(), train_predicted.numpy())\n\n            test_outputs = model(X_test_tensor)\n            test_predicted = (test_outputs > 0.5).float()\n            test_accuracy = accuracy_score(y_test_tensor.numpy(), test_predicted.numpy())\n\n        print(f'--- Final Results ---')\n        print(f'Optimizer: {optimizer_name}, LR: {lr}')\n        print(f'Train Loss: {loss.item():.4f}, Train Accuracy: {train_accuracy:.4f}')\n        print(f'Test Accuracy: {test_accuracy:.4f}\\n')\n        return loss.item(), train_accuracy, test_accuracy\n\n    # 4. 运行实验\n    print(\"--- Testing SGD ---\")\n    for lr in [0.1, 0.01, 0.001]:\n        train_and_evaluate('SGD', lr)\n\n    print(\"\\n--- Testing Adam ---\")\n    for lr in [0.01, 0.001, 0.0001]:\n        train_and_evaluate('Adam', lr)\n    ```\n    **分析:** 观察不同学习率下模型的收敛速度（损失下降的快慢）和最终的训练/测试准确率。通常，过高的学习率会导致震荡甚至发散，过低的学习率会使得收敛非常缓慢。Adam通常比SGD收敛更快，并且对学习率的选择不太敏感。\n    </details>\n\n2.  **问题:** 实现一个简单的RNN或LSTM模型，用于预测一个简单的序列数据，例如时间序列的下一个值。\n    *   **要求:**\n        *   生成一个简单的周期性序列数据（如正弦波或Sawtooth波）。\n        *   将序列分割成输入序列和目标值（例如，使用前10个点预测第11个点）。\n        *   构建一个包含一个LSTM层和线性输出层的模型。\n        *   训练模型，并可视化预测结果与真实值。\n    <details>\n    <summary>参考实现思路</summary>\n    ```python\n    import torch\n    import torch.nn as nn\n    import numpy as np\n    import matplotlib.pyplot as plt\n\n    # 1. 数据生成 (Sawtooth wave example)\n    def generate_sawtooth_wave(n_samples=1000, seq_length=10):\n        X_data = []\n        y_data = []\n        for _ in range(n_samples):\n            # Generate a sequence of numbers from 0 to 10, then wrap around\n            start_val = np.random.randint(0, 10)\n            sequence = np.arange(start_val, start_val + seq_length + 1) % 10\n            X_data.append(sequence[:seq_length])\n            y_data.append(sequence[seq_length])\n        return np.array(X_data), np.array(y_data)\n\n    seq_length = 10\n    n_samples = 1000\n    X_np, y_np = generate_sawtooth_wave(n_samples, seq_length)\n\n    # Reshape for LSTM: (batch_size, seq_length, input_size)\n    X_np = X_np.reshape(-1, seq_length, 1)\n    y_np = y_np.reshape(-1, 1)\n\n    X_tensor = torch.tensor(X_np, dtype=torch.float32)\n    y_tensor = torch.tensor(y_np, dtype=torch.float32)\n\n    # 2. 模型定义\n    class SimpleRNN(nn.Module):\n        def __init__(self, input_size, hidden_size, output_size):\n            super(SimpleRNN, self).__init__()\n            self.hidden_size = hidden_size\n            # Using LSTM\n            self.rnn = nn.LSTM(input_size, hidden_size, batch_first=True)\n            self.fc = nn.Linear(hidden_size, output_size)\n\n        def forward(self, x):\n            # Initial hidden and cell states\n            h0 = torch.zeros(1, x.size(0), self.hidden_size).to(x.device)\n            c0 = torch.zeros(1, x.size(0), self.hidden_size).to(x.device)\n\n            # Forward propagate LSTM\n            out, _ = self.rnn(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size)\n\n            # Decode the hidden state of the last time step\n            out = self.fc(out[:, -1, :]) # Use the output of the last time step\n            return out\n\n    # 3. 训练\n    input_size = 1\n    hidden_size = 50\n    output_size = 1\n    model = SimpleRNN(input_size, hidden_size, output_size)\n\n    criterion = nn.MSELoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.005)\n\n    num_epochs = 200\n    for epoch in range(num_epochs):\n        outputs = model(X_tensor)\n        loss = criterion(outputs, y_tensor)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if (epoch+1) % 50 == 0:\n            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n\n    # 4. 可视化预测\n    model.eval()\n    with torch.no_grad():\n        predicted_values = model(X_tensor).numpy()\n\n    plt.figure(figsize=(12, 6))\n    plt.plot(y_np, label='Actual Values', alpha=0.7)\n    plt.plot(predicted_values, label='Predicted Values', alpha=0.7)\n    plt.title('Sawtooth Wave Prediction')\n    plt.xlabel('Sample Index')\n    plt.ylabel('Value')\n    plt.legend()\n    plt.show()\n    ```\n    **注意:** 实际运行此代码需要安装 `matplotlib`。\n    </details>\n\n## 4. 思考题 (Thought-Provoking Questions)\n\n1.  **问题:** Transformer 模型中的自注意力 (Self-Attention) 机制如何克服了传统 RNN 在处理长序列时的瓶颈（如梯度消失）？其计算复杂度如何？\n    <details>\n    <summary>答案</summary>\n    Transformer 的自注意力机制克服了 RNN 的长序列处理瓶颈，主要因为它：\n    1.  **并行化:** 自注意力机制在计算序列中所有位置之间的关联性时，可以并行进行，不像 RNN 需要按时间步顺序计算。这显著提高了训练速度。\n    2.  **直接路径:** 它不像 RNN 那样依赖于隐藏状态在时间步之间逐层传递信息。自注意力机制直接计算序列中任意两个位置之间的依赖关系（通过计算查询 (Query)、键 (Key)、值 (Value)），从而为长距离依赖关系提供了直接的通路，有效缓解了梯度消失问题。\n    3.  **捕获全局依赖:** 理论上，自注意力机制可以捕获序列中任意两个位置之间的关系，无论它们相距多远。\n\n    **计算复杂度:**\n    对于长度为 $N$ 的序列和隐藏维度为 $D$ 的表示：\n    *   **自注意力机制:** 计算 $Q, K, V$ 矩阵的复杂度为 $O(N \\cdot D^2)$。计算注意力分数矩阵 ($Q K^T$) 的复杂度为 $O(N^2 \\cdot D)$。最终加权求和的复杂度为 $O(N^2 \\cdot D)$。因此，自注意力机制的主要复杂度瓶颈是 $O(N^2 \\cdot D)$，其中 $N^2$ 项是二次的，对于非常长的序列会成为问题。\n    *   **RNN:** 顺序计算的计算复杂度是 $O(N \\cdot D^2)$。\n\n    因此，虽然自注意力机制在捕获长距离依赖方面更有效且可并行化，但其计算复杂度随序列长度 $N$ 呈二次方增长，这限制了它直接应用于非常长的序列。\n    </details>\n\n2.  **问题:** 在深度学习模型中，Batch Normalization (BN) 和 Layer Normalization (LN) 的主要区别是什么？它们分别在什么场景下表现更好？\n    <details>\n    <summary>答案</summary>\n    Batch Normalization (BN) 和 Layer Normalization (LN) 都是用于对神经网络的激活值进行归一化的技术，以稳定训练过程、加速收敛并起到一定的正则化作用。它们的主要区别在于归一化的维度：\n\n    **Batch Normalization (BN):**\n    *   **归一化维度:** 对一个**批次 (batch)** 中的所有样本，对**每个特征 (channel)** 进行归一化。即，对于一个批次大小为 $m$、特征维度为 $d$ 的数据，BN 会计算 $m$ 个样本在第 $i$ 个特征上的均值和方差。\n    *   **计算:**\n        $$ \\mu_i = \\frac{1}{m} \\sum_{k=1}^m x_{k,i} $$\n        $$ \\sigma_i^2 = \\frac{1}{m} \\sum_{k=1}^m (x_{k,i} - \\mu_i)^2 $$\n        $$ y_{k,i} = \\frac{x_{k,i} - \\mu_i}{\\sqrt{\\sigma_i^2 + \\epsilon}} \\odot \\gamma_i + \\beta_i $$\n        其中 $\\gamma_i$ 和 $\\beta_i$ 是可学习的缩放和平移参数。\n    *   **优点:** 在CNN等模型中效果显著，有助于缓解内部协变量偏移 (Internal Covariate Shift)，提升训练稳定性。\n    *   **缺点:**\n        *   **对批次大小敏感:** 当批次大小很小时，均值和方差的估计不够准确，可能导致性能下降。\n        *   **在RNN中实现复杂:** 由于RNN是序列模型，每个时间步的批次统计量不同，BN 的应用需要特殊处理（如使用运行均值/方差）。\n\n    **Layer Normalization (LN):**\n    *   **归一化维度:** 对**一个样本**中的所有特征进行归一化。即，对于批次中的每个样本，LN 会独立计算该样本所有特征的均值和方差。\n    *   **计算:**\n        $$ \\mu_k = \\frac{1}{d} \\sum_{i=1}^d x_{k,i} $$\n        $$ \\sigma_k^2 = \\frac{1}{d} \\sum_{i=1}^d (x_{k,i} - \\mu_k)^2 $$\n        $$ y_{k,i} = \\frac{x_{k,i} - \\mu_k}{\\sqrt{\\sigma_k^2 + \\epsilon}} \\odot \\gamma_i + \\beta_i $$\n        其中 $\\gamma_i$ 和 $\\beta_i$ 也是可学习的参数。\n    *   **优点:**\n        *   **不依赖批次大小:** 对批次大小不敏感，在小批次场景下表现稳定。\n        *   **适用于RNN和Transformer:** 在序列模型中，LN 可以独立地对每个时间步的隐藏状态进行归一化，非常有效。\n    *   **缺点:** 在CNN中，由于其按通道归一化的特性，BN 通常比LN更能捕捉到通道间的统计信息，因此在CNN中BN可能效果更好。\n\n    **场景选择:**\n    *   **CNN:** BN 通常是首选，因为它在处理图像特征时效果更好。\n    *   **RNN, Transformer, 或小批次训练:** LN 是更适合的选择，因为它不依赖批次大小，并且能很好地处理序列数据。\n    </details>\n```",
  "model": "google/gemini-2.5-flash-lite-preview-06-17",
  "usage": {
    "input_tokens": 6427,
    "output_tokens": 5163,
    "total_tokens": 11590
  },
  "cost": 0.0027079,
  "model_info": {
    "input_cost_per_1m": 0.1,
    "output_cost_per_1m": 0.4,
    "context_length": 1048576,
    "useable": true
  }
}