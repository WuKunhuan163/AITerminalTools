#!/usr/bin/env python3
"""
EXTRACT_PDF.py - Enhanced PDF extraction using MinerU with integrated post-processing
All-in-one PDF processing tool with image analysis using IMG2TEXT
"""

import os
import sys
import json
import subprocess
import hashlib
import re
import shutil
import time
from pathlib import Path
from typing import Dict, List, Optional, Tuple


def get_pdf_extractor_data_dir():
    """Get the PDF extractor data directory path."""
    script_dir = Path(__file__).parent
    # ä¼˜å…ˆä½¿ç”¨EXTRACT_PDF_DATAç›®å½•ï¼ˆæ•°æ®ä¸ä»£ç åˆ†ç¦»ï¼‰
    data_dir = script_dir / "EXTRACT_PDF_DATA"
    if not data_dir.exists():
        data_dir.mkdir(parents=True, exist_ok=True)
        # åˆ›å»ºå¿…è¦çš„å­ç›®å½•
        (data_dir / "images").mkdir(exist_ok=True)
        (data_dir / "markdown").mkdir(exist_ok=True)
    return data_dir


def save_to_unified_data_directory(content: str, pdf_path: Path, page_spec: str = None, images_data: list = None, output_dir: Path = None) -> Tuple[str, str]:
    """
    ç»Ÿä¸€çš„æ•°æ®å­˜å‚¨æ¥å£ï¼Œä¾›basicå’Œmineruæ¨¡å¼å…±ç”¨
    
    Args:
        content: markdownå†…å®¹
        pdf_path: åŸPDFæ–‡ä»¶è·¯å¾„
        page_spec: é¡µç è§„æ ¼ (å¦‚ "1", "1-5", "1,3,5")
        images_data: å›¾ç‰‡æ•°æ®åˆ—è¡¨ [{'bytes': bytes, 'hash': str, 'filename': str}, ...]
    
    Returns:
        tuple: (data_directory_md_path, pdf_directory_md_path)
    """
    import shutil
    
    # è·å–æ•°æ®ç›®å½•
    data_dir = get_pdf_extractor_data_dir()
    markdown_dir = data_dir / "markdown"
    images_dir = data_dir / "images"
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    markdown_dir.mkdir(parents=True, exist_ok=True)
    images_dir.mkdir(parents=True, exist_ok=True)
    
    # æ‰¾åˆ°ä¸‹ä¸€ä¸ªå¯ç”¨çš„æ•°å­—æ–‡ä»¶å
    counter = 0
    while True:
        target_file = markdown_dir / f"{counter}.md"
        if not target_file.exists():
            break
        counter += 1
    
    # ä¿å­˜markdownåˆ°æ•°æ®ç›®å½•
    with open(target_file, 'w', encoding='utf-8') as f:
        f.write(content)
    
    # ä¿å­˜å›¾ç‰‡åˆ°æ•°æ®ç›®å½•
    if images_data:
        for img_data in images_data:
            img_file = images_dir / img_data['filename']
            with open(img_file, 'wb') as f:
                f.write(img_data['bytes'])
    
    # åˆ›å»ºè¾“å‡ºç›®å½•çš„æ–‡ä»¶ï¼ˆå¦‚æœæŒ‡å®šäº†è¾“å‡ºç›®å½•ï¼Œå¦åˆ™ä½¿ç”¨PDFåŒå±‚ç›®å½•ï¼‰
    pdf_stem = pdf_path.stem
    if page_spec:
        pdf_stem_with_pages = f"{pdf_stem}_p{page_spec}"
    else:
        pdf_stem_with_pages = pdf_stem
    
    # ä½¿ç”¨æŒ‡å®šçš„è¾“å‡ºç›®å½•æˆ–PDFåŒå±‚ç›®å½•
    output_parent = output_dir if output_dir else pdf_path.parent
    same_name_md_file = output_parent / f"{pdf_stem_with_pages}.md"
    
    # æ›´æ–°å›¾ç‰‡è·¯å¾„åˆ°ç»å¯¹è·¯å¾„ (æŒ‡å‘EXTRACT_PDF_DATA)
    updated_content = update_image_paths_to_data_directory(content, str(data_dir))
    
    # ä¿å­˜åˆ°PDFåŒå±‚ç›®å½•
    with open(same_name_md_file, 'w', encoding='utf-8') as f:
        f.write(updated_content)
    
    # å¤åˆ¶å›¾ç‰‡åˆ°è¾“å‡ºç›®å½•çš„imagesæ–‡ä»¶å¤¹
    if images_data:
        output_images_dir = output_parent / "images"
        output_images_dir.mkdir(exist_ok=True)
        
        for img_data in images_data:
            src_file = images_dir / img_data['filename']
            dst_file = output_images_dir / img_data['filename']
            if src_file.exists():
                shutil.copy2(src_file, dst_file)
    
    return str(target_file), str(same_name_md_file)


def update_image_paths_to_data_directory(content: str, data_dir: str) -> str:
    """æ›´æ–°markdownå†…å®¹ä¸­çš„å›¾ç‰‡è·¯å¾„ï¼ŒæŒ‡å‘EXTRACT_PDF_DATAç›®å½•"""
    import re
    
    # å°†ç›¸å¯¹è·¯å¾„çš„å›¾ç‰‡å¼•ç”¨æ›´æ–°ä¸ºç»å¯¹è·¯å¾„
    def replace_image_path(match):
        image_filename = match.group(2)
        abs_image_path = Path(data_dir) / "images" / image_filename
        return f"![{match.group(1)}]({abs_image_path})"
    
    # åŒ¹é… ![...](images/filename) æ ¼å¼
    updated_content = re.sub(r'!\[([^\]]*)\]\(images/([^)]+)\)', replace_image_path, content)
    
    return updated_content


def create_postprocess_status_file(pdf_path: Path, page_spec: str = None, images_data: list = None) -> str:
    """åˆ›å»ºåå¤„ç†çŠ¶æ€æ–‡ä»¶ï¼Œç”¨äºè¿½è¸ªplaceholderå¤„ç†çŠ¶æ€"""
    import json
    from datetime import datetime
    
    pdf_stem = pdf_path.stem
    if page_spec:
        pdf_stem_with_pages = f"{pdf_stem}_p{page_spec}"
    else:
        pdf_stem_with_pages = pdf_stem
    
    status_file = pdf_path.parent / f"{pdf_stem_with_pages}_postprocess.json"
    
    # åˆ›å»ºçŠ¶æ€æ•°æ®
    status_data = {
        "pdf_file": str(pdf_path),
        "created_at": datetime.now().isoformat(),
        "page_range": page_spec,
        "total_items": len(images_data) if images_data else 0,
        "processed_items": 0,
        "items": []
    }
    
    # æ·»åŠ å›¾ç‰‡é¡¹ç›®
    if images_data:
        for img_data in images_data:
            item = {
                "id": img_data['hash'],
                "type": "image",  # basicæ¨¡å¼ä¸»è¦å¤„ç†å›¾ç‰‡
                "filename": img_data['filename'],
                "image_path": f"images/{img_data['filename']}",  # æ·»åŠ image_pathå­—æ®µ
                "processor": "basic_extractor",
                "processed": False,
                "placeholder": f"![](images/{img_data['filename']})",
                "bbox": img_data.get('bbox', []),
                "page": img_data.get('page', 1)
            }
            status_data["items"].append(item)
    
    # ä¿å­˜çŠ¶æ€æ–‡ä»¶
    with open(status_file, 'w', encoding='utf-8') as f:
        json.dump(status_data, f, ensure_ascii=False, indent=2)
    
    print(f"Post-processing status saved to: {status_file.name}")
    return str(status_file)

# åŠ è½½ç¯å¢ƒå˜é‡
from dotenv import load_dotenv
load_dotenv()

# å…¨å±€å˜é‡
original_pdf_dir = None

def is_run_environment(command_identifier=None):
    """Check if running in RUN environment by checking environment variables"""
    if command_identifier:
        return os.environ.get(f'RUN_IDENTIFIER_{command_identifier}') == 'True'
    return False

def write_to_json_output(data, command_identifier=None):
    """å°†ç»“æœå†™å…¥åˆ°æŒ‡å®šçš„ JSON è¾“å‡ºæ–‡ä»¶ä¸­"""
    if not is_run_environment(command_identifier):
        return False
    
    # Get the specific output file for this command identifier
    if command_identifier:
        output_file = os.environ.get(f'RUN_DATA_FILE_{command_identifier}')
    else:
        output_file = os.environ.get('RUN_DATA_FILE')
    
    if not output_file:
        return False
    
    try:
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        output_path = Path(output_file)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
        return True
    except Exception as e:
        print(f"Error writing to JSON output file: {e}")
        return False

class PDFExtractor:
    """PDFæå–å™¨ï¼Œé›†æˆæ‰€æœ‰PDFå¤„ç†åŠŸèƒ½"""
    
    def __init__(self, debug: bool = False):
        self.debug = debug
        self.script_dir = Path(__file__).parent
        self.proj_dir = self.script_dir / "EXTRACT_PDF_PROJ"
        
    def extract_pdf_basic(self, pdf_path: Path, page_spec: str = None, output_dir: Path = None) -> Tuple[bool, str]:
        """åŸºç¡€PDFæå–åŠŸèƒ½ - ä½¿ç”¨ç»Ÿä¸€æ•°æ®å­˜å‚¨æ¥å£"""
        import time
        import hashlib
        
        start_time = time.time()
        
        try:
            # ä½¿ç”¨Pythonçš„åŸºç¡€PDFå¤„ç†åº“
            import fitz  # PyMuPDF
            
            # æ‰“å¼€PDFæ–‡ä»¶
            doc = fitz.open(str(pdf_path))
            
            # ç¡®å®šè¦å¤„ç†çš„é¡µé¢
            if page_spec:
                pages = self._parse_page_spec(page_spec, doc.page_count)
            else:
                pages = list(range(doc.page_count))
            
            content = []
            images_data = []
            
            # ç»“æŸæ€§æ ‡ç‚¹ç¬¦å·åˆ—è¡¨ï¼ˆç”¨äºæ–‡æœ¬å¤„ç†ï¼‰
            ending_punctuations = {'ã€‚', '.', '!', '?', 'ï¼', 'ï¼Ÿ', ':', 'ï¼š', ';', 'ï¼›'}
            
            # å¤„ç†æ¯ä¸€é¡µ
            for page_num in pages:
                page = doc[page_num]
                
                # æå–æ–‡æœ¬
                text = page.get_text()
                
                # æå–å›¾ç‰‡ - ä½¿ç”¨å›¾ç‰‡åˆå¹¶åŠŸèƒ½
                image_list = page.get_images(full=True)
                page_content = f"# Page {page_num + 1}\n\n"
                
                # å›¾ç‰‡åˆå¹¶å¤„ç†ï¼šå°†ä¸´è¿‘çš„å›¾ç‰‡åˆå¹¶æˆä¸€å¼ å¤§å›¾
                if image_list:
                    # ä½¿ç”¨ä¸basicæ¨¡å¼ç›¸åŒçš„å›¾ç‰‡åˆå¹¶é€»è¾‘
                    merged_images_info = self._merge_nearby_images_to_data(doc, page, image_list, page_num + 1)
                    
                    # æ”¶é›†å›¾ç‰‡æ•°æ®
                    images_data.extend(merged_images_info)
                    
                    # ä¸ºæ¯ä¸ªåˆå¹¶åçš„å›¾ç‰‡æ·»åŠ placeholderï¼ˆä¸basicæ¨¡å¼ä¸€è‡´ï¼‰
                    for img_info in merged_images_info:
                        page_content += f"[placeholder: image]\n"
                        page_content += f"![](images/{img_info['filename']})\n\n"
                
                # å¤„ç†æ­£æ–‡æ¢è¡Œç¬¦ï¼ˆä¸basicæ¨¡å¼ä¸€è‡´ï¼‰
                processed_text = self._process_text_linebreaks(text, ending_punctuations)
                
                # æ·»åŠ é¡µé¢æ–‡æœ¬
                page_content += f"{processed_text}\n\n"
                content.append(page_content)
            
            doc.close()
            
            # åˆå¹¶æ‰€æœ‰å†…å®¹
            full_content = '\n'.join(content)
            
            # ä½¿ç”¨ç»Ÿä¸€æ•°æ®å­˜å‚¨æ¥å£ä¿å­˜æ•°æ®
            data_md_path, pdf_md_path = save_to_unified_data_directory(
                full_content, pdf_path, page_spec, images_data, output_dir
            )
            
            # åˆ›å»ºextract_dataæ–‡ä»¶å¤¹ï¼ˆç”¨æˆ·è¦æ±‚ï¼‰
            if output_dir:
                extract_data_dir = output_dir / f"{pdf_path.stem}_extract_data"
                extract_data_dir.mkdir(exist_ok=True)
                
                # å¤åˆ¶markdownæ–‡ä»¶åˆ°extract_dataæ–‡ä»¶å¤¹
                extract_data_md = extract_data_dir / f"{pdf_path.stem}.md"
                import shutil
                shutil.copy2(pdf_md_path, extract_data_md)
                
                # å¤åˆ¶å›¾ç‰‡åˆ°extract_dataæ–‡ä»¶å¤¹
                if images_data:
                    extract_data_images_dir = extract_data_dir / "images"
                    extract_data_images_dir.mkdir(exist_ok=True)
                    
                    images_dir = output_dir / "images"
                    if images_dir.exists():
                        for img_data in images_data:
                            src_file = images_dir / img_data['filename']
                            dst_file = extract_data_images_dir / img_data['filename']
                            if src_file.exists():
                                shutil.copy2(src_file, dst_file)
                
                print(f"Created extract_data folder: {extract_data_dir}")
            
            # åˆ›å»ºpostprocessçŠ¶æ€æ–‡ä»¶
            if images_data:
                create_postprocess_status_file(pdf_path, page_spec, images_data)
            
            # è®¡ç®—å¤„ç†æ—¶é—´
            end_time = time.time()
            processing_time = end_time - start_time
            
            print(f"Total processing time: {processing_time:.2f} seconds")
            print(f"Data saved to: {data_md_path}")
            if images_data:
                print(f"Extracted {len(images_data)} images")
            
            return True, f"Basic extraction completed: {pdf_md_path}"
            
        except Exception as e:
            return False, f"Basic extraction failed: {str(e)}"
    
    def extract_pdf_mineru(self, pdf_path: Path, page_spec: str = None, output_dir: Path = None, 
                          enable_analysis: bool = False, use_batch_processing: bool = True) -> Tuple[bool, str]:
        """ä½¿ç”¨MinerUè¿›è¡ŒPDFæå–"""
        import time
        
        start_time = time.time()
        
        try:
            # å¦‚æœå¯ç”¨æ‰¹å¤„ç†æ¨¡å¼ï¼Œä½¿ç”¨æ–°çš„åˆ†é¡µå¤„ç†å™¨
            if use_batch_processing:
                try:
                    from EXTRACT_PDF_PROJ.page_batch_processor import PageBatchProcessor
                    
                    processor = PageBatchProcessor()
                    
                    # è®¾ç½®è¾“å‡ºç›®å½•
                    if output_dir is None:
                        output_dir = self.data_dir / "batch_output" / pdf_path.stem
                    
                    print(f"Using batch processing mode for PDF: {pdf_path.name}")
                    success, message = processor.process_pdf_batch(pdf_path, output_dir, page_spec)
                    
                    if success:
                        # è®¡ç®—å¤„ç†æ—¶é—´
                        end_time = time.time()
                        processing_time = end_time - start_time
                        return True, f"æ‰¹å¤„ç†å®Œæˆ ({processing_time:.1f}s): {message}"
                    else:
                        print(f"Batch processing failed, fallback to traditional mode: {message}")
                        # ç»§ç»­ä½¿ç”¨ä¼ ç»Ÿæ¨¡å¼
                except ImportError as e:
                    print(f"Batch processing module unavailable, using traditional mode: {e}")
                except Exception as e:
                    print(f"Batch processing mode error, using traditional mode: {e}")
            
            # ä¼ ç»Ÿæ¨¡å¼å¤„ç†
            print("Using traditional mode to process PDF...")
            
            # æ£€æŸ¥MinerU CLIæ˜¯å¦å¯ç”¨
            mineru_cli = self.proj_dir / "pdf_extract_cli.py"
            if not mineru_cli.exists():
                return False, "MinerU CLI not available"
            
            # æ„å»ºMinerUå‘½ä»¤
            cmd = [
                sys.executable, 
                str(mineru_cli),
                str(pdf_path)
            ]
            
            if page_spec:
                cmd.extend(['--page', page_spec])
            
            if output_dir:
                cmd.extend(['--output', str(output_dir)])
            
            # å§‹ç»ˆä½¿ç”¨MinerU
            cmd.append('--use-mineru')
            
            if not enable_analysis:
                cmd.append('--no-image-api')
                cmd.append('--async-mode')
            
            # æ‰§è¡ŒMinerU
            result = subprocess.run(cmd, capture_output=True, text=True, check=False)
            
            # Print stderr for debugging
            if result.stderr:
                print(result.stderr, file=sys.stderr)
            
            # è®¡ç®—å¤„ç†æ—¶é—´
            end_time = time.time()
            processing_time = end_time - start_time
            
            if result.returncode == 0:
                # æ£€æŸ¥æ˜¯å¦æœ‰è¾“å‡ºæ–‡ä»¶è¢«åˆ›å»ºï¼Œå¹¶å¤åˆ¶åˆ°ç”¨æˆ·æŒ‡å®šçš„ç›®å½•
                output_file = self._handle_mineru_output(pdf_path, output_dir, result.stdout, page_spec)
                print(f"Total processing time: {processing_time:.2f} seconds")
                return True, f"MinerU extraction completed: {output_file}"
            else:
                print(f"Total processing time: {processing_time:.2f} seconds")
                return False, f"MinerU extraction failed: {result.stderr}"
                
        except Exception as e:
            return False, f"MinerU extraction error: {str(e)}"
    
    def _parse_page_spec(self, page_spec: str, total_pages: int) -> List[int]:
        """è§£æé¡µé¢è§„æ ¼"""
        pages = []
        
        for part in page_spec.split(','):
            part = part.strip()
            if '-' in part:
                start, end = part.split('-', 1)
                start = int(start.strip()) - 1  # è½¬æ¢ä¸º0-based
                end = int(end.strip()) - 1
                pages.extend(range(max(0, start), min(total_pages, end + 1)))
            else:
                page = int(part.strip()) - 1  # è½¬æ¢ä¸º0-based
                if 0 <= page < total_pages:
                    pages.append(page)
        
        return sorted(list(set(pages)))
    
    def _handle_mineru_output(self, pdf_path: Path, output_dir: Path, stdout: str, page_spec: str = None) -> str:
        """å¤„ç†MinerUè¾“å‡ºï¼Œå°†æ–‡ä»¶å¤åˆ¶åˆ°ç”¨æˆ·æŒ‡å®šçš„ç›®å½•å¹¶ä¿®æ­£å›¾ç‰‡è·¯å¾„"""
        try:
            # ç¡®å®šè¾“å‡ºç›®å½•
            if output_dir is None:
                output_dir = pdf_path.parent
            else:
                output_dir.mkdir(parents=True, exist_ok=True)
            
            # æŸ¥æ‰¾MinerUç”Ÿæˆçš„markdownæ–‡ä»¶
            mineru_data_dir = get_pdf_extractor_data_dir() / "markdown"
            if mineru_data_dir.exists():
                # æ‰¾åˆ°æœ€æ–°çš„markdownæ–‡ä»¶
                md_files = list(mineru_data_dir.glob("*.md"))
                if md_files:
                    # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œå–æœ€æ–°çš„
                    latest_md = max(md_files, key=lambda f: f.stat().st_mtime)
                    
                    # è¯»å–markdownå†…å®¹å¹¶ä¿®æ­£å›¾ç‰‡è·¯å¾„
                    with open(latest_md, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    # ä¿®æ­£å›¾ç‰‡è·¯å¾„ï¼šä»ç›¸å¯¹è·¯å¾„æ”¹ä¸ºç»å¯¹è·¯å¾„
                    images_dir = get_pdf_extractor_data_dir() / "images"
                    content = self._fix_image_paths(content, images_dir)
                    
                    # æ„å»ºç›®æ ‡æ–‡ä»¶åï¼ŒåŒ…å«é¡µç ä¿¡æ¯
                    base_name = pdf_path.stem
                    if page_spec:
                        # æ ¼å¼åŒ–é¡µç ä¿¡æ¯ï¼šä¾‹å¦‚ "1,3,5" -> "_p1,3,5"ï¼Œ"1-5" -> "_p1-5"
                        page_suffix = f"_p{page_spec}"
                        target_filename = f"{base_name}{page_suffix}.md"
                    else:
                        target_filename = f"{base_name}.md"
                    
                    target_file = output_dir / target_filename
                    with open(target_file, 'w', encoding='utf-8') as f:
                        f.write(content)
                    
                    return str(target_file)
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ–‡ä»¶ï¼Œè¿”å›åŸå§‹è¾“å‡º
            return stdout.strip()
            
        except Exception as e:
            return f"Output handling failed: {str(e)}"
    
    def _fix_image_paths(self, content: str, images_dir: Path) -> str:
        """ä¿®æ­£markdownå†…å®¹ä¸­çš„å›¾ç‰‡è·¯å¾„"""
        import re
        
        # åŒ¹é…å›¾ç‰‡å¼•ç”¨ï¼š![alt](images/filename.jpg)
        pattern = r'!\[([^\]]*)\]\(images/([^)]+)\)'
        
        def replace_path(match):
            alt_text = match.group(1)
            filename = match.group(2)
            # ä½¿ç”¨ç»å¯¹è·¯å¾„
            absolute_path = images_dir / filename
            return f'![{alt_text}]({absolute_path})'
        
        return re.sub(pattern, replace_path, content)
    
    def clean_data(self) -> Tuple[bool, str]:
        """æ¸…ç†EXTRACT_PDF_PROJä¸­çš„ç¼“å­˜æ•°æ®"""
        try:
            data_dir = get_pdf_extractor_data_dir()
            
            if not data_dir.exists():
                return True, "No cached data found"
            
            # ç»Ÿè®¡è¦åˆ é™¤çš„æ–‡ä»¶
            markdown_dir = data_dir / "markdown"
            images_dir = data_dir / "images"
            
            md_count = len(list(markdown_dir.glob("*.md"))) if markdown_dir.exists() else 0
            img_count = len(list(images_dir.glob("*"))) if images_dir.exists() else 0
            
            # åˆ é™¤markdownæ–‡ä»¶
            if markdown_dir.exists():
                for md_file in markdown_dir.glob("*.md"):
                    md_file.unlink()
                print(f"Deleted {md_count} markdown files")
            
            # åˆ é™¤å›¾ç‰‡æ–‡ä»¶
            if images_dir.exists():
                for img_file in images_dir.glob("*"):
                    if img_file.is_file():
                        img_file.unlink()
                print(f"Deleted {img_count} image files")
            
            # åˆ é™¤å…¶ä»–ç¼“å­˜æ–‡ä»¶
            cache_files = [
                data_dir / "images_analysis_cache.json"
            ]
            
            cache_count = 0
            for cache_file in cache_files:
                if cache_file.exists():
                    cache_file.unlink()
                    cache_count += 1
            
            if cache_count > 0:
                print(f"Deleted {cache_count} cache files")
            
            total_deleted = md_count + img_count + cache_count
            if total_deleted > 0:
                return True, f"Successfully cleaned {total_deleted} cached files"
            else:
                return True, "No files to clean"
                
        except Exception as e:
            return False, f"Failed to clean data: {str(e)}"
    
    def extract_pdf(self, pdf_path: str, page_spec: str = None, output_dir: str = None, 
                   engine_mode: str = "mineru", use_batch_processing: bool = True) -> Tuple[bool, str]:
        """æ‰§è¡ŒPDFæå–"""
        pdf_path = Path(pdf_path).expanduser().resolve()
        
        # æ˜¾ç¤ºå¤„ç†ä¿¡æ¯
        engine_descriptions = {
            "basic": "Basic extractor (no image/formula/table processing)",
            "basic-asyn": "Basic extractor asynchronous mode (disable analysis)",
            "mineru": "MinerU extractor (batch processing enabled)" if use_batch_processing else "MinerU extractor (traditional mode)",
            "mineru-asyn": "MinerU extractor asynchronous mode (disable analysis)",
            "full": "Full processing pipeline (includes image/formula/table processing)"
        }
        
        if engine_mode in engine_descriptions:
            print(f"Using engine: {engine_descriptions[engine_mode]}")
        
        if not pdf_path.exists():
            return False, f"PDF file not found: {pdf_path}"
        
        output_dir_path = Path(output_dir) if output_dir else None
        
        # æ ¹æ®å¼•æ“æ¨¡å¼é€‰æ‹©å¤„ç†æ–¹å¼
        if engine_mode == "basic":
            return self.extract_pdf_basic_with_images(pdf_path, page_spec, output_dir_path)
        elif engine_mode == "basic-asyn":
            return self.extract_pdf_basic(pdf_path, page_spec, output_dir_path)
        elif engine_mode == "mineru":
            return self.extract_pdf_mineru(pdf_path, page_spec, output_dir_path, enable_analysis=False, use_batch_processing=use_batch_processing)
        elif engine_mode == "mineru-asyn":
            return self.extract_pdf_mineru(pdf_path, page_spec, output_dir_path, enable_analysis=False, use_batch_processing=use_batch_processing)
        elif engine_mode == "full":
            return self.extract_pdf_mineru(pdf_path, page_spec, output_dir_path, enable_analysis=True, use_batch_processing=use_batch_processing)
        else:
            return False, f"Unknown engine mode: {engine_mode}"
    
    def extract_pdf_basic_with_images(self, pdf_path: Path, page_spec: str = None, output_dir: Path = None) -> Tuple[bool, str]:
        """åŸºç¡€PDFæå–åŠŸèƒ½ï¼ŒåŒ…å«å›¾ç‰‡æå–å’Œplaceholderç”Ÿæˆ - ä½¿ç”¨ç»Ÿä¸€æ•°æ®å­˜å‚¨"""
        import time
        import hashlib
        from PIL import Image
        
        start_time = time.time()
        
        try:
            # ä½¿ç”¨Pythonçš„åŸºç¡€PDFå¤„ç†åº“
            import fitz  # PyMuPDF
            
            # æ‰“å¼€PDFæ–‡ä»¶
            doc = fitz.open(str(pdf_path))
            
            # ç¡®å®šè¦å¤„ç†çš„é¡µé¢
            if page_spec:
                pages = self._parse_page_spec(page_spec, doc.page_count)
            else:
                pages = list(range(doc.page_count))
            
            content = []
            images_data = []
            
            # ç»“æŸæ€§æ ‡ç‚¹ç¬¦å·åˆ—è¡¨
            ending_punctuations = {'ã€‚', '.', '!', '?', 'ï¼', 'ï¼Ÿ', ':', 'ï¼š', ';', 'ï¼›'}
            
            for page_num in pages:
                page = doc[page_num]
                text = page.get_text()
                
                # æå–é¡µé¢ä¸­çš„å›¾ç‰‡
                image_list = page.get_images(full=True)
                page_content = f"# Page {page_num + 1}\n\n"
                
                # å›¾ç‰‡åˆå¹¶å¤„ç†ï¼šå°†ä¸´è¿‘çš„å›¾ç‰‡åˆå¹¶æˆä¸€å¼ å¤§å›¾
                if image_list:
                    merged_images_info = self._merge_nearby_images_to_data(doc, page, image_list, page_num + 1)
                    
                    # æ”¶é›†å›¾ç‰‡æ•°æ®
                    images_data.extend(merged_images_info)
                    
                    # ä¸ºæ¯ä¸ªåˆå¹¶åçš„å›¾ç‰‡æ·»åŠ placeholder
                    for img_info in merged_images_info:
                        page_content += f"[placeholder: image]\n"
                        page_content += f"![](images/{img_info['filename']})\n\n"
                
                # å¤„ç†æ­£æ–‡æ¢è¡Œç¬¦
                processed_text = self._process_text_linebreaks(text, ending_punctuations)
                
                # æ·»åŠ é¡µé¢æ–‡æœ¬
                page_content += f"{processed_text}\n\n"
                content.append(page_content)
            
            doc.close()
            
            # åˆå¹¶æ‰€æœ‰å†…å®¹
            full_content = '\n'.join(content)
            
            # ä½¿ç”¨ç»Ÿä¸€æ•°æ®å­˜å‚¨æ¥å£ä¿å­˜æ•°æ®
            data_md_path, pdf_md_path = save_to_unified_data_directory(
                full_content, pdf_path, page_spec, images_data, output_dir
            )
            
            # åˆ›å»ºpostprocessçŠ¶æ€æ–‡ä»¶
            if images_data:
                create_postprocess_status_file(pdf_path, page_spec, images_data)
            
            # è®¡ç®—å¤„ç†æ—¶é—´
            end_time = time.time()
            processing_time = end_time - start_time
            
            print(f"Total processing time: {processing_time:.2f} seconds")
            print(f"Data saved to: {data_md_path}")
            if images_data:
                print(f"Extracted and merged {len(images_data)} images")
            
            return True, f"Basic extraction with images completed: {pdf_md_path}"
            
        except Exception as e:
            return False, f"Basic extraction with images failed: {str(e)}"
    
    def _merge_nearby_images_to_data(self, doc, page, image_list, page_num):
        """é€šè¿‡PDFæˆªå±åˆå¹¶ä¸´è¿‘çš„å›¾ç‰‡ï¼Œè¿”å›å›¾ç‰‡æ•°æ®"""
        import hashlib
        import fitz
        
        images_data = []
        
        if not image_list:
            return images_data
        
        try:
            # è·å–æ‰€æœ‰å›¾ç‰‡çš„bboxä¿¡æ¯
            image_rects = []
            for img_index, img in enumerate(image_list):
                try:
                    xref = img[0]
                    # è·å–å›¾ç‰‡åœ¨é¡µé¢ä¸­çš„çŸ©å½¢åŒºåŸŸ
                    img_rects = page.get_image_rects(xref)
                    if img_rects:
                        for rect in img_rects:
                            image_rects.append({
                                'index': img_index,
                                'xref': xref,
                                'bbox': rect,
                                'page': page_num
                            })
                except Exception as e:
                    print(f"Warning: Error getting image {img_index} position: {e}")
                    continue
            
            if not image_rects:
                return images_data
            
            # å¦‚æœåªæœ‰ä¸€å¼ å›¾ç‰‡ï¼Œç›´æ¥æˆªå±
            if len(image_rects) == 1:
                rect_info = image_rects[0]
                bbox = rect_info['bbox']
                
                # å¯¹å•å¼ å›¾ç‰‡åŒºåŸŸè¿›è¡Œæˆªå±
                pix = page.get_pixmap(clip=bbox, dpi=200)
                img_bytes = pix.tobytes("png")
                img_hash = hashlib.md5(img_bytes).hexdigest()
                img_filename = f"{img_hash}.png"
                
                images_data.append({
                    'bytes': img_bytes,
                    'hash': img_hash,
                    'filename': img_filename,
                    'bbox': list(bbox),
                    'page': page_num
                })
                
                pix = None
                print(f"Screenshot saved single image: {img_filename}")
                
            else:
                # å¤šå¼ å›¾ç‰‡ï¼šè®¡ç®—åˆå¹¶åçš„bboxå¹¶æˆªå±
                # æ‰¾åˆ°æ‰€æœ‰å›¾ç‰‡çš„è¾¹ç•Œ
                min_x0 = min(rect['bbox'].x0 for rect in image_rects)
                min_y0 = min(rect['bbox'].y0 for rect in image_rects)
                max_x1 = max(rect['bbox'].x1 for rect in image_rects)
                max_y1 = max(rect['bbox'].y1 for rect in image_rects)
                
                # åˆ›å»ºåˆå¹¶åçš„bbox
                merged_bbox = fitz.Rect(min_x0, min_y0, max_x1, max_y1)
                
                # å¯¹åˆå¹¶åŒºåŸŸè¿›è¡Œæˆªå±
                pix = page.get_pixmap(clip=merged_bbox, dpi=200)
                img_bytes = pix.tobytes("png")
                img_hash = hashlib.md5(img_bytes).hexdigest()
                img_filename = f"{img_hash}.png"
                
                images_data.append({
                    'bytes': img_bytes,
                    'hash': img_hash,
                    'filename': img_filename,
                    'bbox': list(merged_bbox),
                    'page': page_num
                })
                
                pix = None
                print(f"Screenshot merged {len(image_rects)} images into one large image: {img_filename}")
                
        except Exception as e:
            print(f"PDF screenshot process error: {e}")
            # å¦‚æœæˆªå±å¤±è´¥ï¼Œå›é€€åˆ°ä¼ ç»Ÿæ–¹å¼å¤„ç†
            for img_index, img in enumerate(image_list):
                try:
                    xref = img[0]
                    pix = fitz.Pixmap(doc, xref)
                    
                    # è·³è¿‡CMYKå›¾ç‰‡
                    if pix.n - pix.alpha >= 4:
                        pix = None
                        continue
                    
                    # è½¬æ¢ä¸ºRGB
                    if pix.n - pix.alpha == 1:  # ç°åº¦å›¾
                        pix = fitz.Pixmap(fitz.csRGB, pix)
                    
                    img_data = pix.tobytes("png")
                    img_hash = hashlib.md5(img_data).hexdigest()
                    img_filename = f"{img_hash}.png"
                    
                    # è·å–å›¾ç‰‡ä½ç½®ä¿¡æ¯
                    try:
                        img_rects = page.get_image_rects(xref)
                        bbox = list(img_rects[0]) if img_rects else []
                    except:
                        bbox = []
                    
                    images_data.append({
                        'bytes': img_data,
                        'hash': img_hash,
                        'filename': img_filename,
                        'bbox': bbox,
                        'page': page_num
                    })
                    
                    pix = None
                    
                except Exception as e:
                    print(f"Failed to process image {img_index}: {e}")
        
        return images_data
    
    def _process_text_linebreaks(self, text, ending_punctuations):
        """å¤„ç†æ­£æ–‡æ¢è¡Œç¬¦ï¼Œæ™ºèƒ½åˆå¹¶å¥å­å’Œåˆ†æ®µ"""
        if not text.strip():
            return text
        
        lines = text.split('\n')
        processed_lines = []
        current_paragraph = []
        
        for line in lines:
            line = line.strip()
            
            # è·³è¿‡ç©ºè¡Œ
            if not line:
                if current_paragraph:
                    # å¦‚æœå½“å‰æ®µè½æœ‰å†…å®¹ï¼Œç»“æŸå½“å‰æ®µè½
                    paragraph_text = ' '.join(current_paragraph)
                    processed_lines.append(paragraph_text)
                    current_paragraph = []
                    processed_lines.append('')  # æ·»åŠ ç©ºè¡Œè¡¨ç¤ºæ®µè½åˆ†éš”
                continue
            
            # å°†å½“å‰è¡Œæ·»åŠ åˆ°æ®µè½ä¸­
            current_paragraph.append(line)
            
            # æ£€æŸ¥è¡Œæ˜¯å¦ä»¥ç»“æŸæ€§æ ‡ç‚¹ç¬¦å·ç»“å°¾
            if line and line[-1] in ending_punctuations:
                # ç»“æŸå½“å‰æ®µè½
                paragraph_text = ' '.join(current_paragraph)
                processed_lines.append(paragraph_text)
                current_paragraph = []
                processed_lines.append('')  # æ·»åŠ ç©ºè¡Œè¡¨ç¤ºæ®µè½åˆ†éš”
        
        # å¤„ç†æœ€åä¸€ä¸ªæ®µè½
        if current_paragraph:
            paragraph_text = ' '.join(current_paragraph)
            processed_lines.append(paragraph_text)
        
        # æ¸…ç†å¤šä½™çš„ç©ºè¡Œ
        result = []
        prev_empty = False
        for line in processed_lines:
            if line == '':
                if not prev_empty:
                    result.append(line)
                prev_empty = True
            else:
                result.append(line)
                prev_empty = False
        
        return '\n'.join(result)



class PDFPostProcessor:
    """PDFåå¤„ç†å™¨ï¼Œç”¨äºå¤„ç†å›¾ç‰‡ã€å…¬å¼ã€è¡¨æ ¼çš„æ ‡ç­¾æ›¿æ¢"""
    
    def __init__(self, debug: bool = False):
        self.debug = debug
        self.script_dir = Path(__file__).parent
        
        # Use UNIMERNET tool for formula/table recognition instead of MinerU
        self.unimernet_tool = self.script_dir / "UNIMERNET"
        

    
    def process_file_unified(self, file_path: str, process_type: str, specific_ids: str = None, custom_prompt: str = None, force: bool = False, timeout_multi: float = 1.0) -> bool:
        """
        ç»Ÿä¸€çš„åå¤„ç†æ¥å£ - ä¸ä¾èµ–äºæå–æ¨¡å¼
        
        Args:
            file_path: PDFæ–‡ä»¶è·¯å¾„æˆ–markdownæ–‡ä»¶è·¯å¾„
            process_type: å¤„ç†ç±»å‹ ('image', 'formula', 'table', 'all')
            specific_ids: ç‰¹å®šIDåˆ—è¡¨æˆ–å…³é”®è¯
            custom_prompt: è‡ªå®šä¹‰æç¤ºè¯
            force: æ˜¯å¦å¼ºåˆ¶é‡æ–°å¤„ç†
            
        Returns:
            æ˜¯å¦å¤„ç†æˆåŠŸ
        """
        file_path = Path(file_path)
        
        # ç¡®å®šPDFæ–‡ä»¶å’Œmarkdownæ–‡ä»¶è·¯å¾„
        if file_path.suffix == '.pdf':
            pdf_file_path = file_path
            md_file = file_path.parent / f"{file_path.stem}.md"
        elif file_path.suffix == '.md':
            md_file = file_path
            # å°è¯•æ‰¾åˆ°å¯¹åº”çš„PDFæ–‡ä»¶
            pdf_file_path = file_path.parent / f"{file_path.stem}.pdf"
        else:
            print(f"Error: Unsupported file type: {file_path.suffix}")
            return False
            
        if not md_file.exists():
            print(f"Error: Markdown file not found: {md_file}")
            return False
            
        print(f"Starting unified post-processing {md_file.name}...")
        
        try:
            # ç¬¬ä¸€æ­¥ï¼šç¡®ä¿æœ‰postprocessçŠ¶æ€æ–‡ä»¶
            status_file = self._ensure_postprocess_status_file(pdf_file_path, md_file)
            if not status_file:
                print("Error: Failed to create or find status file")
                return False
            
            # ç¬¬äºŒæ­¥ï¼šè¯»å–çŠ¶æ€æ–‡ä»¶
            with open(status_file, 'r', encoding='utf-8') as f:
                status_data = json.load(f)
            
            # ç¬¬ä¸‰æ­¥ï¼šåŒæ­¥markdownå’ŒJSONä¸­çš„placeholderä¿¡æ¯
            print("Syncing markdown and JSON placeholder information...")
            status_data = self._sync_placeholders_with_markdown(md_file, status_data, status_file)
            
            # ç¬¬å››æ­¥ï¼šç­›é€‰è¦å¤„ç†çš„é¡¹ç›®
            items_to_process = self._filter_items_to_process(status_data, process_type, specific_ids, force)
            
            if not items_to_process:
                print("No items to process")
                return True
            
            # ç¬¬äº”æ­¥ï¼šä½¿ç”¨ç»Ÿä¸€çš„æ··åˆå¤„ç†æ–¹å¼
            success = self._process_items_unified(str(pdf_file_path), str(md_file), status_data, 
                                                items_to_process, process_type, custom_prompt, force, timeout_multi)
            
            return success
            
        except Exception as e:
            print(f"Error: Unified post-processing error: {e}")
            return False
    
    def _ensure_postprocess_status_file(self, pdf_file_path: Path, md_file: Path) -> Optional[Path]:
        """ç¡®ä¿å­˜åœ¨postprocessçŠ¶æ€æ–‡ä»¶ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»º"""
        status_file = pdf_file_path.parent / f"{pdf_file_path.stem}_postprocess.json"
        
        if status_file.exists():
            return status_file
        
        print("ğŸ“„ Status file not found, regenerating from markdown...")
        
        # ä»markdownæ–‡ä»¶åˆ†æplaceholderï¼Œåˆ›å»ºçŠ¶æ€æ–‡ä»¶
        try:
            with open(md_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # æŸ¥æ‰¾æ‰€æœ‰placeholderå’Œå›¾ç‰‡å¼•ç”¨
            import re
            # ä¿®å¤æ­£åˆ™è¡¨è¾¾å¼ä»¥åŒ¹é…åŒ…å«åˆ†æç»“æœçš„å®Œæ•´placeholderå—
            # è¿™ä¸ªæ¨¡å¼åŒ¹é…ï¼š[placeholder: type]\n![...](path) åé¢å¯èƒ½è·Ÿç€åˆ†æç»“æœ
            placeholder_pattern = r'\[placeholder:\s*(\w+)\]\s*\n!\[[^\]]*\]\(([^)]+)\)(?:\s*\n\n\*\*[^*]+\*\*.*?)?'
            matches = re.findall(placeholder_pattern, content, re.DOTALL)
            
            if not matches:
                print("No placeholder found, no post-processing needed")
                return None
            
            # åˆ›å»ºçŠ¶æ€æ•°æ®
            from datetime import datetime
            status_data = {
                "pdf_file": str(pdf_file_path),
                "created_at": datetime.now().isoformat(),
                "page_range": None,
                "total_items": len(matches),
                "processed_items": 0,
                "items": []
            }
            
            # æ·»åŠ é¡¹ç›®
            for item_type, image_path in matches:
                # ä»å›¾ç‰‡è·¯å¾„æå–hash ID
                image_filename = Path(image_path).name
                hash_id = Path(image_path).stem
                
                item = {
                    "id": hash_id,
                    "type": item_type,
                    "filename": image_filename,
                    "image_path": image_path,
                    "processor": "unified_processor",
                    "processed": False,
                    "placeholder": f"[placeholder: {item_type}]",
                    "bbox": [],
                    "page": 1  # é»˜è®¤é¡µç 
                }
                status_data["items"].append(item)
            
            # ä¿å­˜çŠ¶æ€æ–‡ä»¶
            with open(status_file, 'w', encoding='utf-8') as f:
                json.dump(status_data, f, ensure_ascii=False, indent=2)
            
            print(f"Created status file: {status_file.name}")
            return status_file
            
        except Exception as e:
            print(f"Error: Failed to create status file: {e}")
            return None
    
    def _filter_items_to_process(self, status_data: dict, process_type: str, specific_ids: str, force: bool) -> list:
        """ç­›é€‰éœ€è¦å¤„ç†çš„é¡¹ç›®"""
        items_to_process = []
        
        for item in status_data.get('items', []):
            # è·³è¿‡å·²å¤„ç†çš„é¡¹ç›®ï¼ˆé™¤éå¼ºåˆ¶é‡æ–°å¤„ç†ï¼‰
            if item.get('processed', False) and not force:
                continue
            
            item_type = item.get('type')
            item_id = item.get('id')
            
            # æ ¹æ®å¤„ç†ç±»å‹ç­›é€‰
            if process_type != 'all':
                if process_type == 'image' and item_type != 'image':
                    continue
                elif process_type == 'formula' and item_type not in ['formula', 'interline_equation']:
                    continue
                elif process_type == 'table' and item_type != 'table':
                    continue
            
            # æ ¹æ®specific_idsç­›é€‰
            if specific_ids:
                if specific_ids in ['all_images', 'all_formulas', 'all_tables', 'all']:
                    if specific_ids == 'all':
                        pass  # å¤„ç†æ‰€æœ‰ç±»å‹
                    elif specific_ids == 'all_images' and item_type != 'image':
                        continue
                    elif specific_ids == 'all_formulas' and item_type not in ['formula', 'interline_equation']:
                        continue
                    elif specific_ids == 'all_tables' and item_type != 'table':
                        continue
                else:
                    # å…·ä½“çš„hash IDåˆ—è¡¨
                    target_ids = [id.strip() for id in specific_ids.split(',')]
                    if item_id not in target_ids:
                        continue
            
            items_to_process.append(item_id)
        
        return items_to_process
    
    def _process_items_unified(self, pdf_file: str, md_file: str, status_data: dict, 
                             items_to_process: list, process_type: str, custom_prompt: str = None, force: bool = False, timeout_multi: float = 1.0) -> bool:
        """ç»Ÿä¸€çš„é¡¹ç›®å¤„ç†æ–¹æ³•"""
        try:
            # è¯»å–markdownæ–‡ä»¶
            with open(md_file, 'r', encoding='utf-8') as f:
                md_content = f.read()
            
            # å¤„ç†æ¯ä¸ªé¡¹ç›®
            updated = False
            for item_id in items_to_process:
                # æ‰¾åˆ°å¯¹åº”çš„é¡¹ç›®
                item = None
                for status_item in status_data.get('items', []):
                    if status_item.get('id') == item_id:
                        item = status_item
                        break
                
                if not item:
                    print(f"Warning: Item not found: {item_id}")
                    continue
                
                item_type = item.get('type')
                image_path = item.get('image_path', '')
                
                if not image_path:
                    print(f"Warning: Image path is empty: {item_id}")
                    continue
                
                # æŸ¥æ‰¾å®é™…çš„å›¾ç‰‡æ–‡ä»¶è·¯å¾„
                actual_image_path = self._find_actual_image_path(pdf_file, image_path)
                if not actual_image_path:
                    print(f"Warning: Image file not found: {image_path}")
                    continue
                
                print(f"\nProcessing {item_type} item: {item_id}")
                
                # æ ¹æ®ç±»å‹é€‰æ‹©å¤„ç†æ–¹å¼
                result_text = ""
                if item_type == 'image':
                    result_text = self._process_image_with_api(actual_image_path, custom_prompt, timeout_multi)
                elif item_type in ['formula', 'interline_equation']:
                    result_text = self._process_with_unimernet(actual_image_path, "formula", force, timeout_multi)
                elif item_type == 'table':
                    result_text = self._process_with_unimernet(actual_image_path, "table", force, timeout_multi)
                
                if result_text:
                    # æ›´æ–°markdownå†…å®¹
                    success = self._update_markdown_with_result(md_content, item, result_text)
                    if success:
                        md_content = success
                        item['processed'] = True
                        updated = True
                        print(f"Completed {item_type} processing: {item_id}")
                    else:
                        print(f"Warning: Failed to update markdown: {item_id}")
                else:
                    print(f"Error: Processing failed: {item_id}")
            
            if updated:
                # ä¿å­˜æ›´æ–°çš„markdownæ–‡ä»¶
                with open(md_file, 'w', encoding='utf-8') as f:
                    f.write(md_content)
                
                # æ›´æ–°çŠ¶æ€æ–‡ä»¶
                status_file = Path(pdf_file).parent / f"{Path(pdf_file).stem}_postprocess.json"
                with open(status_file, 'w', encoding='utf-8') as f:
                    json.dump(status_data, f, indent=2, ensure_ascii=False)
                
                print(f"Updated file: {Path(md_file).name}")
                return True
            else:
                print("No content to update")
                return True
                
        except Exception as e:
            print(f"Error: Unified processing error: {e}")
            return False
    def _find_actual_image_path(self, pdf_file: str, image_filename: str) -> Optional[str]:
        """æŸ¥æ‰¾å›¾ç‰‡æ–‡ä»¶çš„å®é™…è·¯å¾„"""
        pdf_path = Path(pdf_file)
        pdf_directory = pdf_path.parent
        
        # æ£€æŸ¥å¯èƒ½çš„å›¾ç‰‡ä½ç½®
        possible_locations = [
            Path(image_filename),  # ç»å¯¹è·¯å¾„
            pdf_directory / image_filename,  # ç›¸å¯¹äºPDFçš„è·¯å¾„
            pdf_directory / "images" / Path(image_filename).name,  # PDFç›®å½•ä¸‹çš„imagesæ–‡ä»¶å¤¹
            get_pdf_extractor_data_dir() / "images" / Path(image_filename).name,  # ç»Ÿä¸€æ•°æ®ç›®å½•
        ]
        
        for location in possible_locations:
            if location.exists():
                return str(location)
        
        return None
    
    def _process_image_with_api(self, image_path: str, custom_prompt: str = None, timeout_multi: float = 1.0) -> str:
        """ä½¿ç”¨IMG2TEXT APIå¤„ç†å›¾ç‰‡"""
        try:
            # è°ƒç”¨IMG2TEXTå·¥å…·
            img2text_path = self.script_dir / "IMG2TEXT"
            if not img2text_path.exists():
                return "IMG2TEXT tool not available"
            
            cmd = [str(img2text_path), image_path, "--json"]
            if custom_prompt:
                cmd.extend(["--prompt", custom_prompt])
            
            # è®¡ç®—è¶…æ—¶æ—¶é—´ (IMG2TEXTé»˜è®¤æ²¡æœ‰è¶…æ—¶ï¼Œæˆ‘ä»¬è®¾ç½®2åˆ†é’Ÿ * timeout_multi)
            timeout = int(120 * timeout_multi)
            result = subprocess.run(cmd, capture_output=True, text=True, check=False, timeout=timeout)
            
            if result.returncode == 0:
                try:
                    # å°è¯•è§£æJSONè¾“å‡º
                    output_data = json.loads(result.stdout)
                    if output_data.get('success'):
                        description = output_data.get('result', 'Image analysis completed')
                        return description
                    else:
                        error_msg = output_data.get('error', 'Unknown error')
                        return f"Image analysis failed: {error_msg}"
                except json.JSONDecodeError:
                    # å¦‚æœä¸æ˜¯JSONæ ¼å¼ï¼Œç›´æ¥ä½¿ç”¨è¾“å‡º
                    return result.stdout.strip() if result.stdout.strip() else "Image analysis completed"
            else:
                return f"IMG2TEXT execution failed: {result.stderr}"
                
        except subprocess.TimeoutExpired:
            return f"IMG2TEXT processing timeout (timeout: {int(120 * timeout_multi)} seconds)"
        except Exception as e:
            return f"Image processing error: {e}"
    
    def _sync_placeholders_with_markdown(self, md_file: Path, status_data: dict, status_file: Path) -> dict:
        """åŒæ­¥markdownå’ŒJSONæ–‡ä»¶ä¸­çš„placeholderä¿¡æ¯"""
        try:
            # è¯»å–markdownå†…å®¹
            with open(md_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # æŸ¥æ‰¾æ‰€æœ‰placeholderå’Œå›¾ç‰‡å¼•ç”¨
            import re
            # ä¿®å¤æ­£åˆ™è¡¨è¾¾å¼ä»¥åŒ¹é…åŒ…å«åˆ†æç»“æœçš„å®Œæ•´placeholderå—
            # è¿™ä¸ªæ¨¡å¼åŒ¹é…ï¼š[placeholder: type]\n![...](path) åé¢å¯èƒ½è·Ÿç€åˆ†æç»“æœ
            placeholder_pattern = r'\[placeholder:\s*(\w+)\]\s*\n!\[[^\]]*\]\(([^)]+)\)(?:\s*\n\n\*\*[^*]+\*\*.*?)?'
            matches = re.findall(placeholder_pattern, content, re.DOTALL)
            
            # æ›´æ–°çŠ¶æ€æ•°æ®ä¸­çš„é¡¹ç›®
            existing_items = {item.get('id'): item for item in status_data.get('items', [])}
            updated_items = []
            
            for item_type, image_path in matches:
                # ä»å›¾ç‰‡è·¯å¾„æå–hash ID
                image_filename = Path(image_path).name
                hash_id = Path(image_path).stem
                
                # å¦‚æœé¡¹ç›®å·²å­˜åœ¨ï¼Œä¿æŒå…¶processedçŠ¶æ€
                if hash_id in existing_items:
                    existing_item = existing_items[hash_id]
                    existing_item.update({
                        "type": item_type,
                        "filename": image_filename,
                        "image_path": image_path,
                        "placeholder": f"[placeholder: {item_type}]"
                    })
                    updated_items.append(existing_item)
                else:
                    # æ–°é¡¹ç›®
                    item = {
                        "id": hash_id,
                        "type": item_type,
                        "filename": image_filename,
                        "image_path": image_path,
                        "processor": "unified_processor",
                        "processed": False,
                        "placeholder": f"[placeholder: {item_type}]",
                        "bbox": [],
                        "page": 1
                    }
                    updated_items.append(item)
            
            # æ›´æ–°çŠ¶æ€æ•°æ®
            status_data["items"] = updated_items
            status_data["total_items"] = len(updated_items)
            status_data["processed_items"] = sum(1 for item in updated_items if item.get('processed', False))
            
            # ä¿å­˜æ›´æ–°çš„çŠ¶æ€æ–‡ä»¶
            with open(status_file, 'w', encoding='utf-8') as f:
                json.dump(status_data, f, ensure_ascii=False, indent=2)
            
            return status_data
            
        except Exception as e:
            print(f"Error: Failed to sync placeholder information: {e}")
            return status_data
    
    def _update_markdown_with_result(self, md_content: str, item: dict, result_text: str) -> Optional[str]:
        """æ›´æ–°markdownå†…å®¹ï¼Œä¿ç•™placeholderï¼Œç²¾ç¡®æ›¿æ¢åˆ†æç»“æœï¼Œé¿å…è¯¯åˆ æ­£æ–‡"""
        import re
        
        item_type = item.get('type')
        image_path = item.get('image_path', '')
        image_filename = Path(image_path).name
        escaped_filename = re.escape(image_filename)
        escaped_type = re.escape(item_type)
        
        # ä½¿ç”¨åˆ†æ­¥æ–¹æ³•ï¼šå…ˆæ‰¾åˆ°placeholderå’Œå›¾ç‰‡ï¼Œç„¶åæ£€æŸ¥åé¢æ˜¯å¦æœ‰åˆ†æç»“æœ
        # æ›´ç²¾ç¡®çš„æ¨¡å¼ï¼šå›¾ç‰‡å¼•ç”¨åº”è¯¥ä»¥.jpg/.png/.pdfç­‰ç»“å°¾ï¼Œç„¶åæ˜¯)
        placeholder_img_pattern = (
            rf'\[placeholder:\s*{escaped_type}\]\s*\n'
            rf'!\[[^\]]*\]\([^)]*{escaped_filename}\)'
        )
        
        # æŸ¥æ‰¾placeholderå’Œå›¾ç‰‡çš„ä½ç½®
        placeholder_match = re.search(placeholder_img_pattern, md_content)
        if not placeholder_match:
            print(f"Warning: No matching placeholder pattern found")
            return None
        
        placeholder_and_img = placeholder_match.group(0)
        start_pos = placeholder_match.start()
        end_pos = placeholder_match.end()
        
        # æ£€æŸ¥åé¢æ˜¯å¦æœ‰ç°æœ‰çš„åˆ†æç»“æœéœ€è¦æ›¿æ¢
        remaining_content = md_content[end_pos:]
        
        # å®šä¹‰å„ç§åˆ†æç»“æœçš„æ¨¡å¼ - æ›´ç²¾ç¡®åœ°åŒ¹é…ï¼Œé¿å…è¯¯åˆ æ­£æ–‡
        analysis_patterns = [
            r'\n\n--- å›¾åƒåˆ†æç»“æœ ---.*?\n--------------------',  # --- å›¾åƒåˆ†æç»“æœ --- å—ï¼ˆåŒ…æ‹¬åç»­å†…å®¹å’Œåˆ†éš”çº¿ï¼‰
            r'\n\n\*\*å›¾ç‰‡åˆ†æ:\*\*.*?(?=\n\n(?!\*\*)|$)',  # **å›¾ç‰‡åˆ†æ:** å—ï¼ˆå…¼å®¹æ—§æ ¼å¼ï¼‰
            r'\n\n\*\*è¡¨æ ¼å†…å®¹:\*\*.*?(?=\n\n(?!\*\*)|$)',  # **è¡¨æ ¼å†…å®¹:** å—
            r'\n\n\*\*åˆ†æç»“æœ:\*\*.*?(?=\n\n(?!\*\*)|$)',  # **åˆ†æç»“æœ:** å—
            r'\n\n\$\$\n.*?\n\$\$',  # å¤šè¡Œå…¬å¼å—
            r'\n\n\$\$[^$\n]+\$\$',  # å•è¡Œå…¬å¼å—
            r'\n\n\$\$\n\\text\{.*?\}\n\$\$',  # é”™è¯¯å…¬å¼å—ï¼ˆå¦‚è¯†åˆ«å¤±è´¥ä¿¡æ¯ï¼‰
        ]
        
        # æ‰¾åˆ°æœ€æ—©å‡ºç°çš„åˆ†æç»“æœ
        earliest_match = None
        earliest_pos = len(remaining_content)
        
        for pattern in analysis_patterns:
            match = re.search(pattern, remaining_content, re.DOTALL)
            if match and match.start() < earliest_pos:
                earliest_match = match
                earliest_pos = match.start()
        
        if earliest_match:
            # æœ‰ç°æœ‰åˆ†æç»“æœï¼Œæ›¿æ¢å®ƒ
            print(f"ğŸ” Found existing analysis result, position: {earliest_pos}, length: {earliest_match.end() - earliest_match.start()}")
            analysis_end = end_pos + earliest_match.end()
            before_analysis = md_content[:start_pos]
            after_analysis = md_content[analysis_end:]
        else:
            # æ²¡æœ‰ç°æœ‰åˆ†æç»“æœï¼Œåœ¨placeholderåç›´æ¥æ·»åŠ 
            print(f"ğŸ” No existing analysis result found, adding directly")
            before_analysis = md_content[:start_pos]
            after_analysis = md_content[end_pos:]
        
        # æ„å»ºæ–°çš„å†…å®¹
        if item_type == 'image':
            new_content = f"{placeholder_and_img}\n\n{result_text}"
        elif item_type in ['formula', 'interline_equation']:
            new_content = f"{placeholder_and_img}\n\n{result_text}"
        elif item_type == 'table':
            new_content = f"{placeholder_and_img}\n\n**è¡¨æ ¼å†…å®¹:**\n{result_text}"
        else:
            new_content = f"{placeholder_and_img}\n\n**åˆ†æç»“æœ:**\n{result_text}"
        
        # ç»„åˆæœ€ç»ˆå†…å®¹
        updated_content = before_analysis + new_content + after_analysis
        
        return updated_content
    
    def _process_with_unimernet(self, image_path: str, content_type: str = "auto", force: bool = False, timeout_multi: float = 1.0) -> str:
        """ä½¿ç”¨UNIMERNETå·¥å…·å¤„ç†å…¬å¼æˆ–è¡¨æ ¼å›¾ç‰‡"""
        try:
            # ä½¿ç”¨EXTRACT_IMGå·¥å…·ï¼ˆæ•´åˆäº†UNIMERNETå’Œcacheï¼‰
            extract_img_tool = self.script_dir / "EXTRACT_IMG"
            if not extract_img_tool.exists():
                print(f"Warning: EXTRACT_IMG tool not available: {extract_img_tool}")
                return ""
            
            # æ„å»ºEXTRACT_IMGå‘½ä»¤
            cmd = [str(extract_img_tool), image_path, "--json"]
            if content_type != "auto":
                cmd.extend(["--type", content_type])
            else:
                cmd.extend(["--type", "formula"])  # Default to formula for UNIMERNET
            
            # æ·»åŠ forceå‚æ•°
            if force:
                cmd.append("--force")
            
            # è®¡ç®—è¶…æ—¶æ—¶é—´ (EXTRACT_IMGå†…éƒ¨é»˜è®¤120ç§’ï¼Œæˆ‘ä»¬ä¹˜ä»¥timeout_multi)
            timeout = int(120 * timeout_multi)
            result = subprocess.run(cmd, capture_output=True, text=True, check=False, timeout=timeout)
            
            if result.returncode == 0:
                # è§£æEXTRACT_IMGçš„JSONè¾“å‡º
                try:
                    extract_result = json.loads(result.stdout)
                    if extract_result.get('success'):
                        recognition_result = extract_result.get('result', '')
                        if recognition_result:
                            # Check if it's from cache
                            cache_info = " (from cache)" if extract_result.get('from_cache') else ""
                            # Get processing time if available
                            processing_time = extract_result.get('processing_time', 0)
                            time_info = f" (processing time: {processing_time:.2f} seconds)" if processing_time > 0 else ""
                            print(f"EXTRACT_IMG recognition successful{cache_info}{time_info}: {len(recognition_result)} characters")
                            # Directly format as $$ without description wrapper
                            cleaned_result = recognition_result.strip()
                            return f"$$\n{cleaned_result}\n$$"
                        else:
                            print("Warning: EXTRACT_IMG returned empty result")
                            return f"$$\n\\text{{[formula recognition failed: EXTRACT_IMG returned empty result]}}\n$$"
                    else:
                        error_msg = extract_result.get('error', 'Unknown error')
                        print(f"Error: EXTRACT_IMG processing failed: {error_msg}")
                        return f"$$\n\\text{{[formula recognition failed: {error_msg}]}}\n$$"
                except json.JSONDecodeError as e:
                    error_msg = f"JSON parsing failed: {e}"
                    print(f"Error: Failed to parse EXTRACT_IMG JSON output: {e}")
                    print(f"   original output: {result.stdout[:200]}...")
                    return f"$$\n\\text{{[formula recognition failed: {error_msg}]}}\n$$"
            else:
                error_msg = f"EXTRACT_IMG execution failed: {result.stderr}"
                print(f"Error: EXTRACT_IMG execution failed: {result.stderr}")
                return f"$$\n\\text{{[formula recognition failed: {error_msg}]}}\n$$"
                
        except subprocess.TimeoutExpired:
            timeout_msg = f"EXTRACT_IMG processing timeout (timeout: {int(120 * timeout_multi)} seconds)"
            print(f"Error: {timeout_msg}")
            return f"$$\n\\text{{[formula recognition failed: {timeout_msg}]}}\n$$"
        except Exception as e:
            print(f"Error: UNIMERNET processing error: {e}")
            return f"$$\n\\text{{[formula recognition failed: UNIMERNET processing error: {e}]}}\n$$"
    

    

    
    def _select_markdown_file_interactive(self) -> str:
        """äº¤äº’å¼é€‰æ‹©markdownæ–‡ä»¶"""
        print("ğŸ” Selecting markdown file for post-processing...")
        
        # ä½¿ç”¨FILEDIALOGå·¥å…·é€‰æ‹©æ–‡ä»¶
        try:
            filedialog_path = self.script_dir / "FILEDIALOG"
            if not filedialog_path.exists():
                print("Warning: FILEDIALOG tool not available, using traditional file selection")
                return self._select_markdown_file_traditional()
            
            # è°ƒç”¨FILEDIALOGå·¥å…·é€‰æ‹©.mdæ–‡ä»¶
            cmd = [str(filedialog_path), '--types', 'md', '--title', 'Select Markdown File for Post-processing']
            result = subprocess.run(cmd, capture_output=True, text=True, check=False)
            
            if result.returncode == 0:
                # è§£æFILEDIALOGçš„è¾“å‡º
                output_text = result.stdout.strip()
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯" Selected file:"æ ¼å¼çš„è¾“å‡º
                if " Selected file:" in output_text:
                    lines = output_text.split('\n')
                    for line in lines:
                        if " Selected file:" in line:
                            selected_file = line.split(" Selected file: ", 1)[1].strip()
                            if selected_file and Path(selected_file).exists():
                                print(f" Selected: {Path(selected_file).name}")
                                return selected_file
                            break
                    print("Error: Failed to parse selected file path")
                    return None
                else:
                    # å°è¯•è§£æJSONè¾“å‡ºï¼ˆRUNç¯å¢ƒä¸‹ï¼‰
                    try:
                        output_data = json.loads(output_text)
                        if output_data.get('success') and output_data.get('selected_file'):
                            selected_file = output_data['selected_file']
                            print(f" Selected: {Path(selected_file).name}")
                            return selected_file
                        else:
                            print("Error: User cancelled file selection")
                            return None
                    except json.JSONDecodeError:
                        # å¦‚æœæ—¢ä¸æ˜¯æ ‡å‡†æ ¼å¼ä¹Ÿä¸æ˜¯JSONï¼Œç›´æ¥ä½¿ç”¨è¾“å‡º
                        if output_text and Path(output_text).exists():
                            print(f" Selected: {Path(output_text).name}")
                            return output_text
                        else:
                            print("Error: User cancelled file selection")
                            return None
            else:
                print("Error: File selection failed")
                return None
                
        except Exception as e:
            print(f"Warning: Error using FILEDIALOG: {e}")
            print("Using traditional file selection")
            return self._select_markdown_file_traditional()
    
    def _select_markdown_file_traditional(self) -> str:
        """ä¼ ç»Ÿæ–¹å¼é€‰æ‹©markdownæ–‡ä»¶ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰"""
        print("ğŸ” Searching for EXTRACT_PDF generated markdown files...")
        
        # æœç´¢å½“å‰ç›®å½•åŠå…¶å­ç›®å½•ä¸­çš„markdownæ–‡ä»¶
        md_files = []
        search_dirs = [Path.cwd(), get_pdf_extractor_data_dir()]
        
        for search_dir in search_dirs:
            if search_dir.exists():
                for md_file in search_dir.rglob("*.md"):
                    # æ£€æŸ¥æ˜¯å¦æ˜¯EXTRACT_PDFç”Ÿæˆçš„æ–‡ä»¶
                    # æ–¹æ³•1ï¼šæœ‰å¯¹åº”çš„extract_dataç›®å½•
                    extract_data_dir = md_file.parent / f"{md_file.stem}_extract_data"
                    # æ–¹æ³•2ï¼šæ–‡ä»¶åŒ…å«placeholderæ ‡è®°
                    has_placeholder = False
                    try:
                        with open(md_file, 'r', encoding='utf-8') as f:
                            content = f.read()
                        has_placeholder = '[placeholder:' in content
                    except:
                        pass
                    
                    if extract_data_dir.exists() or has_placeholder:
                        md_files.append(md_file)
        
        if not md_files:
            print("Error: No EXTRACT_PDF generated markdown files found")
            return None
        
        # æ˜¾ç¤ºæ–‡ä»¶åˆ—è¡¨
        print("\nğŸ“„ Found the following markdown files:")
        for i, md_file in enumerate(md_files, 1):
            # æ£€æŸ¥æ˜¯å¦æœ‰å¾…å¤„ç†çš„placeholder
            try:
                with open(md_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                image_count = len(re.findall(r'\[placeholder: image\]', content))
                formula_count = len(re.findall(r'\[placeholder: formula\]', content))
                table_count = len(re.findall(r'\[placeholder: table\]', content))
                total_count = image_count + formula_count + table_count
                
                status = f"({total_count} items to process: images:{image_count} formulas:{formula_count} tables:{table_count})" if total_count > 0 else "(processed)"
                print(f"  {i}. {md_file.name} {status}")
                print(f"     path: {md_file}")
                
            except Exception as e:
                print(f"  {i}. {md_file.name} (cannot read)")
                print(f"     path: {md_file}")
        
        # ç”¨æˆ·é€‰æ‹©
        while True:
            try:
                choice = input(f"\nPlease select the file to process (1-{len(md_files)}, or press Enter to cancel): ").strip()
                
                if not choice:
                    print("Error: Cancelled")
                    return None
                
                choice_num = int(choice)
                if 1 <= choice_num <= len(md_files):
                    selected_file = md_files[choice_num - 1]
                    print(f" Selected: {selected_file.name}")
                    return str(selected_file)
                else:
                    print(f"Error: Please enter a number between 1 and {len(md_files)}")
                    
            except ValueError:
                print("Error: Please enter a valid number")
            except KeyboardInterrupt:
                print("\nError: Cancelled")
                return None
        
    def process_file(self, file_path: str, process_type: str, specific_ids: str = None, custom_prompt: str = None, force: bool = False, timeout_multi: float = 1.0) -> bool:
        """
        å¤„ç†PDFæ–‡ä»¶çš„åå¤„ç† - ä½¿ç”¨ç»Ÿä¸€æ¥å£ï¼ˆä¸ä¾èµ–äºæå–æ¨¡å¼ï¼‰
        
        Args:
            file_path: PDFæ–‡ä»¶è·¯å¾„æˆ–markdownæ–‡ä»¶è·¯å¾„ï¼Œæˆ–è€…"interactive"è¿›å…¥äº¤äº’æ¨¡å¼
            process_type: å¤„ç†ç±»å‹ ('image', 'formula', 'table', 'all')
            specific_ids: ç‰¹å®šIDåˆ—è¡¨æˆ–å…³é”®è¯
            custom_prompt: è‡ªå®šä¹‰æç¤ºè¯
            force: æ˜¯å¦å¼ºåˆ¶é‡æ–°å¤„ç†
            
        Returns:
            æ˜¯å¦å¤„ç†æˆåŠŸ
        """
        # æ£€æŸ¥æ˜¯å¦è¿›å…¥äº¤äº’æ¨¡å¼
        if file_path == "interactive":
            file_path = self._select_markdown_file_interactive()
            if not file_path:
                return False
        
        # ç›´æ¥è°ƒç”¨ç»Ÿä¸€æ¥å£
        return self.process_file_unified(file_path, process_type, specific_ids, custom_prompt, force, timeout_multi)
    

    
    def _sync_placeholders_with_markdown(self, md_file: Path, status_data: dict, status_file: Path) -> dict:
        """
        åŒæ­¥markdownæ–‡ä»¶å’ŒJSONæ–‡ä»¶ä¸­çš„placeholderä¿¡æ¯
        
        Args:
            md_file: markdownæ–‡ä»¶è·¯å¾„
            status_data: JSONçŠ¶æ€æ•°æ®
            status_file: JSONçŠ¶æ€æ–‡ä»¶è·¯å¾„
            
        Returns:
            æ›´æ–°åçš„çŠ¶æ€æ•°æ®
        """
        try:
            # è¯»å–markdownæ–‡ä»¶å†…å®¹
            with open(md_file, 'r', encoding='utf-8') as f:
                md_content = f.read()
            
            # è§£æmarkdownä¸­çš„placeholderä¿¡æ¯
            md_placeholders = self._parse_placeholders_from_markdown(md_content)
            print(f"   ğŸ“‹ Found {len(md_placeholders)} placeholders in markdown")
            
            # åˆ›å»ºJSONä¸­ç°æœ‰é¡¹ç›®çš„æ˜ å°„
            json_items = {item['id']: item for item in status_data.get('items', [])}
            print(f"   ğŸ“„ {len(json_items)} items in JSON")
            
            # åŒæ­¥è¿‡ç¨‹
            updated_items = []
            md_content_modified = False
            
            # 1. å¤„ç†markdownä¸­çš„placeholderï¼Œæ›´æ–°æˆ–æ·»åŠ åˆ°JSON
            for img_id, placeholder_type in md_placeholders.items():
                if img_id in json_items:
                    # æ›´æ–°ç°æœ‰é¡¹ç›®çš„ç±»å‹
                    item = json_items[img_id]
                    old_type = item.get('type', 'unknown')
                    if old_type != placeholder_type:
                        print(f"   Updating item {img_id[:8]}... type: {old_type} â†’ {placeholder_type}")
                        item['type'] = placeholder_type
                        item['processed'] = False  # é‡ç½®å¤„ç†çŠ¶æ€
                        # æ›´æ–°å¤„ç†å™¨
                        if placeholder_type == 'image':
                            item['processor'] = 'Google API'
                        elif placeholder_type in ['formula', 'interline_equation']:
                            item['processor'] = 'UnimerNet'
                        elif placeholder_type == 'table':
                            item['processor'] = 'UnimerNet'
                    updated_items.append(item)
                    del json_items[img_id]  # ä»å¾…å¤„ç†åˆ—è¡¨ä¸­ç§»é™¤
                else:
                    # æ–°å¢é¡¹ç›®åˆ°JSON
                    print(f"   â• Adding new item {img_id[:8]}... type: {placeholder_type}")
                    new_item = {
                        "id": img_id,
                        "type": placeholder_type,
                        "page": 1,  # é»˜è®¤é¡µç 
                        "block_index": -1,  # æ ‡è®°ä¸ºç”¨æˆ·æ·»åŠ 
                        "image_path": f"{img_id}.jpg",
                        "bbox": [],
                        "processed": False,
                        "processor": self._get_processor_for_type(placeholder_type)
                    }
                    updated_items.append(new_item)
            
            # 2. å¤„ç†JSONä¸­å‰©ä½™çš„é¡¹ç›®ï¼ˆmarkdownä¸­ç¼ºå¤±çš„ï¼‰
            for img_id, item in json_items.items():
                print(f"   ğŸ”§ Restoring missing placeholder {img_id[:8]}... type: {item['type']}")
                # åœ¨markdownä¸­æ¢å¤placeholder
                md_content = self._restore_placeholder_in_markdown(md_content, img_id, item['type'])
                md_content_modified = True
                updated_items.append(item)
            
            # 3. ä¿å­˜ä¿®æ”¹åçš„markdownæ–‡ä»¶
            if md_content_modified:
                with open(md_file, 'w', encoding='utf-8') as f:
                    f.write(md_content)
                print(f"   Updated markdown file")
            
            # 4. æ›´æ–°çŠ¶æ€æ•°æ®
            status_data['items'] = updated_items
            status_data['total_items'] = len(updated_items)
            
            # é‡æ–°è®¡ç®—counts
            counts = {"images": 0, "formulas": 0, "tables": 0}
            for item in updated_items:
                if not item.get('processed', False):  # åªè®¡ç®—æœªå¤„ç†çš„é¡¹ç›®
                    item_type = item.get('type', '')
                    if item_type == 'image':
                        counts['images'] += 1
                    elif item_type in ['formula', 'interline_equation']:
                        counts['formulas'] += 1
                    elif item_type == 'table':
                        counts['tables'] += 1
            
            status_data['counts'] = counts
            
            # 5. ä¿å­˜æ›´æ–°åçš„JSONæ–‡ä»¶
            with open(status_file, 'w', encoding='utf-8') as f:
                json.dump(status_data, f, ensure_ascii=False, indent=2)
            
            print(f"    Sync completed: {len(updated_items)} items")
            return status_data
            
        except Exception as e:
            print(f"   Warning: Sync error: {e}")
            return status_data
    
    def _parse_placeholders_from_markdown(self, md_content: str) -> dict:
        """ä»markdownå†…å®¹ä¸­è§£æplaceholderä¿¡æ¯"""
        import re
        
        placeholders = {}
        
        # ä¿®å¤æ­£åˆ™è¡¨è¾¾å¼ä»¥æ­£ç¡®åŒ¹é…å®Œæ•´çš„å“ˆå¸Œæ–‡ä»¶å
        # åŒ¹é… [placeholder: type] åè·Ÿ ![](path/to/hash.ext) çš„æ¨¡å¼
        pattern = r'\[placeholder:\s*(\w+)\]\s*\n!\[[^\]]*\]\([^)]*\/([a-f0-9]{16,64})\.(jpg|jpeg|png|gif|webp)\)'
        
        matches = re.findall(pattern, md_content)
        for placeholder_type, img_id, ext in matches:
            placeholders[img_id] = placeholder_type
        
        return placeholders
    
    def _restore_placeholder_in_markdown(self, md_content: str, img_id: str, placeholder_type: str) -> str:
        """åœ¨markdownä¸­æ¢å¤ç¼ºå¤±çš„placeholder"""
        import re
        
        # æŸ¥æ‰¾å¯¹åº”çš„å›¾ç‰‡å¼•ç”¨
        pattern = rf'!\[[^\]]*\]\([^)]*{re.escape(img_id)}\.jpg\)'
        match = re.search(pattern, md_content)
        
        if match:
            # åœ¨å›¾ç‰‡å‰æ·»åŠ placeholder
            img_ref = match.group(0)
            placeholder_line = f"[placeholder: {placeholder_type}]\n{img_ref}"
            md_content = md_content.replace(img_ref, placeholder_line)
        
        return md_content
    
    def _get_processor_for_type(self, item_type: str) -> str:
        """æ ¹æ®ç±»å‹è·å–å¤„ç†å™¨åç§°"""
        if item_type == 'image':
            return "Google API"
        elif item_type in ['formula', 'interline_equation']:
            return "UnimerNet"
        elif item_type == 'table':
            return "UnimerNet"
        else:
            return "Unknown"

def show_help():
    """æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯"""
    help_text = """EXTRACT_PDF - Enhanced PDF extraction using MinerU with post-processing

Usage: EXTRACT_PDF <pdf_file> [options]
       EXTRACT_PDF --post [<markdown_file>] [--post-type <type>]
       EXTRACT_PDF --full <pdf_file> [options]
       EXTRACT_PDF --clean-data

Options:
  --page <spec>        Extract specific page(s) (e.g., 3, 1-5, 1,3,5)
  --output <dir>       Output directory (default: same as PDF)
  --output-dir <dir>   Alias for --output
  --engine <mode>      Processing engine mode:
                       basic        - Basic extractor, with image/formula/table analysis
                       basic-asyn   - Basic extractor, async mode (disable analysis)
                       mineru       - MinerU extractor, with image/formula/table analysis
                       mineru-asyn  - MinerU extractor, async mode (disable analysis)
                       full         - Full pipeline with image/formula/table analysis
                       (default: mineru)
  --post [<file>]      Post-process markdown file (replace placeholders)
                       If no file specified, enter interactive mode
  --post-type <type>   Post-processing type: image, formula, table, all (default: all)
  --ids <ids>          Specific hash IDs to process (comma-separated) or keywords:
                       all_images, all_formulas, all_tables, all
  --prompt <text>      Custom prompt for IMG2TEXT image analysis
  --force              Force reprocessing even if items are marked as processed
  --unimernet-timeout-multi <multiplier>  Timeout multiplier for UNIMERNET/IMG2TEXT processing
                       Default: 1.0 (120 seconds). Use 2.0 for 240 seconds, etc.
  --full <file>        Full pipeline: extract PDF then post-process automatically
  --clean-data         Clean all cached markdown files and images from EXTRACT_PDF_PROJ
  --help, -h           Show this help message

Examples:
  EXTRACT_PDF document.pdf --page 3
  EXTRACT_PDF paper.pdf --page 1-5 --output /path/to/output
  EXTRACT_PDF document.pdf --engine full
  EXTRACT_PDF document.pdf --engine mineru
  EXTRACT_PDF --post document.md --post-type all
EXTRACT_PDF --post document.md --post-type image
EXTRACT_PDF --post document.md --ids 4edf23de78f80bedade9e9628d7de04677faf669c945a7438bc5741c054af036
EXTRACT_PDF --post document.md --ids all_images --prompt "Analyze this research figure focusing on quantitative results"
EXTRACT_PDF --post document.md --ids all_formulas --unimernet-timeout-multi 2.0  # Double timeout for large formulas
EXTRACT_PDF --post  # Interactive mode
EXTRACT_PDF --full document.pdf  # Full pipeline
EXTRACT_PDF --clean-data  # Clean cached data"""
    
    print(help_text)

def select_pdf_file():
    """ä½¿ç”¨GUIé€‰æ‹©PDFæ–‡ä»¶"""
    try:
        import tkinter as tk
        from tkinter import filedialog
        
        root = tk.Tk()
        root.withdraw()
        
        file_path = filedialog.askopenfilename(
            title="Select PDF file",
            filetypes=[("PDF files", "*.pdf"), ("All files", "*.*")]
        )
        
        return file_path if file_path else None
    except ImportError:
        print("tkinter not available, GUI file selection not supported")
        return None
    except Exception as e:
        print(f"Error in file selection: {e}")
        return None

def main(args=None, command_identifier=None):
    """ä¸»å‡½æ•°"""
    global original_pdf_dir
    # è·å–command_identifier
    if args is None:
        args = sys.argv[1:]
    command_identifier = None
    
    # æ£€æŸ¥æ˜¯å¦è¢«RUNè°ƒç”¨ï¼ˆç¬¬ä¸€ä¸ªå‚æ•°æ˜¯command_identifierï¼‰
    if args and is_run_environment(args[0]):
        command_identifier = args[0]
        args = args[1:]  # ç§»é™¤command_identifierï¼Œä¿ç•™å®é™…å‚æ•°
    if not args:
        # å¦‚æœæ²¡æœ‰å‚æ•°ï¼Œå°è¯•ä½¿ç”¨GUIé€‰æ‹©æ–‡ä»¶
        pdf_file = select_pdf_file()
        if pdf_file:
            print(f"Selected file: {Path(pdf_file).name}")
            print(f"Starting MinerU engine processing...")
            print("Please wait ...")
            
            extractor = PDFExtractor()
            success, message = extractor.extract_pdf(pdf_file)
            
            if success:
                success_data = {
                    "success": True,
                    "message": message
                }
                if is_run_environment(command_identifier):
                    write_to_json_output(success_data, command_identifier)
                else:
                    print(f"{message}")
                return 0
            else:
                error_data = {
                    "success": False,
                    "error": message
                }
                if is_run_environment(command_identifier):
                    write_to_json_output(error_data, command_identifier)
                else:
                    print(f"{message}")
                return 1
        else:
            if is_run_environment(command_identifier):
                error_data = {"success": False, "error": "No PDF file specified"}
                write_to_json_output(error_data, command_identifier)
            else:
                print("Error: No PDF file specified")
                print("Use --help for usage information")
            return 1
    
    # è§£æå‚æ•°
    pdf_file = None
    page_spec = None
    output_dir = None
    engine_mode = "basic"
    post_file = None
    post_type = "all"
    post_ids = None
    post_prompt = None
    post_force = False
    post_timeout_multi = 1.0  # è¶…æ—¶å€æ•°ï¼Œé»˜è®¤1å€
    original_pdf_dir = None
    full_pipeline = False
    clean_data = False
    
    i = 0
    while i < len(args):
        arg = args[i]
        
        if arg in ['--help', '-h']:
            if is_run_environment(command_identifier):
                help_data = {
                    "success": True,
                    "message": "Help information",
                    "help": show_help.__doc__
                }
                write_to_json_output(help_data, command_identifier)
            else:
                show_help()
            return 0
        elif arg == '--page':
            if i + 1 < len(args):
                page_spec = args[i + 1]
                i += 2
            else:
                error_msg = "Error: --page requires a value"
                if is_run_environment(command_identifier):
                    error_data = {"success": False, "error": error_msg}
                    write_to_json_output(error_data, command_identifier)
                else:
                    print(error_msg)
                return 1
        elif arg == '--output' or arg == '--output-dir':
            if i + 1 < len(args):
                output_dir = args[i + 1]
                i += 2
            else:
                error_msg = f"Error: {arg} requires a value"
                if is_run_environment(command_identifier):
                    error_data = {"success": False, "error": error_msg}
                    write_to_json_output(error_data, command_identifier)
                else:
                    print(error_msg)
                return 1
        elif arg == '--engine':
            if i + 1 < len(args):
                engine_mode = args[i + 1]
                if engine_mode not in ['basic', 'basic-asyn', 'mineru', 'mineru-asyn', 'full']:
                    error_msg = f"Error: Invalid engine mode: {engine_mode}"
                    if is_run_environment(command_identifier):
                        error_data = {"success": False, "error": error_msg}
                        write_to_json_output(error_data, command_identifier)
                    else:
                        print(error_msg)
                    return 1
                i += 2
            else:
                error_msg = "Error: --engine requires a value"
                if is_run_environment(command_identifier):
                    error_data = {"success": False, "error": error_msg}
                    write_to_json_output(error_data, command_identifier)
                else:
                    print(error_msg)
                return 1
        elif arg == '--post':
            if i + 1 < len(args) and not args[i + 1].startswith('-'):
                post_file = args[i + 1]
                i += 2
            else:
                # è¿›å…¥interactive mode
                post_file = "interactive"
                i += 1
        elif arg == '--full':
            if i + 1 < len(args) and not args[i + 1].startswith('-'):
                pdf_file = args[i + 1]
                full_pipeline = True
                i += 2
            else:
                error_msg = "Error: --full requires a PDF file"
                if is_run_environment(command_identifier):
                    error_data = {"success": False, "error": error_msg}
                    write_to_json_output(error_data, command_identifier)
                else:
                    print(error_msg)
                return 1
        elif arg == '--clean-data':
            clean_data = True
            i += 1
        elif arg == '--batch':
            # å¯ç”¨æ‰¹å¤„ç†æ¨¡å¼ï¼ˆé»˜è®¤å·²å¯ç”¨ï¼‰
            i += 1
        elif arg == '--no-batch':
            # ç¦ç”¨æ‰¹å¤„ç†æ¨¡å¼
            i += 1
        elif arg == '--status':
            # æ˜¾ç¤ºæ‰¹å¤„ç†çŠ¶æ€
            i += 1
        elif arg == '--ids':
            if i + 1 < len(args):
                post_ids = args[i + 1]
                i += 2
            else:
                error_msg = "Error: --ids requires a value"
                if is_run_environment(command_identifier):
                    error_data = {"success": False, "error": error_msg}
                    write_to_json_output(error_data, command_identifier)
                else:
                    print(error_msg)
                return 1
        elif arg == '--prompt':
            if i + 1 < len(args):
                post_prompt = args[i + 1]
                i += 2
            else:
                error_msg = "Error: --prompt requires a value"
                if is_run_environment(command_identifier):
                    error_data = {"success": False, "error": error_msg}
                    write_to_json_output(error_data, command_identifier)
                else:
                    print(error_msg)
                return 1
        elif arg == '--post-type':
            if i + 1 < len(args):
                post_type = args[i + 1]
                if post_type not in ['image', 'formula', 'table', 'all', 'all_images', 'all_formulas', 'all_tables']:
                    error_msg = f"Error: Invalid post-type: {post_type}"
                    if is_run_environment(command_identifier):
                        error_data = {"success": False, "error": error_msg}
                        write_to_json_output(error_data, command_identifier)
                    else:
                        print(error_msg)
                    return 1
                i += 2
            else:
                error_msg = "Error: --post-type requires a value"
                if is_run_environment(command_identifier):
                    error_data = {"success": False, "error": error_msg}
                    write_to_json_output(error_data, command_identifier)
                else:
                    print(error_msg)
                return 1
        elif arg == '--original-pdf-dir':
            if i + 1 < len(args):
                original_pdf_dir = args[i + 1]
                i += 2
            else:
                error_msg = "Error: --original-pdf-dir requires a value"
                if is_run_environment(command_identifier):
                    error_data = {"success": False, "error": error_msg}
                    write_to_json_output(error_data, command_identifier)
                else:
                    print(error_msg)
                return 1
        elif arg == '--force':
            post_force = True
            i += 1
        elif arg == '--unimernet-timeout-multi':
            if i + 1 < len(args):
                try:
                    post_timeout_multi = float(args[i + 1])
                    if post_timeout_multi <= 0:
                        error_msg = "Error: --unimernet-timeout-multi must be positive"
                        if is_run_environment(command_identifier):
                            error_data = {"success": False, "error": error_msg}
                            write_to_json_output(error_data, command_identifier)
                        else:
                            print(error_msg)
                        return 1
                    i += 2
                except ValueError:
                    error_msg = "Error: --unimernet-timeout-multi must be a number"
                    if is_run_environment(command_identifier):
                        error_data = {"success": False, "error": error_msg}
                        write_to_json_output(error_data, command_identifier)
                    else:
                        print(error_msg)
                    return 1
            else:
                error_msg = "Error: --unimernet-timeout-multi requires a value"
                if is_run_environment(command_identifier):
                    error_data = {"success": False, "error": error_msg}
                    write_to_json_output(error_data, command_identifier)
                else:
                    print(error_msg)
                return 1
        elif arg.startswith('-'):
            error_msg = f"Unknown option: {arg}"
            if is_run_environment(command_identifier):
                error_data = {"success": False, "error": error_msg}
                write_to_json_output(error_data, command_identifier)
            else:
                print(error_msg)
                print("Use --help for usage information")
            return 1
        else:
            if pdf_file is None:
                pdf_file = arg
            else:
                error_msg = "Multiple PDF files specified. Only one file is supported."
                if is_run_environment(command_identifier):
                    error_data = {"success": False, "error": error_msg}
                    write_to_json_output(error_data, command_identifier)
                else:
                    print(error_msg)
                return 1
            i += 1
    
    # å¤„ç†æ¸…ç†æ•°æ®æ¨¡å¼
    if clean_data:
        extractor = PDFExtractor()
        success, message = extractor.clean_data()
        
        if success:
            success_data = {
                "success": True,
                "message": message,
                "action": "clean_data"
            }
            if is_run_environment(command_identifier):
                write_to_json_output(success_data, command_identifier)
            else:
                print(f"{message}")
            return 0
        else:
            error_data = {
                "success": False,
                "error": message,
                "action": "clean_data"
            }
            if is_run_environment(command_identifier):
                write_to_json_output(error_data, command_identifier)
            else:
                print(f"{message}")
            return 1
    
    # å¤„ç†å®Œæ•´æµç¨‹æ¨¡å¼
    if full_pipeline:
        print(f" Starting full pipeline processing: {pdf_file}")
        
        # æ„é€ ç¬¬ä¸€æ­¥å‘½ä»¤ï¼šPDFæå–
        step1_cmd = [sys.executable, __file__, pdf_file]
        if page_spec:
            step1_cmd.extend(["--page", page_spec])
        if output_dir:
            step1_cmd.extend(["--output", output_dir])
        if engine_mode != "mineru":
            step1_cmd.extend(["--engine", engine_mode])
        if clean_data:
            step1_cmd.append("--clean-data")
        
        print("Step 1: PDF extraction...")
        print(f"   Executing command: {' '.join(step1_cmd)}")
        
        try:
            result1 = subprocess.run(step1_cmd, capture_output=True, text=True, check=False)
            
            if result1.returncode != 0:
                error_data = {
                    "success": False,
                    "error": f"PDF extraction failed: {result1.stderr}",
                    "step": "extraction",
                    "command": " ".join(step1_cmd)
                }
                if is_run_environment(command_identifier):
                    write_to_json_output(error_data, command_identifier)
                else:
                    print(f"PDF extraction failed: {result1.stderr}")
                return 1
            
            print(f" PDF extraction completed")
            
            # æ ¹æ®PDFæ–‡ä»¶è·¯å¾„æ¨æ–­markdownæ–‡ä»¶è·¯å¾„
            pdf_path = Path(pdf_file).expanduser().resolve()
            
            # æ„å»ºæ­£ç¡®çš„markdownæ–‡ä»¶åï¼Œè€ƒè™‘é¡µç è§„æ ¼
            if page_spec:
                page_suffix = f"_p{page_spec}"
                md_filename = f"{pdf_path.stem}{page_suffix}.md"
            else:
                md_filename = f"{pdf_path.stem}.md"
            
            if output_dir:
                md_file = Path(output_dir) / md_filename
            else:
                md_file = pdf_path.parent / md_filename
            
            if md_file.exists():
                # æ„é€ ç¬¬äºŒæ­¥å‘½ä»¤ï¼šåå¤„ç†
                step2_cmd = [sys.executable, __file__, "--post", str(md_file)]
                # ä¼ é€’åŸå§‹PDFæ–‡ä»¶ç›®å½•ï¼Œä»¥ä¾¿åå¤„ç†å™¨èƒ½æ‰¾åˆ°çŠ¶æ€æ–‡ä»¶
                step2_cmd.extend(["--original-pdf-dir", str(pdf_path.parent)])
                if post_type != "all":
                    step2_cmd.extend(["--post-type", post_type])
                if post_ids:
                    step2_cmd.extend(["--ids", post_ids])
                if post_prompt:
                    step2_cmd.extend(["--prompt", post_prompt])
                if post_force:
                    step2_cmd.append("--force")
                
                print("Step 2: Post-processing...")
                print(f"   Executing command: {' '.join(step2_cmd)}")
                
                result2 = subprocess.run(step2_cmd, capture_output=True, text=True, check=False)
                
                if result2.returncode == 0:
                    success_data = {
                        "success": True,
                        "message": f"Full pipeline completed: {pdf_file} -> {md_file}",
                        "extraction_output": result1.stdout,
                        "post_processing": "completed",
                        "post_processing_output": result2.stdout,
                        "post_type": post_type,
                        "step1_command": " ".join(step1_cmd),
                        "step2_command": " ".join(step2_cmd)
                    }
                    if is_run_environment(command_identifier):
                        write_to_json_output(success_data, command_identifier)
                    else:
                        print(f"Full pipeline completed: {pdf_file} -> {md_file}")
                    return 0
                else:
                    # å³ä½¿åå¤„ç†å¤±è´¥ï¼ŒPDFæå–å·²æˆåŠŸ
                    warning_data = {
                        "success": True,
                        "message": f"PDF extraction completed but post-processing failed: {md_file}",
                        "extraction_output": result1.stdout,
                        "post_processing": "failed",
                        "post_processing_error": result2.stderr,
                        "post_type": post_type,
                        "step1_command": " ".join(step1_cmd),
                        "step2_command": " ".join(step2_cmd)
                    }
                    if is_run_environment(command_identifier):
                        write_to_json_output(warning_data, command_identifier)
                    else:
                        print(f"PDF extraction completed, but post-processing failed: {md_file}")
                        print("You can later use EXTRACT_PDF --post to manually perform post-processing")
                        print(f"Post-processing error: {result2.stderr}")
                    return 0
            else:
                # markdownæ–‡ä»¶ä¸å­˜åœ¨
                warning_data = {
                    "success": True,
                    "message": f"PDF extraction completed but markdown file not found: {md_file}",
                    "extraction_output": result1.stdout,
                    "post_processing": "skipped",
                    "step1_command": " ".join(step1_cmd)
                }
                if is_run_environment(command_identifier):
                    write_to_json_output(warning_data, command_identifier)
                else:
                    print(f"PDF extraction completed, but markdown file not found: {md_file}")
                return 0
                
        except Exception as e:
            error_data = {
                "success": False,
                "error": f"Full pipeline execution failed: {str(e)}",
                "step": "command_execution"
            }
            if is_run_environment(command_identifier):
                write_to_json_output(error_data, command_identifier)
            else:
                print(f"Full pipeline execution failed: {str(e)}")
            return 1
    
    # å¤„ç†åå¤„ç†æ¨¡å¼
    if post_file:
        processor = PDFPostProcessor(debug=False)
        success = processor.process_file(post_file, post_type, post_ids, post_prompt, force=post_force, timeout_multi=post_timeout_multi)
        
        if success:
            success_data = {
                "success": True,
                "message": f"Post-processing completed: {post_file}",
                "post_type": post_type
            }
            if is_run_environment(command_identifier):
                write_to_json_output(success_data, command_identifier)
            else:
                print(f"Post-processing completed: {post_file}")
            return 0
        else:
            error_data = {
                "success": False,
                "error": f"Post-processing failed: {post_file}",
                "post_type": post_type
            }
            if is_run_environment(command_identifier):
                write_to_json_output(error_data, command_identifier)
            else:
                print(f"Post-processing failed: {post_file}")
            return 1
    
    # æ£€æŸ¥æ˜¯å¦æä¾›äº†PDFæ–‡ä»¶
    if pdf_file is None:
        error_msg = "Error: No PDF file specified"
        if is_run_environment(command_identifier):
            error_data = {"success": False, "error": error_msg}
            write_to_json_output(error_data, command_identifier)
        else:
            print(error_msg)
            print("Use --help for usage information")
        return 1
    
    # æ‰§è¡ŒPDFæå–
    extractor = PDFExtractor()
    success, message = extractor.extract_pdf(pdf_file, page_spec, output_dir, engine_mode)
    
    if success:
        success_data = {
            "success": True,
            "message": message,
            "engine_mode": engine_mode
        }
        if is_run_environment(command_identifier):
            write_to_json_output(success_data, command_identifier)
        else:
            print(f"{message}")
        return 0
    else:
        error_data = {
            "success": False,
            "error": message,
            "engine_mode": engine_mode
        }
        if is_run_environment(command_identifier):
            write_to_json_output(error_data, command_identifier)
        else:
            print(f"{message}")
        return 1

def cleanup_images_folder():
    """Clean up images folder created by MinerU module imports in current working directory"""
    # Only clean up in the script directory (~/.local/bin), not in PDF directories
    script_dir = Path(__file__).parent
    images_path = script_dir / "images"
    
    if images_path.exists() and images_path.is_dir():
        try:
            # Only remove if it's empty or contains only MinerU-generated files
            contents = list(images_path.iterdir())
            if not contents:  # Empty folder
                images_path.rmdir()
            else:
                # Check if all contents are image files (likely from MinerU)
                image_extensions = {'.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff'}
                all_images = all(
                    item.is_file() and item.suffix.lower() in image_extensions 
                    for item in contents
                )
                if all_images and len(contents) < 10:  # Safety check: only clean small image folders
                    shutil.rmtree(images_path)
                    print(f"ğŸ§¹ Cleaned images folder containing {len(contents)} image files")
        except Exception as e:
            # Silently ignore cleanup errors
            pass

if __name__ == "__main__":
    try:
        exit_code = main()
        cleanup_images_folder()
        sys.exit(exit_code)
    except KeyboardInterrupt:
        cleanup_images_folder()
        print("\nCancelled")
        sys.exit(1)
    except Exception as e:
        cleanup_images_folder()
        print(f"Program exception: {e}")
        sys.exit(1) 