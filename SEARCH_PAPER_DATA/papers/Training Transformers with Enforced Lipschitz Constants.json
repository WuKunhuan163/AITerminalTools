{
  "title": "Training Transformers with Enforced Lipschitz Constants",
  "authors": [
    "Laker Newhouse",
    "R. Preston Hess",
    "Franz Cesista",
    "Andrii Zahorodnii",
    "Jeremy Bernstein",
    "Phillip Isola"
  ],
  "abstract": "Neural networks are often highly sensitive to input and weight perturbations.\nThis sensitivity has been linked to pathologies such as vulnerability to\nadversarial examples, divergent training, and overfitting. To combat these\nproblems, past research has looked at building neural networks entirely from\nLipschitz components. However, these techniques have not matured to the point\nwhere researchers have trained a modern architecture such as a transformer with\na Lipschitz certificate enforced beyond initialization. To explore this gap, we\nbegin by developing and benchmarking novel, computationally-efficient tools for\nmaintaining norm-constrained weight matrices. Applying these tools, we are able\nto train transformer models with Lipschitz bounds enforced throughout training.\nWe find that optimizer dynamics matter: switching from AdamW to Muon improves\nstandard methods -- weight decay and spectral normalization -- allowing models\nto reach equal performance with a lower Lipschitz bound. Inspired by Muon's\nupdate having a fixed spectral norm, we co-design a weight constraint method\nthat improves the Lipschitz vs. performance tradeoff on MLPs and 2M parameter\ntransformers. Our 2-Lipschitz transformer on Shakespeare text reaches\nvalidation accuracy 60%. Scaling to 145M parameters, our 10-Lipschitz\ntransformer reaches 21% accuracy on internet text. However, to match the\nNanoGPT baseline validation accuracy of 39.4%, our Lipschitz upper bound\nincreases to 10^264. Nonetheless, our Lipschitz transformers train without\nstability measures such as layer norm, QK norm, and logit tanh softcapping.",
  "url": "http://arxiv.org/abs/2507.13338v1",
  "pdf_url": "http://arxiv.org/pdf/2507.13338v1.pdf",
  "publication_date": "2025-07-17",
  "venue": "arXiv preprint",
  "citation_count": null,
  "source": "arxiv"
}