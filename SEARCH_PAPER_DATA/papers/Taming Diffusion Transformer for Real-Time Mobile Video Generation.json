{
  "title": "Taming Diffusion Transformer for Real-Time Mobile Video Generation",
  "authors": [
    "Yushu Wu",
    "Yanyu Li",
    "Anil Kag",
    "Ivan Skorokhodov",
    "Willi Menapace",
    "Ke Ma",
    "Arpit Sahni",
    "Ju Hu",
    "Aliaksandr Siarohin",
    "Dhritiman Sagar",
    "Yanzhi Wang",
    "Sergey Tulyakov"
  ],
  "abstract": "Diffusion Transformers (DiT) have shown strong performance in video\ngeneration tasks, but their high computational cost makes them impractical for\nresource-constrained devices like smartphones, and real-time generation is even\nmore challenging. In this work, we propose a series of novel optimizations to\nsignificantly accelerate video generation and enable real-time performance on\nmobile platforms. First, we employ a highly compressed variational autoencoder\n(VAE) to reduce the dimensionality of the input data without sacrificing visual\nquality. Second, we introduce a KD-guided, sensitivity-aware tri-level pruning\nstrategy to shrink the model size to suit mobile platform while preserving\ncritical performance characteristics. Third, we develop an adversarial step\ndistillation technique tailored for DiT, which allows us to reduce the number\nof inference steps to four. Combined, these optimizations enable our model to\nachieve over 10 frames per second (FPS) generation on an iPhone 16 Pro Max,\ndemonstrating the feasibility of real-time, high-quality video generation on\nmobile devices.",
  "url": "http://arxiv.org/abs/2507.13343v1",
  "pdf_url": "http://arxiv.org/pdf/2507.13343v1.pdf",
  "publication_date": "2025-07-17",
  "venue": "arXiv preprint",
  "citation_count": null,
  "source": "arxiv"
}