{
  "title": "Language Models Improve When Pretraining Data Matches Target Tasks",
  "authors": [
    "David Mizrahi",
    "Anders Boesen Lindbo Larsen",
    "Jesse Allardice",
    "Suzie Petryk",
    "Yuri Gorokhov",
    "Jeffrey Li",
    "Alex Fang",
    "Josh Gardner",
    "Tom Gunter",
    "Afshin Dehghan"
  ],
  "abstract": "Every data selection method inherently has a target. In practice, these\ntargets often emerge implicitly through benchmark-driven iteration: researchers\ndevelop selection strategies, train models, measure benchmark performance, then\nrefine accordingly. This raises a natural question: what happens when we make\nthis optimization explicit? To explore this, we propose benchmark-targeted\nranking (BETR), a simple method that selects pretraining documents based on\nsimilarity to benchmark training examples. BETR embeds benchmark examples and a\nsample of pretraining documents in a shared space, scores this sample by\nsimilarity to benchmarks, then trains a lightweight classifier to predict these\nscores for the full corpus. We compare data selection methods by training over\n500 models spanning $10^{19}$ to $10^{22}$ FLOPs and fitting scaling laws to\nthem. From this, we find that simply aligning pretraining data to evaluation\nbenchmarks using BETR achieves a 2.1x compute multiplier over DCLM-Baseline\n(4.7x over unfiltered data) and improves performance on 9 out of 10 tasks\nacross all scales. BETR also generalizes well: when targeting a diverse set of\nbenchmarks disjoint from our evaluation suite, it still matches or outperforms\nbaselines. Our scaling analysis further reveals a clear trend: larger models\nrequire less aggressive filtering. Overall, our findings show that directly\nmatching pretraining data to target tasks precisely shapes model capabilities\nand highlight that optimal selection strategies must adapt to model scale.",
  "url": "http://arxiv.org/abs/2507.12466v1",
  "pdf_url": "http://arxiv.org/pdf/2507.12466v1.pdf",
  "publication_date": "2025-07-16",
  "venue": "arXiv preprint",
  "citation_count": null,
  "source": "arxiv"
}